{
  "last_updated": "2025-08-25T08:30:22.499730",
  "total_papers": 25,
  "papers": [
    {
      "title": "Visual Prompting for Robotic Manipulation with Annotation-Guided\n  Pick-and-Place Using ACT",
      "link": "https://arxiv.org/abs/2508.08748v1",
      "pdf_link": "https://arxiv.org/pdf/2508.08748v1.pdf",
      "summary": "Robotic pick-and-place tasks in convenience stores pose challenges due to\ndense object arrangements, occlusions, and variations in object properties such\nas color, shape, size, and texture. These factors complicate trajectory\nplanning and grasping. This paper introduces a perception-action pipeline\nleveraging annotation-guided visual prompting, where bounding box annotations\nidentify both pickable objects and placement locations, providing structured\nspatial guidance. Instead of traditional step-by-step planning, we employ\nAction Chunking with Transformers (ACT) as an imitation learning algorithm,\nenabling the robotic arm to predict chunked action sequences from human\ndemonstrations. This facilitates smooth, adaptive, and data-driven\npick-and-place operations. We evaluate our system based on success rate and\nvisual analysis of grasping behavior, demonstrating improved grasp accuracy and\nadaptability in retail environments.",
      "published": "2025-08-12T08:45:09Z",
      "authors": [
        "Muhammad A. Muttaqien",
        "Tomohiro Motoda",
        "Ryo Hanai",
        "Yukiyasu Domae"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.08748v1",
      "relevance_score": 11
    },
    {
      "title": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like\n  Priors",
      "link": "https://arxiv.org/abs/2508.08896v3",
      "pdf_link": "https://arxiv.org/pdf/2508.08896v3.pdf",
      "summary": "A dexterous hand capable of generalizable grasping objects is fundamental for\nthe development of general-purpose embodied AI. However, previous methods focus\nnarrowly on low-level grasp stability metrics, neglecting affordance-aware\npositioning and human-like poses which are crucial for downstream manipulation.\nTo address these limitations, we propose AffordDex, a novel framework with\ntwo-stage training that learns a universal grasping policy with an inherent\nunderstanding of both motion priors and object affordances. In the first stage,\na trajectory imitator is pre-trained on a large corpus of human hand motions to\ninstill a strong prior for natural movement. In the second stage, a residual\nmodule is trained to adapt these general human-like motions to specific object\ninstances. This refinement is critically guided by two components: our Negative\nAffordance-aware Segmentation (NAA) module, which identifies functionally\ninappropriate contact regions, and a privileged teacher-student distillation\nprocess that ensures the final vision-based policy is highly successful.\nExtensive experiments demonstrate that AffordDex not only achieves universal\ndexterous grasping but also remains remarkably human-like in posture and\nfunctionally appropriate in contact location. As a result, AffordDex\nsignificantly outperforms state-of-the-art baselines across seen objects,\nunseen instances, and even entirely novel categories.",
      "published": "2025-08-12T12:36:01Z",
      "authors": [
        "Haoyu Zhao",
        "Linghao Zhuang",
        "Xingyue Zhao",
        "Cheng Zeng",
        "Haoran Xu",
        "Yuming Jiang",
        "Jun Cen",
        "Kexiang Wang",
        "Jiayan Guo",
        "Siteng Huang",
        "Xin Li",
        "Deli Zhao",
        "Hua Zou"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.08896v3",
      "relevance_score": 11
    },
    {
      "title": "FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile\n  Shortcut Policy",
      "link": "https://arxiv.org/abs/2508.14441v1",
      "pdf_link": "https://arxiv.org/pdf/2508.14441v1.pdf",
      "summary": "Dexterous in-hand manipulation is a long-standing challenge in robotics due\nto complex contact dynamics and partial observability. While humans synergize\nvision and touch for such tasks, robotic approaches often prioritize one\nmodality, therefore limiting adaptability. This paper introduces Flow Before\nImitation (FBI), a visuotactile imitation learning framework that dynamically\nfuses tactile interactions with visual observations through motion dynamics.\nUnlike prior static fusion methods, FBI establishes a causal link between\ntactile signals and object motion via a dynamics-aware latent model. FBI\nemploys a transformer-based interaction module to fuse flow-derived tactile\nfeatures with visual inputs, training a one-step diffusion policy for real-time\nexecution. Extensive experiments demonstrate that the proposed method\noutperforms the baseline methods in both simulation and the real world on two\ncustomized in-hand manipulation tasks and three standard dexterous manipulation\ntasks. Code, models, and more results are available in the website\nhttps://sites.google.com/view/dex-fbi.",
      "published": "2025-08-20T05:53:05Z",
      "authors": [
        "Yijin Chen",
        "Wenqiang Xu",
        "Zhenjun Yu",
        "Tutian Tang",
        "Yutong Li",
        "Siqiong Yao",
        "Cewu Lu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.14441v1",
      "relevance_score": 10
    },
    {
      "title": "Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding\n  Contact on Manifold Meshes for Dexterous In-Hand Manipulation",
      "link": "https://arxiv.org/abs/2508.12439v1",
      "pdf_link": "https://arxiv.org/pdf/2508.12439v1.pdf",
      "summary": "Reasoning about rolling and sliding contact, or roll-slide contact for short,\nis critical for dexterous manipulation tasks that involve intricate geometries.\nBut existing works on roll-slide contact mostly focus on continuous shapes with\ndifferentiable parametrizations. This work extends roll-slide contact modeling\nto manifold meshes. Specifically, we present an integration scheme based on\ngeodesic tracing to first-order time-integrate roll-slide contact directly on\nmeshes, enabling dexterous manipulation to reason over high-fidelity discrete\nrepresentations of an object's true geometry. Using our method, we planned\ndexterous motions of a multi-finger robotic hand manipulating five objects\nin-hand in simulation. The planning was achieved with a least-squares optimizer\nthat strives to maintain the most stable instantaneous grasp by minimizing\ncontact sliding and spinning. Then, we evaluated our method against a baseline\nusing collision detection and a baseline using primitive shapes. The results\nshow that our method performed the best in accuracy and precision, even for\ncoarse meshes. We conclude with a future work discussion on incorporating\nmultiple contacts and contact forces to achieve accurate and robust mesh-based\nsurface contact modeling.",
      "published": "2025-08-17T17:13:25Z",
      "authors": [
        "Sunyu Wang",
        "Arjun S. Lakshmipathy",
        "Jean Oh",
        "Nancy S. Pollard"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.12439v1",
      "relevance_score": 10
    },
    {
      "title": "DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of\n  Fruit",
      "link": "https://arxiv.org/abs/2508.07118v1",
      "pdf_link": "https://arxiv.org/pdf/2508.07118v1.pdf",
      "summary": "DexFruit is a robotic manipulation framework that enables gentle, autonomous\nhandling of fragile fruit and precise evaluation of damage. Many fruits are\nfragile and prone to bruising, thus requiring humans to manually harvest them\nwith care. In this work, we demonstrate by using optical tactile sensing,\nautonomous manipulation of fruit with minimal damage can be achieved. We show\nthat our tactile informed diffusion policies outperform baselines in both\nreduced bruising and pick-and-place success rate across three fruits:\nstrawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,\na novel technique to represent and quantify visual damage in high-resolution 3D\nrepresentation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring\ndamage lack quantitative rigor or require expensive equipment. With FruitSplat,\nwe distill a 2D strawberry mask as well as a 2D bruise segmentation mask into\nthe 3DGS representation. Furthermore, this representation is modular and\ngeneral, compatible with any relevant 2D model. Overall, we demonstrate a 92%\ngrasping policy success rate, up to a 20% reduction in visual bruising, and up\nto an 31% improvement in grasp success rate on challenging fruit compared to\nour baselines across our three tested fruits. We rigorously evaluate this\nresult with over 630 trials. Please checkout our website at\nhttps://dex-fruit.github.io .",
      "published": "2025-08-09T23:38:17Z",
      "authors": [
        "Aiden Swann",
        "Alex Qiu",
        "Matthew Strong",
        "Angelina Zhang",
        "Samuel Morstein",
        "Kai Rayle",
        "Monroe Kennedy III"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.07118v1",
      "relevance_score": 10
    },
    {
      "title": "GraspQP: Differentiable Optimization of Force Closure for Diverse and\n  Robust Dexterous Grasping",
      "link": "https://arxiv.org/abs/2508.15002v1",
      "pdf_link": "https://arxiv.org/pdf/2508.15002v1.pdf",
      "summary": "Dexterous robotic hands enable versatile interactions due to the flexibility\nand adaptability of multi-fingered designs, allowing for a wide range of\ntask-specific grasp configurations in diverse environments. However, to fully\nexploit the capabilities of dexterous hands, access to diverse and high-quality\ngrasp data is essential -- whether for developing grasp prediction models from\npoint clouds, training manipulation policies, or supporting high-level task\nplanning with broader action options. Existing approaches for dataset\ngeneration typically rely on sampling-based algorithms or simplified\nforce-closure analysis, which tend to converge to power grasps and often\nexhibit limited diversity. In this work, we propose a method to synthesize\nlarge-scale, diverse, and physically feasible grasps that extend beyond simple\npower grasps to include refined manipulations, such as pinches and tri-finger\nprecision grasps. We introduce a rigorous, differentiable energy formulation of\nforce closure, implicitly defined through a Quadratic Program (QP).\nAdditionally, we present an adjusted optimization method (MALA*) that improves\nperformance by dynamically rejecting gradient steps based on the distribution\nof energy values across all samples. We extensively evaluate our approach and\ndemonstrate significant improvements in both grasp diversity and the stability\nof final grasp predictions. Finally, we provide a new, large-scale grasp\ndataset for 5,700 objects from DexGraspNet, comprising five different grippers\nand three distinct grasp types.\n  Dataset and Code:https://graspqp.github.io/",
      "published": "2025-08-20T18:43:16Z",
      "authors": [
        "René Zurbrügg",
        "Andrei Cramariuc",
        "Marco Hutter"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.15002v1",
      "relevance_score": 9
    },
    {
      "title": "Investigating Sensors and Methods in Grasp State Classification in\n  Agricultural Manipulation",
      "link": "https://arxiv.org/abs/2508.11588v1",
      "pdf_link": "https://arxiv.org/pdf/2508.11588v1.pdf",
      "summary": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.",
      "published": "2025-08-15T16:47:42Z",
      "authors": [
        "Benjamin Walt",
        "Jordan Westphal",
        "Girish Krishnan"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.11588v1",
      "relevance_score": 8
    },
    {
      "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned\n  Tactile Sensing",
      "link": "https://arxiv.org/abs/2508.08706v2",
      "pdf_link": "https://arxiv.org/pdf/2508.08706v2.pdf",
      "summary": "Recent vision-language-action (VLA) models build upon vision-language\nfoundations, and have achieved promising results and exhibit the possibility of\ntask generalization in robot manipulation. However, due to the heterogeneity of\ntactile sensors and the difficulty of acquiring tactile data, current VLA\nmodels significantly overlook the importance of tactile perception and fail in\ncontact-rich tasks. To address this issue, this paper proposes OmniVTLA, a\nnovel architecture involving tactile sensing. Specifically, our contributions\nare threefold. First, our OmniVTLA features a dual-path tactile encoder\nframework. This framework enhances tactile perception across diverse\nvision-based and force-based tactile sensors by using a pretrained vision\ntransformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we\nintroduce ObjTac, a comprehensive force-based tactile dataset capturing\ntextual, visual, and tactile information for 56 objects across 10 categories.\nWith 135K tri-modal samples, ObjTac supplements existing visuo-tactile\ndatasets. Third, leveraging this dataset, we train a semantically-aligned\ntactile encoder to learn a unified tactile representation, serving as a better\ninitialization for OmniVTLA. Real-world experiments demonstrate substantial\nimprovements over state-of-the-art VLA baselines, achieving 96.9% success rates\nwith grippers, (21.9% higher over baseline) and 100% success rates with\ndexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides,\nOmniVTLA significantly reduces task completion time and generates smoother\ntrajectories through tactile sensing compared to existing VLA. Our ObjTac\ndataset can be found at https://readerek.github.io/Objtac.github.io",
      "published": "2025-08-12T07:53:36Z",
      "authors": [
        "Zhengxue Cheng",
        "Yiqian Zhang",
        "Wenkang Zhang",
        "Haoyu Li",
        "Keyu Wang",
        "Li Song",
        "Hengdi Zhang"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.08706v2",
      "relevance_score": 8
    },
    {
      "title": "UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation\n  Strategy and Dataset for Diverse Dexterous Hands",
      "link": "https://arxiv.org/abs/2508.03339v1",
      "pdf_link": "https://arxiv.org/pdf/2508.03339v1.pdf",
      "summary": "Dexterous grasp datasets are vital for embodied intelligence, but mostly\nemphasize grasp stability, ignoring functional grasps needed for tasks like\nopening bottle caps or holding cup handles. Most rely on bulky, costly, and\nhard-to-control high-DOF Shadow Hands. Inspired by the human hand's\nunderactuated mechanism, we establish UniFucGrasp, a universal functional grasp\nannotation strategy and dataset for multiple dexterous hand types. Based on\nbiomimicry, it maps natural human motions to diverse hand structures and uses\ngeometry-based force closure to ensure functional, stable, human-like grasps.\nThis method supports low-cost, efficient collection of diverse, high-quality\nfunctional grasps. Finally, we establish the first multi-hand functional grasp\ndataset and provide a synthesis model to validate its effectiveness.\nExperiments on the UFG dataset, IsaacSim, and complex robotic tasks show that\nour method improves functional manipulation accuracy and grasp stability,\nenables efficient generalization across diverse robotic hands, and overcomes\nannotation cost and generalization challenges in dexterous grasping. The\nproject page is at https://haochen611.github.io/UFG.",
      "published": "2025-08-05T11:37:38Z",
      "authors": [
        "Haoran Lin",
        "Wenrui Chen",
        "Xianchi Chen",
        "Fan Yang",
        "Qiang Diao",
        "Wenxin Xie",
        "Sijie Wu",
        "Kailun Yang",
        "Maojun Li",
        "Yaonan Wang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "eess.IV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.03339v1",
      "relevance_score": 8
    },
    {
      "title": "GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System",
      "link": "https://arxiv.org/abs/2508.15990v1",
      "pdf_link": "https://arxiv.org/pdf/2508.15990v1.pdf",
      "summary": "Accurately perceiving an object's pose and shape is essential for precise\ngrasping and manipulation. Compared to common vision-based methods, tactile\nsensing offers advantages in precision and immunity to occlusion when tracking\nand reconstructing objects in contact. This makes it particularly valuable for\nin-hand and other high-precision manipulation tasks. In this work, we present\nGelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to\nestimate object pose over long periods and reconstruct object shapes with high\nfidelity. Unlike traditional point cloud-based approaches, GelSLAM uses\ntactile-derived surface normals and curvatures for robust tracking and loop\nclosure. It can track object motion in real time with low error and minimal\ndrift, and reconstruct shapes with submillimeter accuracy, even for low-texture\nobjects such as wooden tools. GelSLAM extends tactile sensing beyond local\ncontact to enable global, long-horizon spatial perception, and we believe it\nwill serve as a foundation for many precise manipulation tasks involving\ninteraction with objects in hand. The video demo is available on our website:\nhttps://joehjhuang.github.io/gelslam.",
      "published": "2025-08-21T22:20:43Z",
      "authors": [
        "Hung-Jui Huang",
        "Mohammad Amin Mirzaee",
        "Michael Kaess",
        "Wenzhen Yuan"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.15990v1",
      "relevance_score": 7
    },
    {
      "title": "Exploiting Policy Idling for Dexterous Manipulation",
      "link": "https://arxiv.org/abs/2508.15669v1",
      "pdf_link": "https://arxiv.org/pdf/2508.15669v1.pdf",
      "summary": "Learning-based methods for dexterous manipulation have made notable progress\nin recent years. However, learned policies often still lack reliability and\nexhibit limited robustness to important factors of variation. One failure\npattern that can be observed across many settings is that policies idle, i.e.\nthey cease to move beyond a small region of states when they reach certain\nstates. This policy idling is often a reflection of the training data. For\ninstance, it can occur when the data contains small actions in areas where the\nrobot needs to perform high-precision motions, e.g., when preparing to grasp an\nobject or object insertion. Prior works have tried to mitigate this phenomenon\ne.g. by filtering the training data or modifying the control frequency.\nHowever, these approaches can negatively impact policy performance in other\nways. As an alternative, we investigate how to leverage the detectability of\nidling behavior to inform exploration and policy improvement. Our approach,\nPause-Induced Perturbations (PIP), applies perturbations at detected idling\nstates, thus helping it to escape problematic basins of attraction. On a range\nof challenging simulated dual-arm tasks, we find that this simple approach can\nalready noticeably improve test-time performance, with no additional\nsupervision or training. Furthermore, since the robot tends to idle at critical\npoints in a movement, we also find that learning from the resulting episodes\nleads to better iterative policy improvement compared to prior approaches. Our\nperturbation strategy also leads to a 15-35% improvement in absolute success\nrate on a real-world insertion task that requires complex multi-finger\nmanipulation.",
      "published": "2025-08-21T15:52:45Z",
      "authors": [
        "Annie S. Chen",
        "Philemon Brakel",
        "Antonia Bronars",
        "Annie Xie",
        "Sandy Huang",
        "Oliver Groth",
        "Maria Bauza",
        "Markus Wulfmeier",
        "Nicolas Heess",
        "Dushyant Rao"
      ],
      "categories": [
        "cs.RO",
        "cs.LG",
        "68T40",
        "I.2.9"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.15669v1",
      "relevance_score": 7
    },
    {
      "title": "Multimodal Spiking Neural Network for Space Robotic Manipulation",
      "link": "https://arxiv.org/abs/2508.07287v1",
      "pdf_link": "https://arxiv.org/pdf/2508.07287v1.pdf",
      "summary": "This paper presents a multimodal control framework based on spiking neural\nnetworks (SNNs) for robotic arms aboard space stations. It is designed to cope\nwith the constraints of limited onboard resources while enabling autonomous\nmanipulation and material transfer in space operations. By combining geometric\nstates with tactile and semantic information, the framework strengthens\nenvironmental awareness and contributes to more robust control strategies. To\nguide the learning process progressively, a dual-channel, three-stage\ncurriculum reinforcement learning (CRL) scheme is further integrated into the\nsystem. The framework was tested across a range of tasks including target\napproach, object grasping, and stable lifting with wall-mounted robotic arms,\ndemonstrating reliable performance throughout. Experimental evaluations\ndemonstrate that the proposed method consistently outperforms baseline\napproaches in both task success rate and energy efficiency. These findings\nhighlight its suitability for real-world aerospace applications.",
      "published": "2025-08-10T10:51:20Z",
      "authors": [
        "Liwen Zhang",
        "Dong Zhou",
        "Shibo Shao",
        "Zihao Su",
        "Guanghui Sun"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.07287v1",
      "relevance_score": 7
    },
    {
      "title": "Taming VR Teleoperation and Learning from Demonstration for Multi-Task\n  Bimanual Table Service Manipulation",
      "link": "https://arxiv.org/abs/2508.14542v2",
      "pdf_link": "https://arxiv.org/pdf/2508.14542v2.pdf",
      "summary": "This technical report presents the champion solution of the Table Service\nTrack in the ICRA 2025 What Bimanuals Can Do (WBCD) competition. We tackled a\nseries of demanding tasks under strict requirements for speed, precision, and\nreliability: unfolding a tablecloth (deformable-object manipulation), placing a\npizza into the container (pick-and-place), and opening and closing a food\ncontainer with the lid. Our solution combines VR-based teleoperation and\nLearning from Demonstrations (LfD) to balance robustness and autonomy. Most\nsubtasks were executed through high-fidelity remote teleoperation, while the\npizza placement was handled by an ACT-based policy trained from 100 in-person\nteleoperated demonstrations with randomized initial configurations. By\ncarefully integrating scoring rules, task characteristics, and current\ntechnical capabilities, our approach achieved both high efficiency and\nreliability, ultimately securing the first place in the competition.",
      "published": "2025-08-20T08:47:40Z",
      "authors": [
        "Weize Li",
        "Zhengxiao Han",
        "Lixin Xu",
        "Xiangyu Chen",
        "Harrison Bounds",
        "Chenrui Zhang",
        "Yifan Xu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.14542v2",
      "relevance_score": 6
    },
    {
      "title": "Scaling Whole-body Multi-contact Manipulation with Contact Optimization",
      "link": "https://arxiv.org/abs/2508.12980v1",
      "pdf_link": "https://arxiv.org/pdf/2508.12980v1.pdf",
      "summary": "Daily tasks require us to use our whole body to manipulate objects, for\ninstance when our hands are unavailable. We consider the issue of providing\nhumanoid robots with the ability to autonomously perform similar whole-body\nmanipulation tasks. In this context, the infinite possibilities for where and\nhow contact can occur on the robot and object surfaces hinder the scalability\nof existing planning methods, which predominantly rely on discrete sampling.\nGiven the continuous nature of contact surfaces, gradient-based optimization\noffers a more suitable approach for finding solutions. However, a key remaining\nchallenge is the lack of an efficient representation of robot surfaces. In this\nwork, we propose (i) a representation of robot and object surfaces that enables\nclosed-form computation of proximity points, and (ii) a cost design that\neffectively guides whole-body manipulation planning. Our experiments\ndemonstrate that the proposed framework can solve problems unaddressed by\nexisting methods, and achieves a 77% improvement in planning time over the\nstate of the art. We also validate the suitability of our approach on real\nhardware through the whole-body manipulation of boxes by a humanoid robot.",
      "published": "2025-08-18T14:56:37Z",
      "authors": [
        "Victor Levé",
        "João Moura",
        "Sachiya Fujita",
        "Tamon Miyake",
        "Steve Tonneau",
        "Sethu Vijayakumar"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.12980v1",
      "relevance_score": 6
    },
    {
      "title": "Fully Spiking Actor-Critic Neural Network for Robotic Manipulation",
      "link": "https://arxiv.org/abs/2508.12038v1",
      "pdf_link": "https://arxiv.org/pdf/2508.12038v1.pdf",
      "summary": "This study proposes a hybrid curriculum reinforcement learning (CRL)\nframework based on a fully spiking neural network (SNN) for 9-degree-of-freedom\nrobotic arms performing target reaching and grasping tasks. To reduce network\ncomplexity and inference latency, the SNN architecture is simplified to include\nonly an input and an output layer, which shows strong potential for\nresource-constrained environments. Building on the advantages of SNNs-high\ninference speed, low energy consumption, and spike-based biological\nplausibility, a temporal progress-partitioned curriculum strategy is integrated\nwith the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy\nconsumption modeling framework is introduced to quantitatively compare the\ntheoretical energy consumption between SNNs and conventional Artificial Neural\nNetworks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized\nobservation space further improve learning efficiency and policy accuracy.\nExperiments on the Isaac Gym simulation platform demonstrate that the proposed\nmethod achieves superior performance under realistic physical constraints.\nComparative evaluations with conventional PPO and ANN baselines validate the\nscalability and energy efficiency of the proposed approach in dynamic robotic\nmanipulation tasks.",
      "published": "2025-08-16T13:27:15Z",
      "authors": [
        "Liwen Zhang",
        "Heng Deng",
        "Guanghui Sun"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.12038v1",
      "relevance_score": 6
    },
    {
      "title": "Visuomotor Grasping with World Models for Surgical Robots",
      "link": "https://arxiv.org/abs/2508.11200v1",
      "pdf_link": "https://arxiv.org/pdf/2508.11200v1.pdf",
      "summary": "Grasping is a fundamental task in robot-assisted surgery (RAS), and\nautomating it can reduce surgeon workload while enhancing efficiency, safety,\nand consistency beyond teleoperated systems. Most prior approaches rely on\nexplicit object pose tracking or handcrafted visual features, limiting their\ngeneralization to novel objects, robustness to visual disturbances, and the\nability to handle deformable objects. Visuomotor learning offers a promising\nalternative, but deploying it in RAS presents unique challenges, such as low\nsignal-to-noise ratio in visual observations, demands for high safety and\nmillimeter-level precision, as well as the complex surgical environment. This\npaper addresses three key challenges: (i) sim-to-real transfer of visuomotor\npolicies to ex vivo surgical scenes, (ii) visuomotor learning using only a\nsingle stereo camera pair -- the standard RAS setup, and (iii) object-agnostic\ngrasping with a single policy that generalizes to diverse, unseen surgical\nobjects without retraining or task-specific models. We introduce Grasp Anything\nfor Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.\nGASv2 leverages a world-model-based architecture and a surgical perception\npipeline for visual observations, combined with a hybrid control system for\nsafe execution. We train the policy in simulation using domain randomization\nfor sim-to-real transfer and deploy it on a real robot in both phantom-based\nand ex vivo surgical settings, using only a single pair of endoscopic cameras.\nExtensive experiments show our policy achieves a 65% success rate in both\nsettings, generalizes to unseen objects and grippers, and adapts to diverse\ndisturbances, demonstrating strong performance, generality, and robustness.",
      "published": "2025-08-15T04:23:07Z",
      "authors": [
        "Hongbin Lin",
        "Bin Li",
        "Kwok Wai Samuel Au"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.11200v1",
      "relevance_score": 6
    },
    {
      "title": "CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail",
      "link": "https://arxiv.org/abs/2508.09558v1",
      "pdf_link": "https://arxiv.org/pdf/2508.09558v1.pdf",
      "summary": "The manipulation of deformable linear flexures has a wide range of\napplications in industry, such as cable routing in automotive manufacturing and\ntextile production. Cable routing, as a complex multi-stage robot manipulation\nscenario, is a challenging task for robot automation. Common parallel\ntwo-finger grippers have the risk of over-squeezing and over-tension when\ngrasping and guiding cables. In this paper, a novel eagle-inspired fingernail\nis designed and mounted on the gripper fingers, which helps with cable grasping\non planar surfaces and in-hand cable guiding operations. Then we present a\nsingle-grasp end-to-end 3D cable routing framework utilizing the proposed\nfingernails, instead of the common pick-and-place strategy. Continuous control\nis achieved to efficiently manipulate cables through vision-based state\nestimation of task configurations and offline trajectory planning based on\nmotion primitives. We evaluate the effectiveness of the proposed framework with\na variety of cables and channel slots, significantly outperforming the\npick-and-place manipulation process under equivalent perceptual conditions. Our\nreconfigurable task setting and the proposed framework provide a reference for\nfuture cable routing manipulations in 3D space.",
      "published": "2025-08-13T07:25:40Z",
      "authors": [
        "Jiahui Zuo",
        "Boyang Zhang",
        "Fumin Zhang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.09558v1",
      "relevance_score": 5
    },
    {
      "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction\n  and Mobile Robotic Manipulation",
      "link": "https://arxiv.org/abs/2508.07770v2",
      "pdf_link": "https://arxiv.org/pdf/2508.07770v2.pdf",
      "summary": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/",
      "published": "2025-08-11T08:56:19Z",
      "authors": [
        "Yizheng Zhang",
        "Zhenjun Yu",
        "Jiaxin Lai",
        "Cewu Lu",
        "Lei Han"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.07770v2",
      "relevance_score": 5
    },
    {
      "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for\n  Policy Reasoning and Dual Robotic Control",
      "link": "https://arxiv.org/abs/2508.05342v1",
      "pdf_link": "https://arxiv.org/pdf/2508.05342v1.pdf",
      "summary": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations.",
      "published": "2025-08-07T12:48:09Z",
      "authors": [
        "Shunlei Li",
        "Longsen Gao",
        "Jin Wang",
        "Chang Che",
        "Xi Xiao",
        "Jiuwen Cao",
        "Yingbai Hu",
        "Hamid Reza Karimi"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.05342v1",
      "relevance_score": 5
    },
    {
      "title": "Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object\n  Manipulation",
      "link": "https://arxiv.org/abs/2508.14042v1",
      "pdf_link": "https://arxiv.org/pdf/2508.14042v1.pdf",
      "summary": "Realizing generalizable dynamic object manipulation is important for\nenhancing manufacturing efficiency, as it eliminates specialized engineering\nfor various scenarios. To this end, imitation learning emerges as a promising\nparadigm, leveraging expert demonstrations to teach a policy manipulation\nskills. Although the generalization of an imitation learning policy can be\nimproved by increasing demonstrations, demonstration collection is\nlabor-intensive. To address this problem, this paper investigates whether\nstrong generalization in dynamic object manipulation is achievable with only a\nfew demonstrations. Specifically, we develop an entropy-based theoretical\nframework to quantify the optimization of imitation learning. Based on this\nframework, we propose a system named Generalizable Entropy-based Manipulation\n(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM\ncan generalize across diverse environment backgrounds, robot embodiments,\nmotion dynamics, and object geometries. Notably, GEM has been deployed in a\nreal canteen for tableware collection. Without any in-scene demonstration, it\nachieves a success rate of over 97% across more than 10,000 operations.",
      "published": "2025-08-19T17:59:59Z",
      "authors": [
        "Zhuoling Li",
        "Xiaoyang Wu",
        "Zhenhua Xu",
        "Hengshuang Zhao"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.14042v1",
      "relevance_score": 4
    },
    {
      "title": "MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for\n  Quadruped Robot with Arm",
      "link": "https://arxiv.org/abs/2508.10538v1",
      "pdf_link": "https://arxiv.org/pdf/2508.10538v1.pdf",
      "summary": "Whole-body loco-manipulation for quadruped robots with arm remains a\nchallenging problem, particularly in achieving multi-task control. To address\nthis, we propose MLM, a reinforcement learning framework driven by both\nreal-world and simulation data. It enables a six-DoF robotic arm--equipped\nquadruped robot to perform whole-body loco-manipulation for multiple tasks\nautonomously or under human teleoperation. To address the problem of balancing\nmultiple tasks during the learning of loco-manipulation, we introduce a\ntrajectory library with an adaptive, curriculum-based sampling mechanism. This\napproach allows the policy to efficiently leverage real-world collected\ntrajectories for learning multi-task loco-manipulation. To address deployment\nscenarios with only historical observations and to enhance the performance of\npolicy execution across tasks with different spatial ranges, we propose a\nTrajectory-Velocity Prediction policy network. It predicts unobservable future\ntrajectories and velocities. By leveraging extensive simulation data and\ncurriculum-based rewards, our controller achieves whole-body behaviors in\nsimulation and zero-shot transfer to real-world deployment. Ablation studies in\nsimulation verify the necessity and effectiveness of our approach, while\nreal-world experiments on the Go2 robot with an Airbot robotic arm demonstrate\nthe policy's good performance in multi-task execution.",
      "published": "2025-08-14T11:18:32Z",
      "authors": [
        "Xin Liu",
        "Bida Ma",
        "Chenkun Qi",
        "Yan Ding",
        " Zhaxizhuoma",
        "Guorong Zhang",
        "Pengan Chen",
        "Kehui Liu",
        "Zhongjie Jia",
        "Chuyue Guan",
        "Yule Mo",
        "Jiaqi Liu",
        "Feng Gao",
        "Jiangwei Zhong",
        "Bin Zhao",
        "Xuelong Li"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.10538v1",
      "relevance_score": 4
    },
    {
      "title": "Improving Generalization of Language-Conditioned Robot Manipulation",
      "link": "https://arxiv.org/abs/2508.02405v1",
      "pdf_link": "https://arxiv.org/pdf/2508.02405v1.pdf",
      "summary": "The control of robots for manipulation tasks generally relies on visual\ninput. Recent advances in vision-language models (VLMs) enable the use of\nnatural language instructions to condition visual input and control robots in a\nwider range of environments. However, existing methods require a large amount\nof data to fine-tune VLMs for operating in unseen environments. In this paper,\nwe present a framework that learns object-arrangement tasks from just a few\ndemonstrations. We propose a two-stage framework that divides\nobject-arrangement tasks into a target localization stage, for picking the\nobject, and a region determination stage for placing the object. We present an\ninstance-level semantic fusion module that aligns the instance-level image\ncrops with the text embedding, enabling the model to identify the target\nobjects defined by the natural language instructions. We validate our method on\nboth simulation and real-world robotic environments. Our method, fine-tuned\nwith a few demonstrations, improves generalization capability and demonstrates\nzero-shot ability in real-robot manipulation scenarios.",
      "published": "2025-08-04T13:29:26Z",
      "authors": [
        "Chenglin Cui",
        "Chaoran Zhu",
        "Changjae Oh",
        "Andrea Cavallaro"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.02405v1",
      "relevance_score": 4
    },
    {
      "title": "Tactile Gesture Recognition with Built-in Joint Sensors for Industrial\n  Robots",
      "link": "https://arxiv.org/abs/2508.12435v1",
      "pdf_link": "https://arxiv.org/pdf/2508.12435v1.pdf",
      "summary": "While gesture recognition using vision or robot skins is an active research\narea in Human-Robot Collaboration (HRC), this paper explores deep learning\nmethods relying solely on a robot's built-in joint sensors, eliminating the\nneed for external sensors. We evaluated various convolutional neural network\n(CNN) architectures and collected two datasets to study the impact of data\nrepresentation and model architecture on the recognition accuracy. Our results\nshow that spectrogram-based representations significantly improve accuracy,\nwhile model architecture plays a smaller role. We also tested generalization to\nnew robot poses, where spectrogram-based models performed better. Implemented\non a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,\nachieved over 95% accuracy in contact detection and gesture classification.\nThese findings demonstrate the feasibility of external-sensor-free tactile\nrecognition and promote further research toward cost-effective, scalable\nsolutions for HRC.",
      "published": "2025-08-17T17:04:58Z",
      "authors": [
        "Deqing Song",
        "Weimin Yang",
        "Maryam Rezayati",
        "Hans Wernher van de Venn"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.12435v1",
      "relevance_score": 4
    },
    {
      "title": "Embodied Tactile Perception of Soft Objects Properties",
      "link": "https://arxiv.org/abs/2508.09836v1",
      "pdf_link": "https://arxiv.org/pdf/2508.09836v1.pdf",
      "summary": "To enable robots to develop human-like fine manipulation, it is essential to\nunderstand how mechanical compliance, multi-modal sensing, and purposeful\ninteraction jointly shape tactile perception. In this study, we use a dedicated\nmodular e-Skin with tunable mechanical compliance and multi-modal sensing\n(normal, shear forces and vibrations) to systematically investigate how sensing\nembodiment and interaction strategies influence robotic perception of objects.\nLeveraging a curated set of soft wave objects with controlled viscoelastic and\nsurface properties, we explore a rich set of palpation primitives-pressing,\nprecession, sliding that vary indentation depth, frequency, and directionality.\nIn addition, we propose the latent filter, an unsupervised, action-conditioned\ndeep state-space model of the sophisticated interaction dynamics and infer\ncausal mechanical properties into a structured latent space. This provides\ngeneralizable and in-depth interpretable representation of how embodiment and\ninteraction determine and influence perception. Our investigation demonstrates\nthat multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced\ninteraction between the environment and mechanical properties of e-Skin, which\nshould be examined alongside the interaction by incorporating temporal\ndynamics.",
      "published": "2025-08-13T14:16:42Z",
      "authors": [
        "Anirvan Dutta",
        "Alexis WM Devillard",
        "Zhihuan Zhang",
        "Xiaoxiao Cheng",
        "Etienne Burdet"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.09836v1",
      "relevance_score": 4
    },
    {
      "title": "Robot and Overhead Crane Collaboration Scheme to Enhance Payload\n  Manipulation",
      "link": "https://arxiv.org/abs/2508.07758v1",
      "pdf_link": "https://arxiv.org/pdf/2508.07758v1.pdf",
      "summary": "This paper presents a scheme to enhance payload manipulation using a robot\ncollaborating with an overhead crane. In the current industrial practice, when\nthe crane's payload has to be accurately manipulated and located in a desired\nposition, the task becomes laborious and risky since the operators have to\nguide the fine motions of the payload by hand. In the proposed collaborative\nscheme, the crane lifts the payload while the robot's end-effector guides it\ntoward the desired position. The only link between the robot and the crane is\nthe interaction force produced during the guiding of the payload. Two\nadmittance transfer functions are considered to accomplish harmless and smooth\ncontact with the payload. The first is used in a position-based admittance\ncontrol integrated with the robot. The second one adds compliance to the crane\nby processing the interaction force through the admittance transfer function to\ngenerate a crane's velocity command that makes the crane follow the payload.\nThen the robot's end-effector and the crane move collaboratively to guide the\npayload to the desired location. A method is presented to design the admittance\ncontrollers that accomplish a fluent robot-crane collaboration. Simulations and\nexperiments validating the scheme potential are shown.",
      "published": "2025-08-11T08:41:13Z",
      "authors": [
        "Antonio Rosales",
        "Alaa Abderrahim",
        "Markku Suomalainen",
        "Mikael Haag",
        "Tapio Heikkilä"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.07758v1",
      "relevance_score": 4
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}