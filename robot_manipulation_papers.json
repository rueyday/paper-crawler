{
  "last_updated": "2025-08-11T08:33:18.275467",
  "total_papers": 25,
  "papers": [
    {
      "title": "TacMan-Turbo: Proactive Tactile Control for Robust and Efficient\n  Articulated Object Manipulation",
      "link": "https://arxiv.org/abs/2508.02204v1",
      "pdf_link": "https://arxiv.org/pdf/2508.02204v1.pdf",
      "summary": "Adept manipulation of articulated objects is essential for robots to operate\nsuccessfully in human environments. Such manipulation requires both\neffectiveness -- reliable operation despite uncertain object structures -- and\nefficiency -- swift execution with minimal redundant steps and smooth actions.\nExisting approaches struggle to achieve both objectives simultaneously: methods\nrelying on predefined kinematic models lack effectiveness when encountering\nstructural variations, while tactile-informed approaches achieve robust\nmanipulation without kinematic priors but compromise efficiency through\nreactive, step-by-step exploration-compensation cycles. This paper introduces\nTacMan-Turbo, a novel proactive tactile control framework for articulated\nobject manipulation that resolves this fundamental trade-off. Unlike previous\napproaches that treat tactile contact deviations merely as error signals\nrequiring compensation, our method interprets these deviations as rich sources\nof local kinematic information. This new perspective enables our controller to\npredict optimal future interactions and make proactive adjustments,\nsignificantly enhancing manipulation efficiency. In comprehensive evaluations\nacross 200 diverse simulated articulated objects and real-world experiments,\nour approach maintains a 100% success rate while significantly outperforming\nthe previous tactile-informed method in time efficiency, action efficiency, and\ntrajectory smoothness (all p-values < 0.0001). These results demonstrate that\nthe long-standing trade-off between effectiveness and efficiency in articulated\nobject manipulation can be successfully resolved without relying on prior\nkinematic knowledge.",
      "published": "2025-08-04T08:53:44Z",
      "authors": [
        "Zihang Zhao",
        "Zhenghao Qi",
        "Yuyang Li",
        "Leiyao Cui",
        "Zhi Han",
        "Lecheng Ruan",
        "Yixin Zhu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.02204v1",
      "relevance_score": 10
    },
    {
      "title": "UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation\n  Strategy and Dataset for Diverse Dexterous Hands",
      "link": "https://arxiv.org/abs/2508.03339v1",
      "pdf_link": "https://arxiv.org/pdf/2508.03339v1.pdf",
      "summary": "Dexterous grasp datasets are vital for embodied intelligence, but mostly\nemphasize grasp stability, ignoring functional grasps needed for tasks like\nopening bottle caps or holding cup handles. Most rely on bulky, costly, and\nhard-to-control high-DOF Shadow Hands. Inspired by the human hand's\nunderactuated mechanism, we establish UniFucGrasp, a universal functional grasp\nannotation strategy and dataset for multiple dexterous hand types. Based on\nbiomimicry, it maps natural human motions to diverse hand structures and uses\ngeometry-based force closure to ensure functional, stable, human-like grasps.\nThis method supports low-cost, efficient collection of diverse, high-quality\nfunctional grasps. Finally, we establish the first multi-hand functional grasp\ndataset and provide a synthesis model to validate its effectiveness.\nExperiments on the UFG dataset, IsaacSim, and complex robotic tasks show that\nour method improves functional manipulation accuracy and grasp stability,\nenables efficient generalization across diverse robotic hands, and overcomes\nannotation cost and generalization challenges in dexterous grasping. The\nproject page is at https://haochen611.github.io/UFG.",
      "published": "2025-08-05T11:37:38Z",
      "authors": [
        "Haoran Lin",
        "Wenrui Chen",
        "Xianchi Chen",
        "Fan Yang",
        "Qiang Diao",
        "Wenxin Xie",
        "Sijie Wu",
        "Kailun Yang",
        "Maojun Li",
        "Yaonan Wang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "eess.IV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.03339v1",
      "relevance_score": 8
    },
    {
      "title": "CleanUpBench: Embodied Sweeping and Grasping Benchmark",
      "link": "https://arxiv.org/abs/2508.05543v1",
      "pdf_link": "https://arxiv.org/pdf/2508.05543v1.pdf",
      "summary": "Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,\nbut most target complex humanoid agents or large-scale simulations that are far\nfrom real-world deployment. In contrast, mobile cleaning robots with dual mode\ncapabilities, such as sweeping and grasping, are rapidly emerging as realistic\nand commercially viable platforms. However, no benchmark currently exists that\nsystematically evaluates these agents in structured, multi-target cleaning\ntasks, revealing a critical gap between academic research and real-world\napplications. We introduce CleanUpBench, a reproducible and extensible\nbenchmark for evaluating embodied agents in realistic indoor cleaning\nscenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service\nrobot equipped with a sweeping mechanism and a six-degree-of-freedom robotic\narm, enabling interaction with heterogeneous objects. The benchmark includes\nmanually designed environments and one procedurally generated layout to assess\ngeneralization, along with a comprehensive evaluation suite covering task\ncompletion, spatial efficiency, motion quality, and control performance. To\nsupport comparative studies, we provide baseline agents based on heuristic\nstrategies and map-based planning. CleanUpBench bridges the gap between\nlow-level skill evaluation and full-scene testing, offering a scalable testbed\nfor grounded, embodied intelligence in everyday settings.",
      "published": "2025-08-07T16:20:31Z",
      "authors": [
        "Wenbo Li",
        "Guanting Chen",
        "Tao Zhao",
        "Jiyao Wang",
        "Tianxin Hu",
        "Yuwen Liao",
        "Weixiang Guo",
        "Shenghai Yuan"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.05543v1",
      "relevance_score": 7
    },
    {
      "title": "A Vision-Based Collision Sensing Method for Stable Circular Object\n  Grasping with A Soft Gripper System",
      "link": "https://arxiv.org/abs/2508.05040v1",
      "pdf_link": "https://arxiv.org/pdf/2508.05040v1.pdf",
      "summary": "External collisions to robot actuators typically pose risks to grasping\ncircular objects. This work presents a vision-based sensing module capable of\ndetecting collisions to maintain stable grasping with a soft gripper system.\nThe system employs an eye-in-palm camera with a broad field of view to\nsimultaneously monitor the motion of fingers and the grasped object.\nFurthermore, we have developed a collision-rich grasping strategy to ensure the\nstability and security of the entire dynamic grasping process. A physical soft\ngripper was manufactured and affixed to a collaborative robotic arm to evaluate\nthe performance of the collision detection mechanism. An experiment regarding\ntesting the response time of the mechanism confirmed the system has the\ncapability to react to the collision instantaneously. A dodging test was\nconducted to demonstrate the gripper can detect the direction and scale of\nexternal collisions precisely.",
      "published": "2025-08-07T05:43:45Z",
      "authors": [
        "Boyang Zhang",
        "Jiahui Zuo",
        "Zeyu Duan",
        "Fumin Zhang"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.05040v1",
      "relevance_score": 7
    },
    {
      "title": "Physics-informed Neural Time Fields for Prehensile Object Manipulation",
      "link": "https://arxiv.org/abs/2508.02976v1",
      "pdf_link": "https://arxiv.org/pdf/2508.02976v1.pdf",
      "summary": "Object manipulation skills are necessary for robots operating in various\ndaily-life scenarios, ranging from warehouses to hospitals. They allow the\nrobots to manipulate the given object to their desired arrangement in the\ncluttered environment. The existing approaches to solving object manipulations\nare either inefficient sampling based techniques, require expert\ndemonstrations, or learn by trial and error, making them less ideal for\npractical scenarios. In this paper, we propose a novel, multimodal\nphysics-informed neural network (PINN) for solving object manipulation tasks.\nOur approach efficiently learns to solve the Eikonal equation without expert\ndata and finds object manipulation trajectories fast in complex, cluttered\nenvironments. Our method is multimodal as it also reactively replans the\nrobot's grasps during manipulation to achieve the desired object poses. We\ndemonstrate our approach in both simulation and real-world scenarios and\ncompare it against state-of-the-art baseline methods. The results indicate that\nour approach is effective across various objects, has efficient training\ncompared to previous learning-based methods, and demonstrates high performance\nin planning time, trajectory length, and success rates. Our demonstration\nvideos can be found at https://youtu.be/FaQLkTV9knI.",
      "published": "2025-08-05T00:55:28Z",
      "authors": [
        "Hanwen Ren",
        "Ruiqi Ni",
        "Ahmed H. Qureshi"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.02976v1",
      "relevance_score": 7
    },
    {
      "title": "RAKOMO: Reachability-Aware K-Order Markov Path Optimization for\n  Quadrupedal Loco-Manipulation",
      "link": "https://arxiv.org/abs/2507.19652v1",
      "pdf_link": "https://arxiv.org/pdf/2507.19652v1.pdf",
      "summary": "Legged manipulators, such as quadrupeds equipped with robotic arms, require\nmotion planning techniques that account for their complex kinematic constraints\nin order to perform manipulation tasks both safely and effectively. However,\ntrajectory optimization methods often face challenges due to the hybrid\ndynamics introduced by contact discontinuities, and tend to neglect leg\nlimitations during planning for computational reasons. In this work, we propose\nRAKOMO, a path optimization technique that integrates the strengths of K-Order\nMarkov Optimization (KOMO) with a kinematically-aware criterion based on the\nreachable region defined as reachability margin. We leverage a neural-network\nto predict the margin and optimize it by incorporating it in the standard KOMO\nformulation. This approach enables rapid convergence of gradient-based motion\nplanning -- commonly tailored for continuous systems -- while adapting it\neffectively to legged manipulators, successfully executing loco-manipulation\ntasks. We benchmark RAKOMO against a baseline KOMO approach through a set of\nsimulations for pick-and-place tasks with the HyQReal quadruped robot equipped\nwith a Kinova Gen3 robotic arm.",
      "published": "2025-07-25T19:55:35Z",
      "authors": [
        "Mattia Risiglione",
        "Abdelrahman Abdalla",
        "Victor Barasuol",
        "Kim Tien Ly",
        "Ioannis Havoutis",
        "Claudio Semini"
      ],
      "categories": [
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.19652v1",
      "relevance_score": 7
    },
    {
      "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation\n  Learning",
      "link": "https://arxiv.org/abs/2508.00491v1",
      "pdf_link": "https://arxiv.org/pdf/2508.00491v1.pdf",
      "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation",
      "published": "2025-08-01T10:09:38Z",
      "authors": [
        "Carlo Alessi",
        "Federico Vasile",
        "Federico Ceola",
        "Giulia Pasquale",
        "Nicolò Boccardo",
        "Lorenzo Natale"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.00491v1",
      "relevance_score": 7
    },
    {
      "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark\n  towards General Grasping",
      "link": "https://arxiv.org/abs/2507.23734v1",
      "pdf_link": "https://arxiv.org/pdf/2507.23734v1.pdf",
      "summary": "General robotic grasping systems require accurate object affordance\nperception in diverse open-world scenarios following human instructions.\nHowever, current studies suffer from the problem of lacking reasoning-based\nlarge-scale affordance prediction data, leading to considerable concern about\nopen-world effectiveness. To address this limitation, we build a large-scale\ngrasping-oriented affordance segmentation benchmark with human-like\ninstructions, named RAGNet. It contains 273k images, 180 categories, and 26k\nreasoning instructions. The images cover diverse embodied data domains, such as\nwild, robot, ego-centric, and even simulation data. They are carefully\nannotated with an affordance map, while the difficulty of language instructions\nis largely increased by removing their category name and only providing\nfunctional descriptions. Furthermore, we propose a comprehensive\naffordance-based grasping framework, named AffordanceNet, which consists of a\nVLM pre-trained on our massive affordance data and a grasping network that\nconditions an affordance map to grasp the target. Extensive experiments on\naffordance segmentation benchmarks and real-robot manipulation tasks show that\nour model has a powerful open-world generalization ability. Our data and code\nis available at https://github.com/wudongming97/AffordanceNet.",
      "published": "2025-07-31T17:17:05Z",
      "authors": [
        "Dongming Wu",
        "Yanping Fu",
        "Saike Huang",
        "Yingfei Liu",
        "Fan Jia",
        "Nian Liu",
        "Feng Dai",
        "Tiancai Wang",
        "Rao Muhammad Anwer",
        "Fahad Shahbaz Khan",
        "Jianbing Shen"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.23734v1",
      "relevance_score": 7
    },
    {
      "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable\n  Visuo-Tactile Gripper",
      "link": "https://arxiv.org/abs/2507.15062v1",
      "pdf_link": "https://arxiv.org/pdf/2507.15062v1.pdf",
      "summary": "Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .",
      "published": "2025-07-20T17:53:59Z",
      "authors": [
        "Xinyue Zhu",
        "Binghao Huang",
        "Yunzhu Li"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.15062v1",
      "relevance_score": 7
    },
    {
      "title": "A Segmented Robot Grasping Perception Neural Network for Edge AI",
      "link": "https://arxiv.org/abs/2507.13970v2",
      "pdf_link": "https://arxiv.org/pdf/2507.13970v2.pdf",
      "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "published": "2025-07-18T14:32:45Z",
      "authors": [
        "Casper Bröcheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico Möckel"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.13970v2",
      "relevance_score": 7
    },
    {
      "title": "Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts",
      "link": "https://arxiv.org/abs/2508.05937v1",
      "pdf_link": "https://arxiv.org/pdf/2508.05937v1.pdf",
      "summary": "Robotic non-destructive disassembly of mating parts remains challenging due\nto the need for flexible manipulation and the limited visibility of internal\nstructures. This study presents an affordance-guided teleoperation system that\nenables intuitive human demonstrations for dual-arm fix-and-disassemble tasks\nfor mating parts. The system visualizes feasible grasp poses and disassembly\ndirections in a virtual environment, both derived from the object's geometry,\nto address occlusions and structural complexity. To prevent excessive position\ntracking under load when following the affordance, we integrate a hybrid\ncontroller that combines position and impedance control into the teleoperated\ndisassembly arm. Real-world experiments validate the effectiveness of the\nproposed system, showing improved task success rates and reduced object pose\ndeviation.",
      "published": "2025-08-08T02:03:12Z",
      "authors": [
        "Gen Sako",
        "Takuya Kiyokawa",
        "Kensuke Harada",
        "Tomoki Ishikura",
        "Naoya Miyaji",
        "Genichiro Matsuda"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.05937v1",
      "relevance_score": 6
    },
    {
      "title": "Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot\n  Context-Aware Grasping",
      "link": "https://arxiv.org/abs/2508.03099v1",
      "pdf_link": "https://arxiv.org/pdf/2508.03099v1.pdf",
      "summary": "We propose Point2Act, which directly retrieves the 3D action point relevant\nfor a contextually described task, leveraging Multimodal Large Language Models\n(MLLMs). Foundation models opened the possibility for generalist robots that\ncan perform a zero-shot task following natural language descriptions within an\nunseen environment. While the semantics obtained from large-scale image and\nlanguage datasets provide contextual understanding in 2D images, the rich yet\nnuanced features deduce blurry 2D regions and struggle to find precise 3D\nlocations for actions. Our proposed 3D relevancy fields bypass the\nhigh-dimensional features and instead efficiently imbue lightweight 2D\npoint-level guidance tailored to the task-specific action. The multi-view\naggregation effectively compensates for misalignments due to geometric\nambiguities, such as occlusion, or semantic uncertainties inherent in the\nlanguage descriptions. The output region is highly localized, reasoning\nfine-grained 3D spatial context that can directly transfer to an explicit\nposition for physical action at the on-the-fly reconstruction of the scene. Our\nfull-stack pipeline, which includes capturing, MLLM querying, 3D\nreconstruction, and grasp pose extraction, generates spatially grounded\nresponses in under 20 seconds, facilitating practical manipulation tasks.\nProject page: https://sangminkim-99.github.io/point2act/",
      "published": "2025-08-05T05:23:19Z",
      "authors": [
        "Sang Min Kim",
        "Hyeongjun Heo",
        "Junho Kim",
        "Yonghyeon Lee",
        "Young Min Kim"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.03099v1",
      "relevance_score": 6
    },
    {
      "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation\n  Model Reasoning and Part Grounding",
      "link": "https://arxiv.org/abs/2507.18276v1",
      "pdf_link": "https://arxiv.org/pdf/2507.18276v1.pdf",
      "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.",
      "published": "2025-07-24T10:25:58Z",
      "authors": [
        "Xiaojie Zhang",
        "Yuanfei Wang",
        "Ruihai Wu",
        "Kunqi Xu",
        "Yu Li",
        "Liuyu Xiang",
        "Hao Dong",
        "Zhaofeng He"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.18276v1",
      "relevance_score": 6
    },
    {
      "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for\n  Policy Reasoning and Dual Robotic Control",
      "link": "https://arxiv.org/abs/2508.05342v1",
      "pdf_link": "https://arxiv.org/pdf/2508.05342v1.pdf",
      "summary": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations.",
      "published": "2025-08-07T12:48:09Z",
      "authors": [
        "Shunlei Li",
        "Longsen Gao",
        "Jin Wang",
        "Chang Che",
        "Xi Xiao",
        "Jiuwen Cao",
        "Yingbai Hu",
        "Hamid Reza Karimi"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.05342v1",
      "relevance_score": 5
    },
    {
      "title": "CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation",
      "link": "https://arxiv.org/abs/2508.03526v1",
      "pdf_link": "https://arxiv.org/pdf/2508.03526v1.pdf",
      "summary": "A central research topic in robotics is how to use this system to interact\nwith the physical world. Traditional manipulation tasks primarily focus on\nsmall objects. However, in factory or home environments, there is often a need\nfor the movement of large objects, such as moving tables. These tasks typically\nrequire multi-robot systems to work collaboratively. Previous research lacks a\nframework that can scale to arbitrary sizes of robots and generalize to various\nkinds of tasks. In this work, we propose CollaBot, a generalist framework for\nsimultaneous collaborative manipulation. First, we use SEEM for scene\nsegmentation and point cloud extraction of the target object. Then, we propose\na collaborative grasping framework, which decomposes the task into local grasp\npose generation and global collaboration. Finally, we design a 2-stage planning\nmodule that can generate collision-free trajectories to achieve this task.\nExperiments show a success rate of 52% across different numbers of robots,\nobjects, and tasks, indicating the effectiveness of the proposed framework.",
      "published": "2025-08-05T14:57:37Z",
      "authors": [
        "Kun Song",
        "Shentao Ma",
        "Gaoming Chen",
        "Ninglong Jin",
        "Guangbao Zhao",
        "Mingyu Ding",
        "Zhenhua Xiong",
        "Jia Pan"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.03526v1",
      "relevance_score": 5
    },
    {
      "title": "Manip4Care: Robotic Manipulation of Human Limbs for Solving Assistive\n  Tasks",
      "link": "https://arxiv.org/abs/2508.02649v1",
      "pdf_link": "https://arxiv.org/pdf/2508.02649v1.pdf",
      "summary": "Enabling robots to grasp and reposition human limbs can significantly enhance\ntheir ability to provide assistive care to individuals with severe mobility\nimpairments, particularly in tasks such as robot-assisted bed bathing and\ndressing. However, existing assistive robotics solutions often assume that the\nhuman remains static or quasi-static, limiting their effectiveness. To address\nthis issue, we present Manip4Care, a modular simulation pipeline that enables\nrobotic manipulators to grasp and reposition human limbs effectively. Our\napproach features a physics simulator equipped with built-in techniques for\ngrasping and repositioning while considering biomechanical and collision\navoidance constraints. Our grasping method employs antipodal sampling with\nforce closure to grasp limbs, and our repositioning system utilizes the Model\nPredictive Path Integral (MPPI) and vector-field-based control method to\ngenerate motion trajectories under collision avoidance and biomechanical\nconstraints. We evaluate this approach across various limb manipulation tasks\nin both supine and sitting positions and compare outcomes for different age\ngroups with differing shoulder joint limits. Additionally, we demonstrate our\napproach for limb manipulation using a real-world mannequin and further\nshowcase its effectiveness in bed bathing tasks.",
      "published": "2025-08-04T17:41:58Z",
      "authors": [
        "Yubin Koh",
        "Ahmed H. Qureshi"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.02649v1",
      "relevance_score": 5
    },
    {
      "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
      "link": "https://arxiv.org/abs/2507.12768v1",
      "pdf_link": "https://arxiv.org/pdf/2507.12768v1.pdf",
      "summary": "Vision-language-action (VLA) models have shown promise on task-conditioned\ncontrol in complex settings such as bimanual manipulation. However, the heavy\nreliance on task-specific human demonstrations limits their generalization and\nincurs high data acquisition costs. In this work, we present a new notion of\ntask-agnostic action paradigm that decouples action execution from\ntask-specific conditioning, enhancing scalability, efficiency, and\ncost-effectiveness. To address the data collection challenges posed by this\nparadigm -- such as low coverage density, behavioral redundancy, and safety\nrisks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a\nscalable self-supervised framework that accelerates collection by over $\n30\\times $ compared to human teleoperation. To further enable effective\nlearning from task-agnostic data, which often suffers from distribution\nmismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics\nmodel equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder\n(DAD). We additionally integrate a video-conditioned action validation module\nto verify the feasibility of learned policies across diverse manipulation\ntasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%\nimprovement in test accuracy and achieves 30-40% higher success rates in\ndownstream tasks such as lifting, pick-and-place, and clicking, using\nreplay-based video validation. Project Page:\nhttps://embodiedfoundation.github.io/vidar_anypos",
      "published": "2025-07-17T03:48:57Z",
      "authors": [
        "Hengkai Tan",
        "Yao Feng",
        "Xinyi Mao",
        "Shuhe Huang",
        "Guodong Liu",
        "Zhongkai Hao",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12768v1",
      "relevance_score": 5
    },
    {
      "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware\n  Diffusers",
      "link": "https://arxiv.org/abs/2507.11287v1",
      "pdf_link": "https://arxiv.org/pdf/2507.11287v1.pdf",
      "summary": "In this paper, we study task-oriented human grasp synthesis, a new grasp\nsynthesis task that demands both task and context awareness. At the core of our\nmethod is the task-aware contact maps. Unlike traditional contact maps that\nonly reason about the manipulated object and its relation with the hand, our\nenhanced maps take into account scene and task information. This comprehensive\nmap is critical for hand-object interaction, enabling accurate grasping poses\nthat align with the task. We propose a two-stage pipeline that first constructs\na task-aware contact map informed by the scene and task. In the subsequent\nstage, we use this contact map to synthesize task-oriented human grasps. We\nintroduce a new dataset and a metric for the proposed task to evaluate our\napproach. Our experiments validate the importance of modeling both scene and\ntask, demonstrating significant improvements over existing methods in both\ngrasp quality and task performance. See our project page for more details:\nhttps://hcis-lab.github.io/TOHGS/",
      "published": "2025-07-15T13:11:55Z",
      "authors": [
        "An-Lun Liu",
        "Yu-Wei Chao",
        "Yi-Ting Chen"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.11287v1",
      "relevance_score": 5
    },
    {
      "title": "Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with\n  Learned Contact Residuals",
      "link": "https://arxiv.org/abs/2508.03003v1",
      "pdf_link": "https://arxiv.org/pdf/2508.03003v1.pdf",
      "summary": "Husky Carbon, a robot developed by Northeastern University, serves as a\nresearch platform to explore unification of posture manipulation and thrust\nvectoring. Unlike conventional quadrupeds, its joint actuators and thrusters\nenable enhanced control authority, facilitating thruster-assisted narrow-path\nwalking. While a unified Model Predictive Control (MPC) framework optimizing\nboth ground reaction forces and thruster forces could theoretically address\nthis control problem, its feasibility is limited by the low torque-control\nbandwidth of the system's lightweight actuators. To overcome this challenge, we\npropose a decoupled control architecture: a Raibert-type controller governs\nlegged locomotion using position-based control, while an MPC regulates the\nthrusters augmented by learned Contact Residual Dynamics (CRD) to account for\nleg-ground impacts. This separation bypasses the torque-control rate bottleneck\nwhile retaining the thruster MPC to explicitly account for leg-ground impact\ndynamics through learned residuals. We validate this approach through both\nsimulation and hardware experiments, showing that the decoupled control\narchitecture with CRD performs more stable behavior in terms of push recovery\nand cat-like walking gait compared to the decoupled controller without CRD.",
      "published": "2025-08-05T02:19:49Z",
      "authors": [
        "Chenghao Wang",
        "Alireza Ramezani"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.03003v1",
      "relevance_score": 4
    },
    {
      "title": "Improving Generalization of Language-Conditioned Robot Manipulation",
      "link": "https://arxiv.org/abs/2508.02405v1",
      "pdf_link": "https://arxiv.org/pdf/2508.02405v1.pdf",
      "summary": "The control of robots for manipulation tasks generally relies on visual\ninput. Recent advances in vision-language models (VLMs) enable the use of\nnatural language instructions to condition visual input and control robots in a\nwider range of environments. However, existing methods require a large amount\nof data to fine-tune VLMs for operating in unseen environments. In this paper,\nwe present a framework that learns object-arrangement tasks from just a few\ndemonstrations. We propose a two-stage framework that divides\nobject-arrangement tasks into a target localization stage, for picking the\nobject, and a region determination stage for placing the object. We present an\ninstance-level semantic fusion module that aligns the instance-level image\ncrops with the text embedding, enabling the model to identify the target\nobjects defined by the natural language instructions. We validate our method on\nboth simulation and real-world robotic environments. Our method, fine-tuned\nwith a few demonstrations, improves generalization capability and demonstrates\nzero-shot ability in real-robot manipulation scenarios.",
      "published": "2025-08-04T13:29:26Z",
      "authors": [
        "Chenglin Cui",
        "Chaoran Zhu",
        "Changjae Oh",
        "Andrea Cavallaro"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.02405v1",
      "relevance_score": 4
    },
    {
      "title": "Improving Tactile Gesture Recognition with Optical Flow",
      "link": "https://arxiv.org/abs/2508.04338v1",
      "pdf_link": "https://arxiv.org/pdf/2508.04338v1.pdf",
      "summary": "Tactile gesture recognition systems play a crucial role in Human-Robot\nInteraction (HRI) by enabling intuitive communication between humans and\nrobots. The literature mainly addresses this problem by applying machine\nlearning techniques to classify sequences of tactile images encoding the\npressure distribution generated when executing the gestures. However, some\ngestures can be hard to differentiate based on the information provided by\ntactile images alone. In this paper, we present a simple yet effective way to\nimprove the accuracy of a gesture recognition classifier. Our approach focuses\nsolely on processing the tactile images used as input by the classifier. In\nparticular, we propose to explicitly highlight the dynamics of the contact in\nthe tactile image by computing the dense optical flow. This additional\ninformation makes it easier to distinguish between gestures that produce\nsimilar tactile images but exhibit different contact dynamics. We validate the\nproposed approach in a tactile gesture recognition task, showing that a\nclassifier trained on tactile images augmented with optical flow information\nachieved a 9% improvement in gesture classification accuracy compared to one\ntrained on standard tactile images.",
      "published": "2025-08-06T11:33:21Z",
      "authors": [
        "Shaohong Zhong",
        "Alessandro Albini",
        "Giammarco Caroleo",
        "Giorgio Cannata",
        "Perla Maiolino"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.04338v1",
      "relevance_score": 4
    },
    {
      "title": "Learning Pivoting Manipulation with Force and Vision Feedback Using\n  Optimization-based Demonstrations",
      "link": "https://arxiv.org/abs/2508.01082v2",
      "pdf_link": "https://arxiv.org/pdf/2508.01082v2.pdf",
      "summary": "Non-prehensile manipulation is challenging due to complex contact\ninteractions between objects, the environment, and robots. Model-based\napproaches can efficiently generate complex trajectories of robots and objects\nunder contact constraints. However, they tend to be sensitive to model\ninaccuracies and require access to privileged information (e.g., object mass,\nsize, pose), making them less suitable for novel objects. In contrast,\nlearning-based approaches are typically more robust to modeling errors but\nrequire large amounts of data. In this paper, we bridge these two approaches to\npropose a framework for learning closed-loop pivoting manipulation. By\nleveraging computationally efficient Contact-Implicit Trajectory Optimization\n(CITO), we design demonstration-guided deep Reinforcement Learning (RL),\nleading to sample-efficient learning. We also present a sim-to-real transfer\napproach using a privileged training strategy, enabling the robot to perform\npivoting manipulation using only proprioception, vision, and force sensing\nwithout access to privileged information. Our method is evaluated on several\npivoting tasks, demonstrating that it can successfully perform sim-to-real\ntransfer. The overview of our method and the hardware experiments are shown at\nhttps://youtu.be/akjGDgfwLbM?si=QVw6ExoPy2VsU2g6",
      "published": "2025-08-01T21:33:46Z",
      "authors": [
        "Yuki Shirai",
        "Kei Ota",
        "Devesh K. Jha",
        "Diego Romeres"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.01082v2",
      "relevance_score": 4
    },
    {
      "title": "Vidar: Embodied Video Diffusion Model for Generalist Bimanual\n  Manipulation",
      "link": "https://arxiv.org/abs/2507.12898v2",
      "pdf_link": "https://arxiv.org/pdf/2507.12898v2.pdf",
      "summary": "Bimanual robotic manipulation, which involves the coordinated control of two\nrobotic arms, is foundational for solving challenging tasks. Despite recent\nprogress in general-purpose manipulation, data scarcity and embodiment\nheterogeneity remain serious obstacles to further scaling up in bimanual\nsettings. In this paper, we introduce Video Diffusion for Action Reasoning\n(Vidar), a two-stage framework that leverages large-scale, diffusion-based\nvideo pre-training and a novel masked inverse dynamics model for action\nprediction. We pre-train the video diffusion model on 750K multi-view videos\nfrom three real-world bimanual robot platforms, utilizing a unified observation\nspace that encodes robot, camera, task, and scene contexts. Our masked inverse\ndynamics model learns masks to extract action-relevant information from\ngenerated trajectories without requiring pixel-level labels, and the masks can\neffectively generalize to unseen backgrounds. Our experiments demonstrate that\nwith only 20 minutes of human demonstrations on an unseen robot platform (only\n1% of typical data requirements), Vidar generalizes to unseen tasks and\nbackgrounds with strong semantic understanding, surpassing state-of-the-art\nmethods. Our findings highlight the potential of video foundation models,\ncoupled with masked action prediction, to enable scalable and generalizable\nrobotic manipulation in diverse real-world settings.",
      "published": "2025-07-17T08:31:55Z",
      "authors": [
        "Yao Feng",
        "Hengkai Tan",
        "Xinyi Mao",
        "Guodong Liu",
        "Shuhe Huang",
        "Chendong Xiang",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12898v2",
      "relevance_score": 4
    },
    {
      "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active\n  Instance-Level Object Understanding",
      "link": "https://arxiv.org/abs/2507.10776v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10776v1.pdf",
      "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.",
      "published": "2025-07-14T20:02:52Z",
      "authors": [
        "Howard H. Qian",
        "Yiting Chen",
        "Gaotian Wang",
        "Podshara Chanrungmaneekul",
        "Kaiyu Hang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10776v1",
      "relevance_score": 4
    },
    {
      "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An\n  Evaluation with Human-Inspired Constraints",
      "link": "https://arxiv.org/abs/2507.10131v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10131v1.pdf",
      "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.",
      "published": "2025-07-14T10:21:27Z",
      "authors": [
        "Cesar Alan Contreras",
        "Manolis Chiou",
        "Alireza Rastegarpanah",
        "Michal Szulik",
        "Rustam Stolkin"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10131v1",
      "relevance_score": 4
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}