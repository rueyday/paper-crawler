{
  "last_updated": "2025-10-20T08:28:39.977894",
  "total_papers": 25,
  "papers": [
    {
      "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation",
      "link": "https://arxiv.org/abs/2510.14647v1",
      "pdf_link": "https://arxiv.org/pdf/2510.14647v1.pdf",
      "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage.",
      "published": "2025-10-16T12:59:34Z",
      "authors": [
        "Jialei Huang",
        "Yang Ye",
        "Yuanqing Gong",
        "Xuezhou Zhu",
        "Yang Gao",
        "Kaifeng Zhang"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.14647v1",
      "relevance_score": 10
    },
    {
      "title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial\n  Transformation for Cross-Embodiment Dexterous Grasping",
      "link": "https://arxiv.org/abs/2510.12724v1",
      "pdf_link": "https://arxiv.org/pdf/2510.12724v1.pdf",
      "summary": "Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping.",
      "published": "2025-10-14T17:06:00Z",
      "authors": [
        "Xin Fei",
        "Zhixuan Xu",
        "Huaicong Fang",
        "Tianrui Zhang",
        "Lin Shao"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.12724v1",
      "relevance_score": 10
    },
    {
      "title": "AVO: Amortized Value Optimization for Contact Mode Switching in\n  Multi-Finger Manipulation",
      "link": "https://arxiv.org/abs/2510.07548v1",
      "pdf_link": "https://arxiv.org/pdf/2510.07548v1.pdf",
      "summary": "Dexterous manipulation tasks often require switching between different\ncontact modes, such as rolling, sliding, sticking, or non-contact contact\nmodes. When formulating dexterous manipulation tasks as a trajectory\noptimization problem, a common approach is to decompose these tasks into\nsub-tasks for each contact mode, which are each solved independently.\nOptimizing each sub-task independently can limit performance, as optimizing\ncontact points, contact forces, or other variables without information about\nfuture sub-tasks can place the system in a state from which it is challenging\nto make progress on subsequent sub-tasks. Further, optimizing these sub-tasks\nis very computationally expensive. To address these challenges, we propose\nAmortized Value Optimization (AVO), which introduces a learned value function\nthat predicts the total future task performance. By incorporating this value\nfunction into the cost of the trajectory optimization at each planning step,\nthe value function gradients guide the optimizer toward states that minimize\nthe cost in future sub-tasks. This effectively bridges separately optimized\nsub-tasks, and accelerates the optimization by reducing the amount of online\ncomputation needed. We validate AVO on a screwdriver grasping and turning task\nin both simulation and real world experiments, and show improved performance\neven with 50% less computational budget compared to trajectory optimization\nwithout the value function.",
      "published": "2025-10-08T21:03:14Z",
      "authors": [
        "Adam Hung",
        "Fan Yang",
        "Abhinav Kumar",
        "Sergio Aguilera Marinovic",
        "Soshi Iba",
        "Rana Soltani Zarrin",
        "Dmitry Berenson"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.07548v1",
      "relevance_score": 10
    },
    {
      "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place\n  Reasoning",
      "link": "https://arxiv.org/abs/2510.14584v1",
      "pdf_link": "https://arxiv.org/pdf/2510.14584v1.pdf",
      "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors.",
      "published": "2025-10-16T11:43:02Z",
      "authors": [
        "Benno Wingender",
        "Nils Dengler",
        "Rohit Menon",
        "Sicong Pan",
        "Maren Bennewitz"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.14584v1",
      "relevance_score": 9
    },
    {
      "title": "A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for\n  Scooping and Self-Adaptive Grasping in Environmental Constraints",
      "link": "https://arxiv.org/abs/2510.13535v1",
      "pdf_link": "https://arxiv.org/pdf/2510.13535v1.pdf",
      "summary": "This paper presents a novel underactuated adaptive robotic hand, Hockens-A\nHand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,\nand a specialized four-bar linkage to achieve three adaptive grasping modes:\nparallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand\nrequires only a single linear actuator, leveraging passive mechanical\nintelligence to ensure adaptability and compliance in unstructured\nenvironments. Specifically, the vertical motion of the Hoeckens mechanism\nintroduces compliance, the double-parallelogram linkage ensures line contact at\nthe fingertip, and the four-bar amplification system enables natural\ntransitions between different grasping modes. Additionally, the inclusion of a\nmesh-textured silicone phalanx further enhances the ability to envelop objects\nof various shapes and sizes. This study employs detailed kinematic analysis to\noptimize the push angle and design the linkage lengths for optimal performance.\nSimulations validated the design by analyzing the fingertip motion and ensuring\nsmooth transitions between grasping modes. Furthermore, the grasping force was\nanalyzed using power equations to enhance the understanding of the system's\nperformance.Experimental validation using a 3D-printed prototype demonstrates\nthe three grasping modes of the hand in various scenarios under environmental\nconstraints, verifying its grasping stability and broad applicability.",
      "published": "2025-10-15T13:27:46Z",
      "authors": [
        "Wentao Guo",
        "Yizhou Wang",
        "Wenzeng Zhang"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.13535v1",
      "relevance_score": 9
    },
    {
      "title": "Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and\n  Self-Adaptive Grasping",
      "link": "https://arxiv.org/abs/2510.13553v2",
      "pdf_link": "https://arxiv.org/pdf/2510.13553v2.pdf",
      "summary": "This paper presents the Hoecken-D Hand, an underactuated robotic gripper that\ncombines a modified Hoecken linkage with a differential spring mechanism to\nachieve both linear parallel pinching and a mid-stroke transition to adaptive\nenvelope. The original Hoecken linkage is reconfigured by replacing one member\nwith differential links, preserving straight-line guidance while enabling\ncontact-triggered reconfiguration without additional actuators. A\ndouble-parallelogram arrangement maintains fingertip parallelism during\nconventional pinching, whereas the differential mechanism allows one finger to\nwrap inward upon encountering an obstacle, improving stability on irregular or\nthin objects. The mechanism can be driven by a single linear actuator,\nminimizing complexity and cost; in our prototype, each finger is driven by its\nown linear actuator for simplicity. We perform kinematic modeling and force\nanalysis to characterize grasp performance, including simulated grasping forces\nand spring-opening behavior under varying geometric parameters. The design was\nprototyped using PLA-based 3D printing, achieving a linear pinching span of\napproximately 200 mm. Preliminary tests demonstrate reliable grasping in both\nmodes across a wide range of object geometries, highlighting the Hoecken-D Hand\nas a compact, adaptable, and cost-effective solution for manipulation in\nunstructured environments.",
      "published": "2025-10-15T13:49:02Z",
      "authors": [
        "Wentao Guo",
        "Wenzeng Zhang"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.13553v2",
      "relevance_score": 8
    },
    {
      "title": "Tactile-Conditioned Diffusion Policy for Force-Aware Robotic\n  Manipulation",
      "link": "https://arxiv.org/abs/2510.13324v1",
      "pdf_link": "https://arxiv.org/pdf/2510.13324v1.pdf",
      "summary": "Contact-rich manipulation depends on applying the correct grasp forces\nthroughout the manipulation task, especially when handling fragile or\ndeformable objects. Most existing imitation learning approaches often treat\nvisuotactile feedback only as an additional observation, leaving applied forces\nas an uncontrolled consequence of gripper commands. In this work, we present\nForce-Aware Robotic Manipulation (FARM), an imitation learning framework that\nintegrates high-dimensional tactile data to infer tactile-conditioned force\nsignals, which in turn define a matching force-based action space. We collect\nhuman demonstrations using a modified version of the handheld Universal\nManipulation Interface (UMI) gripper that integrates a GelSight Mini visual\ntactile sensor. For deploying the learned policies, we developed an actuated\nvariant of the UMI gripper with geometry matching our handheld version. During\npolicy rollouts, the proposed FARM diffusion policy jointly predicts robot\npose, grip width, and grip force. FARM outperforms several baselines across\nthree tasks with distinct force requirements -- high-force, low-force, and\ndynamic force adaptation -- demonstrating the advantages of its two key\ncomponents: leveraging force-grounded, high-dimensional tactile observations\nand a force-based control space. The codebase and design files are open-sourced\nand available at https://tactile-farm.github.io .",
      "published": "2025-10-15T09:11:14Z",
      "authors": [
        "Erik Helmut",
        "Niklas Funk",
        "Tim Schneider",
        "Cristiana de Farias",
        "Jan Peters"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.13324v1",
      "relevance_score": 8
    },
    {
      "title": "DexCanvas: Bridging Human Demonstrations and Robot Learning for\n  Dexterous Manipulation",
      "link": "https://arxiv.org/abs/2510.15786v1",
      "pdf_link": "https://arxiv.org/pdf/2510.15786v1.pdf",
      "summary": "We present DexCanvas, a large-scale hybrid real-synthetic human manipulation\ndataset containing 7,000 hours of dexterous hand-object interactions seeded\nfrom 70 hours of real human demonstrations, organized across 21 fundamental\nmanipulation types based on the Cutkosky taxonomy. Each entry combines\nsynchronized multi-view RGB-D, high-precision mocap with MANO hand parameters,\nand per-frame contact points with physically consistent force profiles. Our\nreal-to-sim pipeline uses reinforcement learning to train policies that control\nan actuated MANO hand in physics simulation, reproducing human demonstrations\nwhile discovering the underlying contact forces that generate the observed\nobject motion. DexCanvas is the first manipulation dataset to combine\nlarge-scale real demonstrations, systematic skill coverage based on established\ntaxonomies, and physics-validated contact annotations. The dataset can\nfacilitate research in robotic manipulation learning, contact-rich control, and\nskill transfer across different hand morphologies.",
      "published": "2025-10-17T16:08:14Z",
      "authors": [
        "Xinyue Xu",
        "Jieqiang Sun",
        " Jing",
        " Dai",
        "Siyuan Chen",
        "Lanjie Ma",
        "Ke Sun",
        "Bin Zhao",
        "Jianbo Yuan",
        "Yiwen Lu"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.15786v1",
      "relevance_score": 7
    },
    {
      "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation\n  Learning based Dexterous Manipulation",
      "link": "https://arxiv.org/abs/2510.14771v1",
      "pdf_link": "https://arxiv.org/pdf/2510.14771v1.pdf",
      "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL.",
      "published": "2025-10-16T15:07:18Z",
      "authors": [
        "Xu Chi",
        "Chao Zhang",
        "Yang Su",
        "Lingfeng Dou",
        "Fujia Yang",
        "Jiakuo Zhao",
        "Haoyu Zhou",
        "Xiaoyou Jia",
        "Yong Zhou",
        "Shan An"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.14771v1",
      "relevance_score": 7
    },
    {
      "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic\n  Recovery",
      "link": "https://arxiv.org/abs/2510.14768v1",
      "pdf_link": "https://arxiv.org/pdf/2510.14768v1.pdf",
      "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries.",
      "published": "2025-10-16T15:04:01Z",
      "authors": [
        "Fan Yang",
        "Zixuan Huang",
        "Abhinav Kumar",
        "Sergio Aguilera Marinovic",
        "Soshi Iba",
        "Rana Soltani Zarrin",
        "Dmitry Berenson"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.14768v1",
      "relevance_score": 7
    },
    {
      "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via\n  Simulation Fine-Tunin",
      "link": "https://arxiv.org/abs/2510.14930v1",
      "pdf_link": "https://arxiv.org/pdf/2510.14930v1.pdf",
      "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/.",
      "published": "2025-10-16T17:41:36Z",
      "authors": [
        "Binghao Huang",
        "Jie Xu",
        "Iretiayo Akinola",
        "Wei Yang",
        "Balakumar Sundaralingam",
        "Rowland O'Flaherty",
        "Dieter Fox",
        "Xiaolong Wang",
        "Arsalan Mousavian",
        "Yu-Wei Chao",
        "Yunzhu Li"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.14930v1",
      "relevance_score": 7
    },
    {
      "title": "Placeit! A Framework for Learning Robot Object Placement Skills",
      "link": "https://arxiv.org/abs/2510.09267v1",
      "pdf_link": "https://arxiv.org/pdf/2510.09267v1.pdf",
      "summary": "Robotics research has made significant strides in learning, yet mastering\nbasic skills like object placement remains a fundamental challenge. A key\nbottleneck is the acquisition of large-scale, high-quality data, which is often\na manual and laborious process. Inspired by Graspit!, a foundational work that\nused simulation to automatically generate dexterous grasp poses, we introduce\nPlaceit!, an evolutionary-computation framework for generating valid placement\npositions for rigid objects. Placeit! is highly versatile, supporting tasks\nfrom placing objects on tables to stacking and inserting them. Our experiments\nshow that by leveraging quality-diversity optimization, Placeit! significantly\noutperforms state-of-the-art methods across all scenarios for generating\ndiverse valid poses. A pick&place pipeline built on our framework achieved a\n90% success rate over 120 real-world deployments. This work positions Placeit!\nas a powerful tool for open-environment pick-and-place tasks and as a valuable\nengine for generating the data needed to train simulation-based foundation\nmodels in robotics.",
      "published": "2025-10-10T11:08:06Z",
      "authors": [
        "Amina Ferrad",
        "Johann Huber",
        "François Hélénon",
        "Julien Gleyze",
        "Mahdi Khoramshahi",
        "Stéphane Doncieux"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.09267v1",
      "relevance_score": 7
    },
    {
      "title": "DexMan: Learning Bimanual Dexterous Manipulation from Human and\n  Generated Videos",
      "link": "https://arxiv.org/abs/2510.08475v1",
      "pdf_link": "https://arxiv.org/pdf/2510.08475v1.pdf",
      "summary": "We present DexMan, an automated framework that converts human visual\ndemonstrations into bimanual dexterous manipulation skills for humanoid robots\nin simulation. Operating directly on third-person videos of humans manipulating\nrigid objects, DexMan eliminates the need for camera calibration, depth\nsensors, scanned 3D object assets, or ground-truth hand and object motion\nannotations. Unlike prior approaches that consider only simplified floating\nhands, it directly controls a humanoid robot and leverages novel contact-based\nrewards to improve policy learning from noisy hand-object poses estimated from\nin-the-wild videos.\n  DexMan achieves state-of-the-art performance in object pose estimation on the\nTACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.\nMeanwhile, its reinforcement learning policy surpasses previous methods by 19%\nin success rate on OakInk-v2. Furthermore, DexMan can generate skills from both\nreal and synthetic videos, without the need for manual data collection and\ncostly motion capture, and enabling the creation of large-scale, diverse\ndatasets for training generalist dexterous manipulation.",
      "published": "2025-10-09T17:17:05Z",
      "authors": [
        "Jhen Hsieh",
        "Kuan-Hsun Tu",
        "Kuo-Han Hung",
        "Tsung-Wei Ke"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.08475v1",
      "relevance_score": 7
    },
    {
      "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped\n  Approach For Adaptive Contact-Rich Manipulation",
      "link": "https://arxiv.org/abs/2510.14643v1",
      "pdf_link": "https://arxiv.org/pdf/2510.14643v1.pdf",
      "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations.",
      "published": "2025-10-16T12:55:28Z",
      "authors": [
        "Lara Brudermüller",
        "Brandon Hung",
        "Xinghao Zhu",
        "Jiuguang Wang",
        "Nick Hawes",
        "Preston Culbertson",
        "Simon Le Cleac'h"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.14643v1",
      "relevance_score": 6
    },
    {
      "title": "Learning to Grasp Anything by Playing with Random Toys",
      "link": "https://arxiv.org/abs/2510.12866v1",
      "pdf_link": "https://arxiv.org/pdf/2510.12866v1.pdf",
      "summary": "Robotic manipulation policies often struggle to generalize to novel objects,\nlimiting their real-world utility. In contrast, cognitive science suggests that\nchildren develop generalizable dexterous manipulation skills by mastering a\nsmall set of simple toys and then applying that knowledge to more complex\nitems. Inspired by this, we study if similar generalization capabilities can\nalso be achieved by robots. Our results indicate robots can learn generalizable\ngrasping using randomly assembled objects that are composed from just four\nshape primitives: spheres, cuboids, cylinders, and rings. We show that training\non these \"toys\" enables robust generalization to real-world objects, yielding\nstrong zero-shot performance. Crucially, we find the key to this generalization\nis an object-centric visual representation induced by our proposed detection\npooling mechanism. Evaluated in both simulation and on physical robots, our\nmodel achieves a 67% real-world grasping success rate on the YCB dataset,\noutperforming state-of-the-art approaches that rely on substantially more\nin-domain data. We further study how zero-shot generalization performance\nscales by varying the number and diversity of training toys and the\ndemonstrations per toy. We believe this work offers a promising path to\nscalable and generalizable learning in robotic manipulation. Demonstration\nvideos, code, checkpoints and our dataset are available on our project page:\nhttps://lego-grasp.github.io/ .",
      "published": "2025-10-14T17:56:10Z",
      "authors": [
        "Dantong Niu",
        "Yuvan Sharma",
        "Baifeng Shi",
        "Rachel Ding",
        "Matteo Gioia",
        "Haoru Xue",
        "Henry Tsai",
        "Konstantinos Kallidromitis",
        "Anirudh Pai",
        "Shankar Shastry",
        "Trevor Darrell",
        "Jitendra Malik",
        "Roei Herzig"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.12866v1",
      "relevance_score": 6
    },
    {
      "title": "Two-stream network-driven vision-based tactile sensor for object feature\n  extraction and fusion perception",
      "link": "https://arxiv.org/abs/2510.12528v1",
      "pdf_link": "https://arxiv.org/pdf/2510.12528v1.pdf",
      "summary": "Tactile perception is crucial for embodied intelligent robots to recognize\nobjects. Vision-based tactile sensors extract object physical attributes\nmultidimensionally using high spatial resolution; however, this process\ngenerates abundant redundant information. Furthermore, single-dimensional\nextraction, lacking effective fusion, fails to fully characterize object\nattributes. These challenges hinder the improvement of recognition accuracy. To\naddress this issue, this study introduces a two-stream network feature\nextraction and fusion perception strategy for vision-based tactile systems.\nThis strategy employs a distributed approach to extract internal and external\nobject features. It obtains depth map information through three-dimensional\nreconstruction while simultaneously acquiring hardness information by measuring\ncontact force data. After extracting features with a convolutional neural\nnetwork (CNN), weighted fusion is applied to create a more informative and\neffective feature representation. In standard tests on objects of varying\nshapes and hardness, the force prediction error is 0.06 N (within a 12 N\nrange). Hardness recognition accuracy reaches 98.0%, and shape recognition\naccuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in\nactual grasping scenarios exceeds 98.5%. Focused on object physical attributes\nperception, this method enhances the artificial tactile system ability to\ntransition from perception to cognition, enabling its use in embodied\nperception applications.",
      "published": "2025-10-14T13:53:38Z",
      "authors": [
        "Muxing Huang",
        "Zibin Chen",
        "Weiliang Xu",
        "Zilan Li",
        "Yuanzhi Zhou",
        "Guoyuan Zhou",
        "Wenjing Chen",
        "Xinming Li"
      ],
      "categories": [
        "cs.RO",
        "physics.app-ph"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.12528v1",
      "relevance_score": 6
    },
    {
      "title": "DeLTa: Demonstration and Language-Guided Novel Transparent Object\n  Manipulation",
      "link": "https://arxiv.org/abs/2510.05662v1",
      "pdf_link": "https://arxiv.org/pdf/2510.05662v1.pdf",
      "summary": "Despite the prevalence of transparent object interactions in human everyday\nlife, transparent robotic manipulation research remains limited to\nshort-horizon tasks and basic grasping capabilities.Although some methods have\npartially addressed these issues, most of them have limitations in\ngeneralizability to novel objects and are insufficient for precise long-horizon\nrobot manipulation. To address this limitation, we propose DeLTa (Demonstration\nand Language-Guided Novel Transparent Object Manipulation), a novel framework\nthat integrates depth estimation, 6D pose estimation, and vision-language\nplanning for precise long-horizon manipulation of transparent objects guided by\nnatural task instructions. A key advantage of our method is its\nsingle-demonstration approach, which generalizes 6D trajectories to novel\ntransparent objects without requiring category-level priors or additional\ntraining. Additionally, we present a task planner that refines the\nVLM-generated plan to account for the constraints of a single-arm, eye-in-hand\nrobot for long-horizon object manipulation tasks. Through comprehensive\nevaluation, we demonstrate that our method significantly outperforms existing\ntransparent object manipulation approaches, particularly in long-horizon\nscenarios requiring precise manipulation capabilities. Project page:\nhttps://sites.google.com/view/DeLTa25/",
      "published": "2025-10-07T08:18:29Z",
      "authors": [
        "Taeyeop Lee",
        "Gyuree Kang",
        "Bowen Wen",
        "Youngho Kim",
        "Seunghyeok Back",
        "In So Kweon",
        "David Hyunchul Shim",
        "Kuk-Jin Yoon"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.05662v1",
      "relevance_score": 6
    },
    {
      "title": "Prescribed Performance Control of Deformable Object Manipulation in\n  Spatial Latent Space",
      "link": "https://arxiv.org/abs/2510.14234v1",
      "pdf_link": "https://arxiv.org/pdf/2510.14234v1.pdf",
      "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod.",
      "published": "2025-10-16T02:26:48Z",
      "authors": [
        "Ning Han",
        "Gu Gong",
        "Bin Zhang",
        "Yuexuan Xu",
        "Bohan Yang",
        "Yunhui Liu",
        "David Navarro-Alarcon"
      ],
      "categories": [
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.14234v1",
      "relevance_score": 5
    },
    {
      "title": "Active Tactile Exploration for Rigid Body Pose and Shape Estimation",
      "link": "https://arxiv.org/abs/2510.13595v1",
      "pdf_link": "https://arxiv.org/pdf/2510.13595v1.pdf",
      "summary": "General robot manipulation requires the handling of previously unseen\nobjects. Learning a physically accurate model at test time can provide\nsignificant benefits in data efficiency, predictability, and reuse between\ntasks. Tactile sensing can compliment vision with its robustness to occlusion,\nbut its temporal sparsity necessitates careful online exploration to maintain\ndata efficiency. Direct contact can also cause an unrestrained object to move,\nrequiring both shape and location estimation. In this work, we propose a\nlearning and exploration framework that uses only tactile data to\nsimultaneously determine the shape and location of rigid objects with minimal\nrobot motion. We build on recent advances in contact-rich system identification\nto formulate a loss function that penalizes physical constraint violation\nwithout introducing the numerical stiffness inherent in rigid-body contact.\nOptimizing this loss, we can learn cuboid and convex polyhedral geometries with\nless than 10s of randomly collected data after first contact. Our exploration\nscheme seeks to maximize Expected Information Gain and results in significantly\nfaster learning in both simulated and real-robot experiments. More information\ncan be found at https://dairlab.github.io/activetactile",
      "published": "2025-10-15T14:24:57Z",
      "authors": [
        "Ethan K. Gordon",
        "Bruke Baraki",
        "Hien Bui",
        "Michael Posa"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.13595v1",
      "relevance_score": 5
    },
    {
      "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
      "link": "https://arxiv.org/abs/2510.11660v2",
      "pdf_link": "https://arxiv.org/pdf/2510.11660v2.pdf",
      "summary": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets. The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/.",
      "published": "2025-10-13T17:34:48Z",
      "authors": [
        "Yi Yang",
        "Kefan Gu",
        "Yuqing Wen",
        "Hebei Li",
        "Yucheng Zhao",
        "Tiancai Wang",
        "Xudong Liu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.11660v2",
      "relevance_score": 5
    },
    {
      "title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike\n  Streams",
      "link": "https://arxiv.org/abs/2510.10602v1",
      "pdf_link": "https://arxiv.org/pdf/2510.10602v1.pdf",
      "summary": "Most robotic grasping systems rely on converting sensor data into explicit 3D\npoint clouds, which is a computational step not found in biological\nintelligence. This paper explores a fundamentally different, neuro-inspired\nparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that\nmimics the biological visuomotor pathway, processing raw, asynchronous events\nfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.\nOur model fuses these stereo spike streams and uses a recurrent spiking neural\nnetwork, analogous to high-level visual processing, to iteratively refine grasp\nhypotheses without ever reconstructing a point cloud. To validate this\napproach, we built a large-scale synthetic benchmark dataset. Experiments show\nthat SpikeGrasp surpasses traditional point-cloud-based baselines, especially\nin cluttered and textureless scenes, and demonstrates remarkable data\nefficiency. By establishing the viability of this end-to-end, neuro-inspired\napproach, SpikeGrasp paves the way for future systems capable of the fluid and\nefficient manipulation seen in nature, particularly for dynamic objects.",
      "published": "2025-10-12T13:36:40Z",
      "authors": [
        "Zhuoheng Gao",
        "Jiyao Zhang",
        "Zhiyong Xie",
        "Hao Dong",
        "Zhaofei Yu",
        "Rongmei Chen",
        "Guozhang Chen",
        "Tiejun Huang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.10602v1",
      "relevance_score": 5
    },
    {
      "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated\n  Videos",
      "link": "https://arxiv.org/abs/2510.08568v1",
      "pdf_link": "https://arxiv.org/pdf/2510.08568v1.pdf",
      "summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.",
      "published": "2025-10-09T17:59:55Z",
      "authors": [
        "Hongyu Li",
        "Lingfeng Sun",
        "Yafei Hu",
        "Duy Ta",
        "Jennifer Barry",
        "George Konidaris",
        "Jiahui Fu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.08568v1",
      "relevance_score": 5
    },
    {
      "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via\n  Joint-Wise Neural Dynamics Model",
      "link": "https://arxiv.org/abs/2510.08556v1",
      "pdf_link": "https://arxiv.org/pdf/2510.08556v1.pdf",
      "summary": "Achieving generalized in-hand object rotation remains a significant challenge\nin robotics, largely due to the difficulty of transferring policies from\nsimulation to the real world. The complex, contact-rich dynamics of dexterous\nmanipulation create a \"reality gap\" that has limited prior work to constrained\nscenarios involving simple geometries, limited object sizes and aspect ratios,\nconstrained wrist poses, or customized hands. We address this sim-to-real\nchallenge with a novel framework that enables a single policy, trained in\nsimulation, to generalize to a wide variety of objects and conditions in the\nreal world. The core of our method is a joint-wise dynamics model that learns\nto bridge the reality gap by effectively fitting limited amount of real-world\ncollected data and then adapting the sim policy's actions accordingly. The\nmodel is highly data-efficient and generalizable across different whole-hand\ninteraction distributions by factorizing dynamics across joints, compressing\nsystem-wide influences into low-dimensional variables, and learning each\njoint's evolution from its own dynamic profile, implicitly capturing these net\neffects. We pair this with a fully autonomous data collection strategy that\ngathers diverse, real-world interaction data with minimal human intervention.\nOur complete pipeline demonstrates unprecedented generality: a single policy\nsuccessfully rotates challenging objects with complex shapes (e.g., animals),\nhigh aspect ratios (up to 5.33), and small sizes, all while handling diverse\nwrist orientations and rotation axes. Comprehensive real-world evaluations and\na teleoperation application for complex tasks validate the effectiveness and\nrobustness of our approach. Website: https://meowuu7.github.io/DexNDM/",
      "published": "2025-10-09T17:59:11Z",
      "authors": [
        "Xueyi Liu",
        "He Wang",
        "Li Yi"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.08556v1",
      "relevance_score": 5
    },
    {
      "title": "VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought\n  Reasoning for Language-driven Grasp Generation",
      "link": "https://arxiv.org/abs/2510.05827v1",
      "pdf_link": "https://arxiv.org/pdf/2510.05827v1.pdf",
      "summary": "Robotic grasping is one of the most fundamental tasks in robotic\nmanipulation, and grasp detection/generation has long been the subject of\nextensive research. Recently, language-driven grasp generation has emerged as a\npromising direction due to its practical interaction capabilities. However,\nmost existing approaches either lack sufficient reasoning and generalization\ncapabilities or depend on complex modular pipelines. Moreover, current grasp\nfoundation models tend to overemphasize dialog and object semantics, resulting\nin inferior performance and restriction to single-object grasping. To maintain\nstrong reasoning ability and generalization in cluttered environments, we\npropose VCoT-Grasp, an end-to-end grasp foundation model that incorporates\nvisual chain-of-thought reasoning to enhance visual understanding for grasp\ngeneration. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically\nfocuses on visual inputs while providing interpretable reasoning traces. For\ntraining, we refine and introduce a large-scale dataset, VCoT-GraspSet,\ncomprising 167K synthetic images with over 1.36M grasps, as well as 400+\nreal-world images with more than 1.2K grasps, annotated with intermediate\nbounding boxes. Extensive experiments on both VCoT-GraspSet and real robot\ndemonstrate that our method significantly improves grasp success rates and\ngeneralizes effectively to unseen objects, backgrounds, and distractors. More\ndetails can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.",
      "published": "2025-10-07T11:50:26Z",
      "authors": [
        "Haoran Zhang",
        "Shuanghao Bai",
        "Wanqi Zhou",
        "Yuedi Zhang",
        "Qi Zhang",
        "Pengxiang Ding",
        "Cheng Chi",
        "Donglin Wang",
        "Badong Chen"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.05827v1",
      "relevance_score": 5
    },
    {
      "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement\n  Learning",
      "link": "https://arxiv.org/abs/2510.14830v1",
      "pdf_link": "https://arxiv.org/pdf/2510.14830v1.pdf",
      "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours.",
      "published": "2025-10-16T16:07:50Z",
      "authors": [
        "Kun Lei",
        "Huanyu Li",
        "Dongjie Yu",
        "Zhenyu Wei",
        "Lingxiao Guo",
        "Zhennan Jiang",
        "Ziyu Wang",
        "Shiyu Liang",
        "Huazhe Xu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.14830v1",
      "relevance_score": 4
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}