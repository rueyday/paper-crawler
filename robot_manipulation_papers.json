{
  "last_updated": "2025-10-06T08:27:38.086678",
  "total_papers": 25,
  "papers": [
    {
      "title": "CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from\n  Human-like Contact Representations",
      "link": "https://arxiv.org/abs/2509.24661v1",
      "pdf_link": "https://arxiv.org/pdf/2509.24661v1.pdf",
      "summary": "Cross-embodiment dexterous grasp synthesis refers to adaptively generating\nand optimizing grasps for various robotic hands with different morphologies.\nThis capability is crucial for achieving versatile robotic manipulation in\ndiverse environments and requires substantial amounts of reliable and diverse\ngrasp data for effective model training and robust generalization. However,\nexisting approaches either rely on physics-based optimization that lacks\nhuman-like kinematic understanding or require extensive manual data collection\nprocesses that are limited to anthropomorphic structures. In this paper, we\npropose CEDex, a novel cross-embodiment dexterous grasp synthesis method at\nscale that bridges human grasping kinematics and robot kinematics by aligning\nrobot kinematic models with generated human-like contact representations. Given\nan object's point cloud and an arbitrary robotic hand model, CEDex first\ngenerates human-like contact representations using a Conditional Variational\nAuto-encoder pretrained on human contact data. It then performs kinematic human\ncontact alignment through topological merging to consolidate multiple human\nhand parts into unified robot components, followed by a signed distance\nfield-based grasp optimization with physics-aware constraints. Using CEDex, we\nconstruct the largest cross-embodiment grasp dataset to date, comprising 500K\nobjects across four gripper types with 20M total grasps. Extensive experiments\nshow that CEDex outperforms state-of-the-art approaches and our dataset\nbenefits cross-embodiment grasp learning with high-quality diverse grasps.",
      "published": "2025-09-29T12:08:04Z",
      "authors": [
        "Zhiyuan Wu",
        "Rolandos Alexandros Potamias",
        "Xuyang Zhang",
        "Zhongqun Zhang",
        "Jiankang Deng",
        "Shan Luo"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.24661v1",
      "relevance_score": 11
    },
    {
      "title": "GES-UniGrasp: A Two-Stage Dexterous Grasping Strategy With\n  Geometry-Based Expert Selection",
      "link": "https://arxiv.org/abs/2509.23567v1",
      "pdf_link": "https://arxiv.org/pdf/2509.23567v1.pdf",
      "summary": "Robust and human-like dexterous grasping of general objects is a critical\ncapability for advancing intelligent robotic manipulation in real-world\nscenarios. However, existing reinforcement learning methods guided by grasp\npriors often result in unnatural behaviors. In this work, we present\n\\textit{ContactGrasp}, a robotic dexterous pre-grasp and grasp dataset that\nexplicitly accounts for task-relevant wrist orientation and thumb-index\npinching coordination. The dataset covers 773 objects in 82 categories,\nproviding a rich foundation for training human-like grasp strategies. Building\nupon this dataset, we perform geometry-based clustering to group objects by\nshape, enabling a two-stage Geometry-based Expert Selection (GES) framework\nthat selects among specialized experts for grasping diverse object geometries,\nthereby enhancing adaptability to diverse shapes and generalization across\ncategories. Our approach demonstrates natural grasp postures and achieves high\nsuccess rates of 99.4\\% and 96.3\\% on the train and test sets, respectively,\nshowcasing strong generalization and high-quality grasp execution.",
      "published": "2025-09-28T01:56:34Z",
      "authors": [
        "Fangting Xu",
        "Jilin Zhu",
        "Xiaoming Gu",
        "Jianzhong Tang"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.23567v1",
      "relevance_score": 11
    },
    {
      "title": "In-Hand Manipulation of Articulated Tools with Dexterous Robot Hands\n  with Sim-to-Real Transfer",
      "link": "https://arxiv.org/abs/2509.23075v1",
      "pdf_link": "https://arxiv.org/pdf/2509.23075v1.pdf",
      "summary": "Reinforcement learning (RL) and sim-to-real transfer have advanced robotic\nmanipulation of rigid objects. Yet, policies remain brittle when applied to\narticulated mechanisms due to contact-rich dynamics and under-modeled joint\nphenomena such as friction, stiction, backlash, and clearances. We address this\nchallenge through dexterous in-hand manipulation of articulated tools using a\nrobotic hand with reduced articulation and kinematic redundancy relative to the\nhuman hand. Our controller augments a simulation-trained base policy with a\nsensor-driven refinement learned from hardware demonstrations, conditioning on\nproprioception and target articulation states while fusing whole-hand tactile\nand force feedback with the policy's internal action intent via\ncross-attention-based integration. This design enables online adaptation to\ninstance-specific articulation properties, stabilizes contact interactions,\nregulates internal forces, and coordinates coupled-link motion under\nperturbations. We validate our approach across a diversity of real-world\nexamples, including scissors, pliers, minimally invasive surgical tools, and\nstaplers. We achieve robust transfer from simulation to hardware, improved\ndisturbance resilience, and generalization to previously unseen articulated\ntools, thereby reducing reliance on precise physical modeling in contact-rich\nsettings.",
      "published": "2025-09-27T02:56:57Z",
      "authors": [
        "Soofiyan Atar",
        "Daniel Huang",
        "Florian Richter",
        "Michael Yip"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.23075v1",
      "relevance_score": 10
    },
    {
      "title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration",
      "link": "https://arxiv.org/abs/2509.22149v1",
      "pdf_link": "https://arxiv.org/pdf/2509.22149v1.pdf",
      "summary": "Universal grasping with multi-fingered dexterous hands is a fundamental\nchallenge in robotic manipulation. While recent approaches successfully learn\nclosed-loop grasping policies using reinforcement learning (RL), the inherent\ndifficulty of high-dimensional, long-horizon exploration necessitates complex\nreward and curriculum design, often resulting in suboptimal solutions across\ndiverse objects. We propose DemoGrasp, a simple yet effective method for\nlearning universal dexterous grasping. We start from a single successful\ndemonstration trajectory of grasping a specific object and adapt to novel\nobjects and poses by editing the robot actions in this trajectory: changing the\nwrist pose determines where to grasp, and changing the hand joint angles\ndetermines how to grasp. We formulate this trajectory editing as a single-step\nMarkov Decision Process (MDP) and use RL to optimize a universal policy across\nhundreds of objects in parallel in simulation, with a simple reward consisting\nof a binary success term and a robot-table collision penalty. In simulation,\nDemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow\nHand, outperforming previous state-of-the-art methods. It also shows strong\ntransferability, achieving an average success rate of 84.6% across diverse\ndexterous hand embodiments on six unseen object datasets, while being trained\non only 175 objects. Through vision-based imitation learning, our policy\nsuccessfully grasps 110 unseen real-world objects, including small, thin items.\nIt generalizes to spatial, background, and lighting changes, supports both RGB\nand depth inputs, and extends to language-guided grasping in cluttered scenes.",
      "published": "2025-09-26T10:09:51Z",
      "authors": [
        "Haoqi Yuan",
        "Ziye Huang",
        "Ye Wang",
        "Chuan Mao",
        "Chaoyi Xu",
        "Zongqing Lu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.22149v1",
      "relevance_score": 10
    },
    {
      "title": "TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand\n  Object Poses",
      "link": "https://arxiv.org/abs/2509.25746v1",
      "pdf_link": "https://arxiv.org/pdf/2509.25746v1.pdf",
      "summary": "Despite progress in both traditional dexterous grasping pipelines and recent\nVision-Language-Action (VLA) approaches, the grasp execution stage remains\nprone to pose inaccuracies, especially in long-horizon tasks, which undermines\noverall performance. To address this \"last-mile\" challenge, we propose\nTacRefineNet, a tactile-only framework that achieves fine in-hand pose\nrefinement of known objects in arbitrary target poses using multi-finger\nfingertip sensing. Our method iteratively adjusts the end-effector pose based\non tactile feedback, aligning the object to the desired configuration. We\ndesign a multi-branch policy network that fuses tactile inputs from multiple\nfingers along with proprioception to predict precise control updates. To train\nthis policy, we combine large-scale simulated data from a physics-based tactile\nmodel in MuJoCo with real-world data collected from a physical system.\nComparative experiments show that pretraining on simulated data and fine-tuning\nwith a small amount of real data significantly improves performance over\nsimulation-only training. Extensive real-world experiments validate the\neffectiveness of the method, achieving millimeter-level grasp accuracy using\nonly tactile input. To our knowledge, this is the first method to enable\narbitrary in-hand pose refinement via multi-finger tactile sensing alone.\nProject website is available at https://sites.google.com/view/tacrefinenet",
      "published": "2025-09-30T04:05:03Z",
      "authors": [
        "Shuaijun Wang",
        "Haoran Zhou",
        "Diyun Xiang",
        "Yangwei You"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.25746v1",
      "relevance_score": 8
    },
    {
      "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot\n  Grasping with Ambiguous Instructions",
      "link": "https://arxiv.org/abs/2510.02104v1",
      "pdf_link": "https://arxiv.org/pdf/2510.02104v1.pdf",
      "summary": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp.",
      "published": "2025-10-02T15:10:26Z",
      "authors": [
        "Yunhan Lin",
        "Wenqi Wu",
        "Zhijie Zhang",
        "Huasong Min"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.02104v1",
      "relevance_score": 7
    },
    {
      "title": "Shared Object Manipulation with a Team of Collaborative Quadrupeds",
      "link": "https://arxiv.org/abs/2510.00682v1",
      "pdf_link": "https://arxiv.org/pdf/2510.00682v1.pdf",
      "summary": "Utilizing teams of multiple robots is advantageous for handling bulky\nobjects. Many related works focus on multi-manipulator systems, which are\nlimited by workspace constraints. In this paper, we extend a classical hybrid\nmotion-force controller to a team of legged manipulator systems, enabling\ncollaborative loco-manipulation of rigid objects with a force-closed grasp. Our\nnovel approach allows the robots to flexibly coordinate their movements,\nachieving efficient and stable object co-manipulation and transport, validated\nthrough extensive simulations and real-world experiments.",
      "published": "2025-10-01T09:04:38Z",
      "authors": [
        "Shengzhi Wang",
        "Niels Dehio",
        "Xuanqi Zeng",
        "Xian Yang",
        "Lingwei Zhang",
        "Yun-Hui Liu",
        "K. W. Samuel Au"
      ],
      "categories": [
        "cs.RO",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.00682v1",
      "relevance_score": 7
    },
    {
      "title": "ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm",
      "link": "https://arxiv.org/abs/2509.26236v1",
      "pdf_link": "https://arxiv.org/pdf/2509.26236v1.pdf",
      "summary": "The rapid increase in the development of humanoid robots and customized\nmanufacturing solutions has brought dexterous manipulation to the forefront of\nmodern robotics. Over the past decade, several expensive dexterous hands have\ncome to market, but advances in hardware design, particularly in servo motors\nand 3D printing, have recently facilitated an explosion of cheaper open-source\nhands. Most hands are anthropomorphic to allow use of standard human tools, and\nattempts to increase dexterity often sacrifice anthropomorphism. We introduce\nthe open-source ISyHand (pronounced easy-hand), a highly dexterous, low-cost,\neasy-to-manufacture, on-joint servo-driven robot hand. Our hand uses\noff-the-shelf Dynamixel motors, fasteners, and 3D-printed parts, can be\nassembled within four hours, and has a total material cost of about 1,300 USD.\nThe ISyHands's unique articulated-palm design increases overall dexterity with\nonly a modest sacrifice in anthropomorphism. To demonstrate the utility of the\narticulated palm, we use reinforcement learning in simulation to train the hand\nto perform a classical in-hand manipulation task: cube reorientation. Our\nnovel, systematic experiments show that the simulated ISyHand outperforms the\ntwo most comparable hands in early training phases, that all three perform\nsimilarly well after policy convergence, and that the ISyHand significantly\noutperforms a fixed-palm version of its own design. Additionally, we deploy a\npolicy trained on cube reorientation on the real hand, demonstrating its\nability to perform real-world dexterous manipulation.",
      "published": "2025-09-30T13:31:10Z",
      "authors": [
        "Benjamin A. Richardson",
        "Felix Grüninger",
        "Lukas Mack",
        "Joerg Stueckler",
        "Katherine J. Kuchenbecker"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.26236v1",
      "relevance_score": 7
    },
    {
      "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware\n  Representation for Contact-Rich Manipulation",
      "link": "https://arxiv.org/abs/2509.19047v1",
      "pdf_link": "https://arxiv.org/pdf/2509.19047v1.pdf",
      "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.",
      "published": "2025-09-23T14:15:19Z",
      "authors": [
        "Geonhyup Lee",
        "Yeongjin Lee",
        "Kangmin Kim",
        "Seongju Lee",
        "Sangjun Noh",
        "Seunghyeok Back",
        "Kyoobin Lee"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19047v1",
      "relevance_score": 7
    },
    {
      "title": "Anomaly detection for generic failure monitoring in robotic assembly,\n  screwing and manipulation",
      "link": "https://arxiv.org/abs/2509.26308v1",
      "pdf_link": "https://arxiv.org/pdf/2509.26308v1.pdf",
      "summary": "Out-of-distribution states in robot manipulation often lead to unpredictable\nrobot behavior or task failure, limiting success rates and increasing risk of\ndamage. Anomaly detection (AD) can identify deviations from expected patterns\nin data, which can be used to trigger failsafe behaviors and recovery\nstrategies. Prior work has applied data-driven AD to time series data in\nspecific robotic tasks, but its transferability across control strategies and\ntask types has not been shown. Leveraging time series data, such as\nforce/torque signals, allows to directly capture robot-environment\ninteractions, crucial for manipulation and online failure detection. Their\nbroad availability, high sampling rates, and low dimensionality enable high\ntemporal resolution and efficient processing. As robotic tasks can have widely\nsignal characteristics and requirements, AD methods which can be applied in the\nsame way to a wide range of tasks is needed, ideally with good data efficiency.\nWe examine three industrial robotic tasks, each presenting several anomalies.\nTest scenarios in robotic cabling, screwing, and sanding are built, and\nmultimodal time series data is gathered. Several autoencoder-based methods are\ncompared, evaluating generalization across tasks and control methods (diffusion\npolicy, position, and impedance control). This allows us to validate the\nintegration of AD in complex tasks involving tighter tolerances and variation\nfrom both the robot and its environment. Additionally, we evaluate data\nefficiency, detection latency, and task characteristics which support robust\ndetection. The results indicate reliable detection with AUROC exceeding 0.93 in\nfailures in the cabling and screwing task, such as incorrect or misaligned\nparts and obstructed targets. In the polishing task, only severe failures were\nreliably detected, while more subtle failure types remained undetected.",
      "published": "2025-09-30T14:22:45Z",
      "authors": [
        "Niklas Grambow",
        "Lisa-Marie Fenner",
        "Felipe Kempkes",
        "Philip Hotz",
        "Dingyuan Wan",
        "Jörg Krüger",
        "Kevin Haninger"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.26308v1",
      "relevance_score": 6
    },
    {
      "title": "FTACT: Force Torque aware Action Chunking Transformer for\n  Pick-and-Reorient Bottle Task",
      "link": "https://arxiv.org/abs/2509.23112v1",
      "pdf_link": "https://arxiv.org/pdf/2509.23112v1.pdf",
      "summary": "Manipulator robots are increasingly being deployed in retail environments,\nyet contact rich edge cases still trigger costly human teleoperation. A\nprominent example is upright lying beverage bottles, where purely visual cues\nare often insufficient to resolve subtle contact events required for precise\nmanipulation. We present a multimodal Imitation Learning policy that augments\nthe Action Chunking Transformer with force and torque sensing, enabling\nend-to-end learning over images, joint states, and forces and torques. Deployed\non Ghost, single-arm platform by Telexistence Inc, our approach improves\nPick-and-Reorient bottle task by detecting and exploiting contact transitions\nduring pressing and placement. Hardware experiments demonstrate greater task\nsuccess compared to baseline matching the observation space of ACT as an\nablation and experiments indicate that force and torque signals are beneficial\nin the press and place phases where visual observability is limited, supporting\nthe use of interaction forces as a complementary modality for contact rich\nskills. The results suggest a practical path to scaling retail manipulation by\ncombining modern imitation learning architectures with lightweight force and\ntorque sensing.",
      "published": "2025-09-27T04:52:13Z",
      "authors": [
        "Ryo Watanabe",
        "Maxime Alvarez",
        "Pablo Ferreiro",
        "Pavel Savkin",
        "Genki Sano"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.23112v1",
      "relevance_score": 6
    },
    {
      "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
      "link": "https://arxiv.org/abs/2509.21006v1",
      "pdf_link": "https://arxiv.org/pdf/2509.21006v1.pdf",
      "summary": "We address natural language pick-and-place in unseen, unpredictable indoor\nenvironments with AnywhereVLA, a modular framework for mobile manipulation. A\nuser text prompt serves as an entry point and is parsed into a structured task\ngraph that conditions classical SLAM with LiDAR and cameras, metric semantic\nmapping, and a task-aware frontier exploration policy. An approach planner then\nselects visibility and reachability aware pre grasp base poses. For\ninteraction, a compact SmolVLA manipulation head is fine tuned on platform pick\nand place trajectories for the SO-101 by TheRobotStudio, grounding local visual\ncontext and sub-goals into grasp and place proposals. The full system runs\nfully onboard on consumer-level hardware, with Jetson Orin NX for perception\nand VLA and an Intel NUC for SLAM, exploration, and control, sustaining\nreal-time operation. We evaluated AnywhereVLA in a multi-room lab under static\nscenes and normal human motion. In this setting, the system achieves a $46\\%$\noverall task success rate while maintaining throughput on embedded compute. By\ncombining a classical stack with a fine-tuned VLA manipulation, the system\ninherits the reliability of geometry-based navigation with the agility and task\ngeneralization of language-conditioned manipulation.",
      "published": "2025-09-25T11:04:44Z",
      "authors": [
        "Konstantin Gubernatorov",
        "Artem Voronov",
        "Roman Voronov",
        "Sergei Pasynkov",
        "Stepan Perminov",
        "Ziang Guo",
        "Dzmitry Tsetserukou"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.21006v1",
      "relevance_score": 6
    },
    {
      "title": "Simultaneous estimation of contact position and tool shape with\n  high-dimensional parameters using force measurements and particle filtering",
      "link": "https://arxiv.org/abs/2509.19732v1",
      "pdf_link": "https://arxiv.org/pdf/2509.19732v1.pdf",
      "summary": "Estimating the contact state between a grasped tool and the environment is\nessential for performing contact tasks such as assembly and object\nmanipulation. Force signals are valuable for estimating the contact state, as\nthey can be utilized even when the contact location is obscured by the tool.\nPrevious studies proposed methods for estimating contact positions using\nforce/torque signals; however, most methods require the geometry of the tool\nsurface to be known. Although several studies have proposed methods that do not\nrequire the tool shape, these methods require considerable time for estimation\nor are limited to tools with low-dimensional shape parameters. Here, we propose\na method for simultaneously estimating the contact position and tool shape,\nwhere the tool shape is represented by a grid, which is high-dimensional (more\nthan 1000 dimensional). The proposed method uses a particle filter in which\neach particle has individual tool shape parameters, thereby to avoid directly\nhandling a high-dimensional parameter space. The proposed method is evaluated\nthrough simulations and experiments using tools with curved shapes on a plane.\nConsequently, the proposed method can estimate the shape of the tool\nsimultaneously with the contact positions, making the contact-position\nestimation more accurate.",
      "published": "2025-09-24T03:24:01Z",
      "authors": [
        "Kyo Kutsuzawa",
        "Mitsuhiro Hayashibe"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19732v1",
      "relevance_score": 6
    },
    {
      "title": "DexFlyWheel: A Scalable and Self-improving Data Generation Framework for\n  Dexterous Manipulation",
      "link": "https://arxiv.org/abs/2509.23829v1",
      "pdf_link": "https://arxiv.org/pdf/2509.23829v1.pdf",
      "summary": "Dexterous manipulation is critical for advancing robot capabilities in\nreal-world applications, yet diverse and high-quality datasets remain scarce.\nExisting data collection methods either rely on human teleoperation or require\nsignificant human engineering, or generate data with limited diversity, which\nrestricts their scalability and generalization. In this paper, we introduce\nDexFlyWheel, a scalable data generation framework that employs a self-improving\ncycle to continuously enrich data diversity. Starting from efficient seed\ndemonstrations warmup, DexFlyWheel expands the dataset through iterative\ncycles. Each cycle follows a closed-loop pipeline that integrates Imitation\nLearning (IL), residual Reinforcement Learning (RL), rollout trajectory\ncollection, and data augmentation. Specifically, IL extracts human-like\nbehaviors from demonstrations, and residual RL enhances policy generalization.\nThe learned policy is then used to generate trajectories in simulation, which\nare further augmented across diverse environments and spatial configurations\nbefore being fed back into the next cycle. Over successive iterations, a\nself-improving data flywheel effect emerges, producing datasets that cover\ndiverse scenarios and thereby scaling policy performance. Experimental results\ndemonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across\nfour challenging tasks. Policies trained on our dataset achieve an average\nsuccess rate of 81.9\\% on the challenge test sets and successfully transfer to\nthe real world through digital twin, achieving a 78.3\\% success rate on\ndual-arm lift tasks.",
      "published": "2025-09-28T12:07:02Z",
      "authors": [
        "Kefei Zhu",
        "Fengshuo Bai",
        "YuanHao Xiang",
        "Yishuai Cai",
        "Xinglin Chen",
        "Ruochong Li",
        "Xingtao Wang",
        "Hao Dong",
        "Yaodong Yang",
        "Xiaopeng Fan",
        "Yuanpei Chen"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.23829v1",
      "relevance_score": 6
    },
    {
      "title": "HeLoM: Hierarchical Learning for Whole-Body Loco-Manipulation in Hexapod\n  Robot",
      "link": "https://arxiv.org/abs/2509.23651v1",
      "pdf_link": "https://arxiv.org/pdf/2509.23651v1.pdf",
      "summary": "Robots in real-world environments are often required to move/manipulate\nobjects comparable in weight to their own bodies. Compared to grasping and\ncarrying, pushing provides a more straightforward and efficient non-prehensile\nmanipulation strategy, avoiding complex grasp design while leveraging direct\ncontact to regulate an object's pose. Achieving effective pushing, however,\ndemands both sufficient manipulation forces and the ability to maintain\nstability, which is particularly challenging when dealing with heavy or\nirregular objects. To address these challenges, we propose HeLoM, a\nlearning-based hierarchical whole-body manipulation framework for a hexapod\nrobot that exploits coordinated multi-limb control. Inspired by the cooperative\nstrategies of multi-legged insects, our framework leverages redundant contact\npoints and high degrees of freedom to enable dynamic redistribution of contact\nforces. HeLoM's high-level planner plans pushing behaviors and target object\nposes, while its low-level controller maintains locomotion stability and\ngenerates dynamically consistent joint actions. Our policies trained in\nsimulation are directly deployed on real robots without additional fine-tuning.\nThis design allows the robot to maintain balance while exerting continuous and\ncontrollable pushing forces through coordinated foreleg interaction and\nsupportive hind-leg propulsion. We validate the effectiveness of HeLoM through\nboth simulation and real-world experiments. Results show that our framework can\nstably push boxes of varying sizes and unknown physical properties to\ndesignated goal poses in the real world.",
      "published": "2025-09-28T05:34:39Z",
      "authors": [
        "Xinrong Yang",
        "Peizhuo Li",
        "Hongyi Li",
        "Junkai Lu",
        "Linnan Chang",
        "Yuhong Cao",
        "Yifeng Zhang",
        "Ge Sun",
        "Guillaume Sartoretti"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.23651v1",
      "relevance_score": 6
    },
    {
      "title": "Zero-shot Whole-Body Manipulation with a Large-Scale Soft Robotic Torso\n  via Guided Reinforcement Learning",
      "link": "https://arxiv.org/abs/2509.23556v1",
      "pdf_link": "https://arxiv.org/pdf/2509.23556v1.pdf",
      "summary": "Whole-body manipulation is a powerful yet underexplored approach that enables\nrobots to interact with large, heavy, or awkward objects using more than just\ntheir end-effectors. Soft robots, with their inherent passive compliance, are\nparticularly well-suited for such contact-rich manipulation tasks, but their\nuncertainties in kinematics and dynamics pose significant challenges for\nsimulation and control. In this work, we address this challenge with a\nsimulation that can run up to 350x real time on a single thread in MuJoCo and\nprovide a detailed analysis of the critical tradeoffs between speed and\naccuracy for this simulation. Using this framework, we demonstrate a successful\nzero-shot sim-to-real transfer of a learned whole-body manipulation policy,\nachieving an 88% success rate on the Baloo hardware platform. We show that\nguiding RL with a simple motion primitive is critical to this success where\nstandard reward shaping methods struggled to produce a stable and successful\npolicy for whole-body manipulation. Furthermore, our analysis reveals that the\nlearned policy does not simply mimic the motion primitive. It exhibits\nbeneficial reactive behavior, such as re-grasping and perturbation recovery. We\nanalyze and contrast this learned policy against an open-loop baseline to show\nthat the policy can also exhibit aggressive over-corrections under\nperturbation. To our knowledge, this is the first demonstration of forceful,\nsix-DoF whole-body manipulation using two continuum soft arms on a large-scale\nplatform (10 kg payloads), with zero-shot policy transfer.",
      "published": "2025-09-28T01:32:52Z",
      "authors": [
        "Curtis C. Johnson",
        "Carlo Alessi",
        "Egidio Falotico",
        "Marc D. Killpack"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.23556v1",
      "relevance_score": 6
    },
    {
      "title": "U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic\n  Manipulation",
      "link": "https://arxiv.org/abs/2510.02526v1",
      "pdf_link": "https://arxiv.org/pdf/2510.02526v1.pdf",
      "summary": "Robots manipulating in changing environments must act on percepts that are\nlate, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer\nthat leaves the low-level controller unchanged while re-aiming task goals\n(pre-contact, contact, post) as new observations arrive. Unlike motion\nretargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming\nas a first-class, pluggable module between perception and control. Our main\ntechnical contribution is UAR-PF, an uncertainty-aware retargeter that\nmaintains a distribution over object pose under sensing lag and selects goals\nthat maximize expected progress. We instantiate a reproducible Shift x Lag\nstress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion,\nwhere the object undergoes abrupt in-plane shifts while synthetic perception\nlag is injected during approach. Across 0-10 cm shifts and 0-400 ms lags,\nUAR-PF and ICP degrade gracefully relative to a no-retarget baseline, achieving\nhigher success with modest end-effector travel and fewer aborts; simple\noperational safeguards further improve stability. Contributions: (1) UAR-PF for\nlag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting\ninterface; and (3) a reproducible Shift x Lag benchmark with evaluation on\npick, push, stacking, and peg insertion.",
      "published": "2025-10-02T19:54:45Z",
      "authors": [
        "Anamika J H",
        "Anujith Muraleedharan"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.02526v1",
      "relevance_score": 5
    },
    {
      "title": "Contrastive Representation Regularization for Vision-Language-Action\n  Models",
      "link": "https://arxiv.org/abs/2510.01711v1",
      "pdf_link": "https://arxiv.org/pdf/2510.01711v1.pdf",
      "summary": "Vision-Language-Action (VLA) models have shown its capabilities in robot\nmanipulation by leveraging rich representations from pre-trained\nVision-Language Models (VLMs). However, their representations arguably remain\nsuboptimal, lacking sensitivity to robotic signals such as control actions and\nproprioceptive states. To address the issue, we introduce Robot State-aware\nContrastive Loss (RS-CL), a simple and effective representation regularization\nfor VLA models, designed to bridge the gap between VLM representations and\nrobotic signals. In particular, RS-CL aligns the representations more closely\nwith the robot's proprioceptive states, by using relative distances between the\nstates as soft supervision. Complementing the original action prediction\nobjective, RS-CL effectively enhances control-relevant representation learning,\nwhile being lightweight and fully compatible with standard VLA training\npipeline. Our empirical results demonstrate that RS-CL substantially improves\nthe manipulation performance of state-of-the-art VLA models; it pushes the\nprior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,\nthrough more accurate positioning during grasping and placing, and boosts\nsuccess rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.",
      "published": "2025-10-02T06:41:22Z",
      "authors": [
        "Taeyoung Kim",
        "Jimin Lee",
        "Myungkyu Koo",
        "Dongyoung Kim",
        "Kyungmin Lee",
        "Changyeon Kim",
        "Younggyo Seo",
        "Jinwoo Shin"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.01711v1",
      "relevance_score": 5
    },
    {
      "title": "Towards Intuitive Human-Robot Interaction through Embodied\n  Gesture-Driven Control with Woven Tactile Skins",
      "link": "https://arxiv.org/abs/2509.25951v1",
      "pdf_link": "https://arxiv.org/pdf/2509.25951v1.pdf",
      "summary": "This paper presents a novel human-robot interaction (HRI) framework that\nenables intuitive gesture-driven control through a capacitance-based woven\ntactile skin. Unlike conventional interfaces that rely on panels or handheld\ndevices, the woven tactile skin integrates seamlessly with curved robot\nsurfaces, enabling embodied interaction and narrowing the gap between human\nintent and robot response. Its woven design combines fabric-like flexibility\nwith structural stability and dense multi-channel sensing through the\ninterlaced conductive threads. Building on this capability, we define a\ngesture-action mapping of 14 single- and multi-touch gestures that cover\nrepresentative robot commands, including task-space motion and auxiliary\nfunctions. A lightweight convolution-transformer model designed for gesture\nrecognition in real time achieves an accuracy of near-100%, outperforming prior\nbaseline approaches. Experiments on robot arm tasks, including pick-and-place\nand pouring, demonstrate that our system reduces task completion time by up to\n57% compared with keyboard panels and teach pendants. Overall, our proposed\nframework demonstrates a practical pathway toward more natural and efficient\nembodied HRI.",
      "published": "2025-09-30T08:46:17Z",
      "authors": [
        "ChunPing Lam",
        "Xiangjia Chen",
        "Chenming Wu",
        "Hao Chen",
        "Binzhi Sun",
        "Guoxin Fang",
        "Charlie C. L. Wang",
        "Chengkai Dai",
        "Yeung Yam"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.25951v1",
      "relevance_score": 5
    },
    {
      "title": "Multi-Modal Manipulation via Multi-Modal Policy Consensus",
      "link": "https://arxiv.org/abs/2509.23468v1",
      "pdf_link": "https://arxiv.org/pdf/2509.23468v1.pdf",
      "summary": "Effectively integrating diverse sensory modalities is crucial for robotic\nmanipulation. However, the typical approach of feature concatenation is often\nsuboptimal: dominant modalities such as vision can overwhelm sparse but\ncritical signals like touch in contact-rich tasks, and monolithic architectures\ncannot flexibly incorporate new or missing modalities without retraining. Our\nmethod factorizes the policy into a set of diffusion models, each specialized\nfor a single representation (e.g., vision or touch), and employs a router\nnetwork that learns consensus weights to adaptively combine their\ncontributions, enabling incremental of new representations. We evaluate our\napproach on simulated manipulation tasks in {RLBench}, as well as real-world\ntasks such as occluded object picking, in-hand spoon reorientation, and puzzle\ninsertion, where it significantly outperforms feature-concatenation baselines\non scenarios requiring multimodal reasoning. Our policy further demonstrates\nrobustness to physical perturbations and sensor corruption. We further conduct\nperturbation-based importance analysis, which reveals adaptive shifts between\nmodalities.",
      "published": "2025-09-27T19:43:04Z",
      "authors": [
        "Haonan Chen",
        "Jiaming Xu",
        "Hongyu Chen",
        "Kaiwen Hong",
        "Binghao Huang",
        "Chaoqi Liu",
        "Jiayuan Mao",
        "Yunzhu Li",
        "Yilun Du",
        "Katherine Driggs-Campbell"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.23468v1",
      "relevance_score": 5
    },
    {
      "title": "Empart: Interactive Convex Decomposition for Converting Meshes to Parts",
      "link": "https://arxiv.org/abs/2509.22847v1",
      "pdf_link": "https://arxiv.org/pdf/2509.22847v1.pdf",
      "summary": "Simplifying complex 3D meshes is a crucial step in robotics applications to\nenable efficient motion planning and physics simulation. Common methods, such\nas approximate convex decomposition, represent a mesh as a collection of simple\nparts, which are computationally inexpensive to simulate. However, existing\napproaches apply a uniform error tolerance across the entire mesh, which can\nresult in a sub-optimal trade-off between accuracy and performance. For\ninstance, a robot grasping an object needs high-fidelity geometry in the\nvicinity of the contact surfaces but can tolerate a coarser simplification\nelsewhere. A uniform tolerance can lead to excessive detail in non-critical\nareas or insufficient detail where it's needed most.\n  To address this limitation, we introduce Empart, an interactive tool that\nallows users to specify different simplification tolerances for selected\nregions of a mesh. Our method leverages existing convex decomposition\nalgorithms as a sub-routine but uses a novel, parallelized framework to handle\nregion-specific constraints efficiently. Empart provides a user-friendly\ninterface with visual feedback on approximation error and simulation\nperformance, enabling designers to iteratively refine their decomposition. We\ndemonstrate that our approach significantly reduces the number of convex parts\ncompared to a state-of-the-art method (V-HACD) at a fixed error threshold,\nleading to substantial speedups in simulation performance. For a robotic\npick-and-place task, Empart-generated collision meshes reduced the overall\nsimulation time by 69% compared to a uniform decomposition, highlighting the\nvalue of interactive, region-specific simplification for performant robotics\napplications.",
      "published": "2025-09-26T18:56:46Z",
      "authors": [
        "Brandon Vu",
        "Shameek Ganguly",
        "Pushkar Joshi"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.22847v1",
      "relevance_score": 5
    },
    {
      "title": "Generating Stable Placements via Physics-guided Diffusion Models",
      "link": "https://arxiv.org/abs/2509.21664v1",
      "pdf_link": "https://arxiv.org/pdf/2509.21664v1.pdf",
      "summary": "Stably placing an object in a multi-object scene is a fundamental challenge\nin robotic manipulation, as placements must be penetration-free, establish\nprecise surface contact, and result in a force equilibrium. To assess\nstability, existing methods rely on running a simulation engine or resort to\nheuristic, appearance-based assessments. In contrast, our approach integrates\nstability directly into the sampling process of a diffusion model. To this end,\nwe query an offline sampling-based planner to gather multi-modal placement\nlabels and train a diffusion model to generate stable placements. The diffusion\nmodel is conditioned on scene and object point clouds, and serves as a\ngeometry-aware prior. We leverage the compositional nature of score-based\ngenerative models to combine this learned prior with a stability-aware loss,\nthereby increasing the likelihood of sampling from regions of high stability.\nImportantly, this strategy requires no additional re-training or fine-tuning,\nand can be directly applied to off-the-shelf models. We evaluate our method on\nfour benchmark scenes where stability can be accurately computed. Our\nphysics-guided models achieve placements that are 56% more robust to forceful\nperturbations while reducing runtime by 47% compared to a state-of-the-art\ngeometric method.",
      "published": "2025-09-25T22:32:35Z",
      "authors": [
        "Philippe Nadeau",
        "Miguel Rogel",
        "Ivan Bilić",
        "Ivan Petrović",
        "Jonathan Kelly"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.21664v1",
      "relevance_score": 5
    },
    {
      "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding\n  and Forecasting in Robotic Manipulation",
      "link": "https://arxiv.org/abs/2509.26642v1",
      "pdf_link": "https://arxiv.org/pdf/2509.26642v1.pdf",
      "summary": "Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla",
      "published": "2025-09-30T17:59:50Z",
      "authors": [
        "Zhuoyang Liu",
        "Jiaming Liu",
        "Jiadong Xu",
        "Nuowei Han",
        "Chenyang Gu",
        "Hao Chen",
        "Kaichen Zhou",
        "Renrui Zhang",
        "Kai Chin Hsieh",
        "Kun Wu",
        "Zhengping Che",
        "Jian Tang",
        "Shanghang Zhang"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.26642v1",
      "relevance_score": 5
    },
    {
      "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks",
      "link": "https://arxiv.org/abs/2509.19696v2",
      "pdf_link": "https://arxiv.org/pdf/2509.19696v2.pdf",
      "summary": "Learning methods excel at motion generation in the information domain but are\nnot primarily designed for physical interaction in the energy domain. Impedance\nControl shapes physical interaction but requires task-aware tuning by selecting\nfeasible impedance parameters. We present Diffusion-Based Impedance Learning, a\nframework that combines both domains. A Transformer-based Diffusion Model with\ncross-attention to external wrenches reconstructs a simulated Zero-Force\nTrajectory (sZFT). This captures both translational and rotational task-space\nbehavior. For rotations, we introduce a novel SLERP-based quaternion noise\nscheduler that ensures geometric consistency. The reconstructed sZFT is then\npassed to an energy-based estimator that updates stiffness and damping\nparameters. A directional rule is applied that reduces impedance along non task\naxes while preserving rigidity along task directions. Training data were\ncollected for a parkour scenario and robotic-assisted therapy tasks using\nteleoperation with Apple Vision Pro. With only tens of thousands of samples,\nthe model achieved sub-millimeter positional accuracy and sub-degree rotational\naccuracy. Its compact model size enabled real-time torque control and\nautonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller\nachieved smooth parkour traversal within force and velocity limits and 30/30\nsuccess rates for cylindrical, square, and star peg insertions without any\npeg-specific demonstrations in the training data set. All code for the\nTransformer-based Diffusion Model, the robot controller, and the Apple Vision\nPro telemanipulation framework is publicly available. These results mark an\nimportant step towards Physical AI, fusing model-based control for physical\ninteraction with learning-based methods for trajectory generation.",
      "published": "2025-09-24T02:07:17Z",
      "authors": [
        "Noah Geiger",
        "Tamim Asfour",
        "Neville Hogan",
        "Johannes Lachner"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19696v2",
      "relevance_score": 5
    },
    {
      "title": "Differentiable Skill Optimisation for Powder Manipulation in Laboratory\n  Automation",
      "link": "https://arxiv.org/abs/2510.01438v1",
      "pdf_link": "https://arxiv.org/pdf/2510.01438v1.pdf",
      "summary": "Robotic automation is accelerating scientific discovery by reducing manual\neffort in laboratory workflows. However, precise manipulation of powders\nremains challenging, particularly in tasks such as transport that demand\naccuracy and stability. We propose a trajectory optimisation framework for\npowder transport in laboratory settings, which integrates differentiable\nphysics simulation for accurate modelling of granular dynamics, low-dimensional\nskill-space parameterisation to reduce optimisation complexity, and a\ncurriculum-based strategy that progressively refines task competence over long\nhorizons. This formulation enables end-to-end optimisation of contact-rich\nrobot trajectories while maintaining stability and convergence efficiency.\nExperimental results demonstrate that the proposed method achieves superior\ntask success rates and stability compared to the reinforcement learning\nbaseline.",
      "published": "2025-10-01T20:21:46Z",
      "authors": [
        "Minglun Wei",
        "Xintong Yang",
        "Yu-Kun Lai",
        "Ze Ji"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2510.01438v1",
      "relevance_score": 4
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}