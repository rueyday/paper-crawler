{
  "last_updated": "2025-09-15T08:27:54.648988",
  "total_papers": 25,
  "papers": [
    {
      "title": "Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from\n  Human Proprioceptive Sensorimotor Integration",
      "link": "https://arxiv.org/abs/2509.08354v1",
      "pdf_link": "https://arxiv.org/pdf/2509.08354v1.pdf",
      "summary": "Tactile and kinesthetic perceptions are crucial for human dexterous\nmanipulation, enabling reliable grasping of objects via proprioceptive\nsensorimotor integration. For robotic hands, even though acquiring such tactile\nand kinesthetic feedback is feasible, establishing a direct mapping from this\nsensory feedback to motor actions remains challenging. In this paper, we\npropose a novel glove-mediated tactile-kinematic perception-prediction\nframework for grasp skill transfer from human intuitive and natural operation\nto robotic execution based on imitation learning, and its effectiveness is\nvalidated through generalized grasping tasks, including those involving\ndeformable objects. Firstly, we integrate a data glove to capture tactile and\nkinesthetic data at the joint level. The glove is adaptable for both human and\nrobotic hands, allowing data collection from natural human hand demonstrations\nacross different scenarios. It ensures consistency in the raw data format,\nenabling evaluation of grasping for both human and robotic hands. Secondly, we\nestablish a unified representation of multi-modal inputs based on graph\nstructures with polar coordinates. We explicitly integrate the morphological\ndifferences into the designed representation, enhancing the compatibility\nacross different demonstrators and robotic hands. Furthermore, we introduce the\nTactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage\nmultidimensional subgraph convolutions and attention-based LSTM layers to\nextract spatio-temporal features from graph inputs to predict node-based states\nfor each hand joint. These predictions are then mapped to final commands\nthrough a force-position hybrid mapping.",
      "published": "2025-09-10T07:44:12Z",
      "authors": [
        "Ce Guo",
        "Xieyuanli Chen",
        "Zhiwen Zeng",
        "Zirui Guo",
        "Yihong Li",
        "Haoran Xiao",
        "Dewen Hu",
        "Huimin Lu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.08354v1",
      "relevance_score": 9
    },
    {
      "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
      "link": "https://arxiv.org/abs/2509.04441v2",
      "pdf_link": "https://arxiv.org/pdf/2509.04441v2.pdf",
      "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.",
      "published": "2025-09-04T17:57:13Z",
      "authors": [
        "Hao-Shu Fang",
        "Branden Romero",
        "Yichen Xie",
        "Arthur Hu",
        "Bo-Ruei Huang",
        "Juan Alvarez",
        "Matthew Kim",
        "Gabriel Margolis",
        "Kavya Anbarasu",
        "Masayoshi Tomizuka",
        "Edward Adelson",
        "Pulkit Agrawal"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.04441v2",
      "relevance_score": 9
    },
    {
      "title": "Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator",
      "link": "https://arxiv.org/abs/2509.03859v3",
      "pdf_link": "https://arxiv.org/pdf/2509.03859v3.pdf",
      "summary": "Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM.",
      "published": "2025-09-04T03:36:07Z",
      "authors": [
        "Haichao Zhang",
        "Haonan Yu",
        "Le Zhao",
        "Andrew Choi",
        "Qinxun Bai",
        "Yiqing Yang",
        "Wei Xu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.03859v3",
      "relevance_score": 9
    },
    {
      "title": "Optimizing Grasping in Legged Robots: A Deep Learning Approach to\n  Loco-Manipulation",
      "link": "https://arxiv.org/abs/2508.17466v1",
      "pdf_link": "https://arxiv.org/pdf/2508.17466v1.pdf",
      "summary": "Quadruped robots have emerged as highly efficient and versatile platforms,\nexcelling in navigating complex and unstructured terrains where traditional\nwheeled robots might fail. Equipping these robots with manipulator arms unlocks\nthe advanced capability of loco-manipulation to perform complex physical\ninteraction tasks in areas ranging from industrial automation to\nsearch-and-rescue missions. However, achieving precise and adaptable grasping\nin such dynamic scenarios remains a significant challenge, often hindered by\nthe need for extensive real-world calibration and pre-programmed grasp\nconfigurations. This paper introduces a deep learning framework designed to\nenhance the grasping capabilities of quadrupeds equipped with arms, focusing on\nimproved precision and adaptability. Our approach centers on a sim-to-real\nmethodology that minimizes reliance on physical data collection. We developed a\npipeline within the Genesis simulation environment to generate a synthetic\ndataset of grasp attempts on common objects. By simulating thousands of\ninteractions from various perspectives, we created pixel-wise annotated\ngrasp-quality maps to serve as the ground truth for our model. This dataset was\nused to train a custom CNN with a U-Net-like architecture that processes\nmulti-modal input from an onboard RGB and depth cameras, including RGB images,\ndepth maps, segmentation masks, and surface normal maps. The trained model\noutputs a grasp-quality heatmap to identify the optimal grasp point. We\nvalidated the complete framework on a four-legged robot. The system\nsuccessfully executed a full loco-manipulation task: autonomously navigating to\na target object, perceiving it with its sensors, predicting the optimal grasp\npose using our model, and performing a precise grasp. This work proves that\nleveraging simulated training with advanced sensing offers a scalable and\neffective solution for object handling.",
      "published": "2025-08-24T17:47:56Z",
      "authors": [
        "Dilermando Almeida",
        "Guilherme Lazzarini",
        "Juliano Negri",
        "Thiago H. Segreto",
        "Ricardo V. Godoy",
        "Marcelo Becker"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.17466v1",
      "relevance_score": 9
    },
    {
      "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration",
      "link": "https://arxiv.org/abs/2509.09671v1",
      "pdf_link": "https://arxiv.org/pdf/2509.09671v1.pdf",
      "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation.",
      "published": "2025-09-11T17:59:07Z",
      "authors": [
        "Sirui Xu",
        "Yu-Wei Chao",
        "Liuyu Bian",
        "Arsalan Mousavian",
        "Yu-Xiong Wang",
        "Liang-Yan Gui",
        "Wei Yang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.09671v1",
      "relevance_score": 8
    },
    {
      "title": "Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm\n  Robotic Manipulation",
      "link": "https://arxiv.org/abs/2509.07957v1",
      "pdf_link": "https://arxiv.org/pdf/2509.07957v1.pdf",
      "summary": "Acquiring dexterous robotic skills from human video demonstrations remains a\nsignificant challenge, largely due to conventional reliance on low-level\ntrajectory replication, which often fails to generalize across varying objects,\nspatial layouts, and manipulator configurations. To address this limitation, we\nintroduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that\nenables dual-arm robotic systems to perform task-level reasoning and execution\ndirectly from RGB-D human demonstrations. GF-VLA employs an\ninformation-theoretic approach to extract task-relevant cues, selectively\nhighlighting critical hand-object and object-object interactions. These cues\nare structured into temporally ordered scene graphs, which are subsequently\nintegrated with a language-conditioned transformer to produce hierarchical\nbehavior trees and interpretable Cartesian motion primitives. To enhance\nefficiency in bimanual execution, we propose a cross-arm allocation strategy\nthat autonomously determines gripper assignment without requiring explicit\ngeometric modeling. We validate GF-VLA on four dual-arm block assembly\nbenchmarks involving symbolic structure construction and spatial\ngeneralization. Empirical results demonstrate that the proposed representation\nachieves over 95% graph accuracy and 93% subtask segmentation, enabling the\nlanguage-action planner to generate robust, interpretable task policies. When\ndeployed on a dual-arm robot, these policies attain 94% grasp reliability, 89%\nplacement accuracy, and 90% overall task success across stacking,\nletter-formation, and geometric reconfiguration tasks, evidencing strong\ngeneralization and robustness under diverse spatial and semantic variations.",
      "published": "2025-09-09T17:44:36Z",
      "authors": [
        "Shunlei Li",
        "Longsen Gao",
        "Jiuwen Cao",
        "Yingbai Hu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.07957v1",
      "relevance_score": 7
    },
    {
      "title": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward\n  Functions",
      "link": "https://arxiv.org/abs/2509.07445v1",
      "pdf_link": "https://arxiv.org/pdf/2509.07445v1.pdf",
      "summary": "Large language models (LLMs) are beginning to automate reward design for\ndexterous manipulation. However, no prior work has considered tactile sensing,\nwhich is known to be critical for human-like dexterity. We present Text2Touch,\nbringing LLM-crafted rewards to the challenging task of multi-axis in-hand\nobject rotation with real-world vision based tactile sensing in palm-up and\npalm-down configurations. Our prompt engineering strategy scales to over 70\nenvironment variables, and sim-to-real distillation enables successful policy\ntransfer to a tactile-enabled fully actuated four-fingered dexterous robot\nhand. Text2Touch significantly outperforms a carefully tuned human-engineered\nbaseline, demonstrating superior rotation speed and stability while relying on\nreward functions that are an order of magnitude shorter and simpler. These\nresults illustrate how LLM-designed rewards can significantly reduce the time\nfrom concept to deployable dexterous tactile skills, supporting more rapid and\nscalable multimodal robot learning. Project website:\nhttps://hpfield.github.io/text2touch-website",
      "published": "2025-09-09T07:10:39Z",
      "authors": [
        "Harrison Field",
        "Max Yang",
        "Yijiong Lin",
        "Efi Psomopoulou",
        "David Barton",
        "Nathan F. Lepora"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.07445v1",
      "relevance_score": 7
    },
    {
      "title": "First Plan Then Evaluate: Use a Vectorized Motion Planner for Grasping",
      "link": "https://arxiv.org/abs/2509.07162v1",
      "pdf_link": "https://arxiv.org/pdf/2509.07162v1.pdf",
      "summary": "Autonomous multi-finger grasping is a fundamental capability in robotic\nmanipulation. Optimization-based approaches show strong performance, but tend\nto be sensitive to initialization and are potentially time-consuming. As an\nalternative, the generator-evaluator-planner framework has been proposed. A\ngenerator generates grasp candidates, an evaluator ranks the proposed grasps,\nand a motion planner plans a trajectory to the highest-ranked grasp. If the\nplanner doesn't find a trajectory, a new trajectory optimization is started\nwith the next-best grasp as the target and so on. However, executing\nlower-ranked grasps means a lower chance of grasp success, and multiple\ntrajectory optimizations are time-consuming. Alternatively, relaxing the\nthreshold for motion planning accuracy allows for easier computation of a\nsuccessful trajectory but implies lower accuracy in estimating grasp success\nlikelihood. It's a lose-lose proposition: either spend more time finding a\nsuccessful trajectory or have a worse estimate of grasp success. We propose a\nframework that plans trajectories to a set of generated grasp targets in\nparallel, the evaluator estimates the grasp success likelihood of the resulting\ntrajectories, and the robot executes the trajectory most likely to succeed. To\nplan trajectories to different targets efficiently, we propose the use of a\nvectorized motion planner. Our experiments show our approach improves over the\ntraditional generator-evaluator-planner framework across different objects,\ngenerators, and motion planners, and successfully generalizes to novel\nenvironments in the real world, including different shelves and table heights.\nProject website https://sites.google.com/view/fpte",
      "published": "2025-09-08T19:21:42Z",
      "authors": [
        "Martin Matak",
        "Mohanraj Devendran Ashanti",
        "Karl Van Wyk",
        "Tucker Hermans"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.07162v1",
      "relevance_score": 7
    },
    {
      "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense\n  Correspondence and Visuotactile Affordance",
      "link": "https://arxiv.org/abs/2509.03889v1",
      "pdf_link": "https://arxiv.org/pdf/2509.03889v1.pdf",
      "summary": "Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.",
      "published": "2025-09-04T05:16:56Z",
      "authors": [
        "Neha Sunil",
        "Megha Tippur",
        "Arnau Saumell",
        "Edward Adelson",
        "Alberto Rodriguez"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.03889v1",
      "relevance_score": 7
    },
    {
      "title": "GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System",
      "link": "https://arxiv.org/abs/2508.15990v1",
      "pdf_link": "https://arxiv.org/pdf/2508.15990v1.pdf",
      "summary": "Accurately perceiving an object's pose and shape is essential for precise\ngrasping and manipulation. Compared to common vision-based methods, tactile\nsensing offers advantages in precision and immunity to occlusion when tracking\nand reconstructing objects in contact. This makes it particularly valuable for\nin-hand and other high-precision manipulation tasks. In this work, we present\nGelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to\nestimate object pose over long periods and reconstruct object shapes with high\nfidelity. Unlike traditional point cloud-based approaches, GelSLAM uses\ntactile-derived surface normals and curvatures for robust tracking and loop\nclosure. It can track object motion in real time with low error and minimal\ndrift, and reconstruct shapes with submillimeter accuracy, even for low-texture\nobjects such as wooden tools. GelSLAM extends tactile sensing beyond local\ncontact to enable global, long-horizon spatial perception, and we believe it\nwill serve as a foundation for many precise manipulation tasks involving\ninteraction with objects in hand. The video demo is available on our website:\nhttps://joehjhuang.github.io/gelslam.",
      "published": "2025-08-21T22:20:43Z",
      "authors": [
        "Hung-Jui Huang",
        "Mohammad Amin Mirzaee",
        "Michael Kaess",
        "Wenzhen Yuan"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.15990v1",
      "relevance_score": 7
    },
    {
      "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous\n  Manipulation",
      "link": "https://arxiv.org/abs/2509.05513v1",
      "pdf_link": "https://arxiv.org/pdf/2509.05513v1.pdf",
      "summary": "Egocentric human videos provide scalable demonstrations for imitation\nlearning, but existing corpora often lack either fine-grained, temporally\nlocalized action descriptions or dexterous hand annotations. We introduce\nOpenEgo, a multimodal egocentric manipulation dataset with standardized\nhand-pose annotations and intention-aligned action primitives. OpenEgo totals\n1107 hours across six public datasets, covering 290 manipulation tasks in 600+\nenvironments. We unify hand-pose layouts and provide descriptive, timestamped\naction primitives. To validate its utility, we train language-conditioned\nimitation-learning policies to predict dexterous hand trajectories. OpenEgo is\ndesigned to lower the barrier to learning dexterous manipulation from\negocentric video and to support reproducible research in vision-language-action\nlearning. All resources and instructions will be released at\nwww.openegocentric.com.",
      "published": "2025-09-05T21:47:55Z",
      "authors": [
        "Ahad Jawaid",
        "Yu Xiang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.05513v1",
      "relevance_score": 6
    },
    {
      "title": "Inference of Human-derived Specifications of Object Placement via\n  Demonstration",
      "link": "https://arxiv.org/abs/2508.19367v1",
      "pdf_link": "https://arxiv.org/pdf/2508.19367v1.pdf",
      "summary": "As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,\nobject packing, sorting, and kitting), methods focused on understanding\nhuman-acceptable object configurations remain limited expressively with regard\nto capturing spatial relationships important to humans. To advance robotic\nunderstanding of human rules for object arrangement, we introduce\npositionally-augmented RCC (PARCC), a formal logic framework based on region\nconnection calculus (RCC) for describing the relative position of objects in\nspace. Additionally, we introduce an inference algorithm for learning PARCC\nspecifications via demonstrations. Finally, we present the results from a human\nstudy, which demonstrate our framework's ability to capture a human's intended\nspecification and the benefits of learning from demonstration approaches over\nhuman-provided specifications.",
      "published": "2025-08-26T18:57:21Z",
      "authors": [
        "Alex Cuellar",
        "Ho Chit Siu",
        "Julie A Shah"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.19367v1",
      "relevance_score": 6
    },
    {
      "title": "TASC: Task-Aware Shared Control for Teleoperated Manipulation",
      "link": "https://arxiv.org/abs/2509.10416v1",
      "pdf_link": "https://arxiv.org/pdf/2509.10416v1.pdf",
      "summary": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc.",
      "published": "2025-09-12T17:13:18Z",
      "authors": [
        "Ze Fu",
        "Pinhao Song",
        "Yutong Hu",
        "Renaud Detry"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.10416v1",
      "relevance_score": 5
    },
    {
      "title": "TARA: A Low-Cost 3D-Printed Robotic Arm for Accessible Robotics\n  Education",
      "link": "https://arxiv.org/abs/2509.01043v1",
      "pdf_link": "https://arxiv.org/pdf/2509.01043v1.pdf",
      "summary": "The high cost of robotic platforms limits students' ability to gain practical\nskills directly applicable in real-world scenarios. To address this challenge,\nthis paper presents TARA, a low-cost, 3D-printed robotic arm designed for\naccessible robotics education. TARA includes an open-source repository with\ndesign files, assembly instructions, and baseline code, enabling users to build\nand customize the platform. The system balances affordability and\nfunctionality, offering a highly capable robotic arm for approximately 200 USD,\nsignificantly lower than industrial systems that often cost thousands of\ndollars. Experimental validation confirmed accurate performance in basic\nmanipulation tasks. Rather than focusing on performance benchmarking, this work\nprioritizes educational reproducibility, providing a platform that students and\neducators can reliably replicate and extend.",
      "published": "2025-09-01T00:50:20Z",
      "authors": [
        "Thays Leach Mitre"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.01043v1",
      "relevance_score": 5
    },
    {
      "title": "ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with\n  Occupancy-aware 3D Trajectory",
      "link": "https://arxiv.org/abs/2509.05314v1",
      "pdf_link": "https://arxiv.org/pdf/2509.05314v1.pdf",
      "summary": "Data scarcity continues to be a major challenge in the field of robotic\nmanipulation. Although diffusion models provide a promising solution for\ngenerating robotic manipulation videos, existing methods largely depend on 2D\ntrajectories, which inherently face issues with 3D spatial ambiguity. In this\nwork, we present a novel framework named ManipDreamer3D for generating\nplausible 3D-aware robotic manipulation videos from the input image and the\ntext instruction. Our method combines 3D trajectory planning with a\nreconstructed 3D occupancy map created from a third-person perspective, along\nwith a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D\nfirst reconstructs the 3D occupancy representation from the input image and\nthen computes an optimized 3D end-effector trajectory, minimizing path length\nwhile avoiding collisions. Next, we employ a latent editing technique to create\nvideo sequences from the initial image latent and the optimized 3D trajectory.\nThis process conditions our specially trained trajectory-to-video diffusion\nmodel to produce robotic pick-and-place videos. Our method generates robotic\nvideos with autonomously planned plausible 3D trajectories, significantly\nreducing human intervention requirements. Experimental results demonstrate\nsuperior visual quality compared to existing methods.",
      "published": "2025-08-29T10:39:06Z",
      "authors": [
        "Ying Li",
        "Xiaobao Wei",
        "Xiaowei Chi",
        "Yuming Li",
        "Zhongyu Zhao",
        "Hao Wang",
        "Ningning Ma",
        "Ming Lu",
        "Shanghang Zhang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.05314v1",
      "relevance_score": 5
    },
    {
      "title": "Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe\n  ZYZ Regrasp on a Doosan M0609",
      "link": "https://arxiv.org/abs/2508.21272v1",
      "pdf_link": "https://arxiv.org/pdf/2508.21272v1.pdf",
      "summary": "This paper presents the first comprehensive application of legal-action\nmasked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated\ngripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly\nlearning. Our approach represents the first systematic integration of\nconstraint-aware reinforcement learning with singularity-safe motion planning\non a Doosan M0609 collaborative robot. We address critical challenges in\nrobotic manipulation: combinatorial action space explosion, unsafe motion\nplanning, and systematic assembly strategy learning. Our system integrates a\nlegal-action masked DQN with hierarchical architecture that decomposes\nQ-function estimation into orientation and position components, reducing\ncomputational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining\nsolution completeness. The robot-friendly reward function encourages\nground-first, vertically accessible assembly sequences aligned with\nmanipulation constraints. Curriculum learning across three progressive\ndifficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training\nefficiency: 100\\% success rate for Level 1 within 500 episodes, 92.9\\% for\nLevel 2, and 39.9\\% for Level 3 over 105,300 total training episodes.",
      "published": "2025-08-29T00:27:03Z",
      "authors": [
        "Jaehong Oh",
        "Seungjun Jung",
        "Sawoong Kim"
      ],
      "categories": [
        "cs.RO",
        "stat.CO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.21272v1",
      "relevance_score": 5
    },
    {
      "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation",
      "link": "https://arxiv.org/abs/2508.21063v1",
      "pdf_link": "https://arxiv.org/pdf/2508.21063v1.pdf",
      "summary": "Creating assembly products demands significant manual effort and expert\nknowledge in 1) designing the assembly and 2) constructing the product. This\npaper introduces Prompt-to-Product, an automated pipeline that generates\nreal-world assembly products from natural language prompts. Specifically, we\nleverage LEGO bricks as the assembly platform and automate the process of\ncreating brick assembly structures. Given the user design requirements,\nPrompt-to-Product generates physically buildable brick designs, and then\nleverages a bimanual robotic system to construct the real assembly products,\nbringing user imaginations into the real world. We conduct a comprehensive user\nstudy, and the results demonstrate that Prompt-to-Product significantly lowers\nthe barrier and reduces manual effort in creating assembly products from\nimaginative ideas.",
      "published": "2025-08-28T17:59:05Z",
      "authors": [
        "Ruixuan Liu",
        "Philip Huang",
        "Ava Pun",
        "Kangle Deng",
        "Shobhit Aggarwal",
        "Kevin Tang",
        "Michelle Liu",
        "Deva Ramanan",
        "Jun-Yan Zhu",
        "Jiaoyang Li",
        "Changliu Liu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.21063v1",
      "relevance_score": 5
    },
    {
      "title": "Classification of Vision-Based Tactile Sensors: A Review",
      "link": "https://arxiv.org/abs/2509.02478v2",
      "pdf_link": "https://arxiv.org/pdf/2509.02478v2.pdf",
      "summary": "Vision-based tactile sensors (VBTS) have gained widespread application in\nrobotic hands, grippers and prosthetics due to their high spatial resolution,\nlow manufacturing costs, and ease of customization. While VBTSs have common\ndesign features, such as a camera module, they can differ in a rich diversity\nof sensing principles, material compositions, multimodal approaches, and data\ninterpretation methods. Here, we propose a novel classification of VBTS that\ncategorizes the technology into two primary sensing principles based on the\nunderlying transduction of contact into a tactile image: the Marker-Based\nTransduction Principle and the Intensity-Based Transduction Principle.\nMarker-Based Transduction interprets tactile information by detecting marker\ndisplacement and changes in marker density. In contrast, Intensity-Based\nTransduction maps external disturbances with variations in pixel values.\nDepending on the design of the contact module, Marker-Based Transduction can be\nfurther divided into two subtypes: Simple Marker-Based (SMB) and Morphological\nMarker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction\nPrinciple encompasses the Reflective Layer-based (RLB) and Transparent\nLayer-Based (TLB) mechanisms. This paper provides a comparative study of the\nhardware characteristics of these four types of sensors including various\ncombination types, and discusses the commonly used methods for interpreting\ntactile information. This~comparison reveals some current challenges faced by\nVBTS technology and directions for future research.",
      "published": "2025-09-02T16:29:06Z",
      "authors": [
        "Haoran Li",
        "Yijiong Lin",
        "Chenghua Lu",
        "Max Yang",
        "Efi Psomopoulou",
        "Nathan F Lepora"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.02478v2",
      "relevance_score": 5
    },
    {
      "title": "One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain\n  Randomization for One-Shot 6D Pose Estimation",
      "link": "https://arxiv.org/abs/2509.07978v1",
      "pdf_link": "https://arxiv.org/pdf/2509.07978v1.pdf",
      "summary": "Estimating the 6D pose of arbitrary unseen objects from a single reference\nimage is critical for robotics operating in the long-tail of real-world\ninstances. However, this setting is notoriously challenging: 3D models are\nrarely available, single-view reconstructions lack metric scale, and domain\ngaps between generated models and real-world images undermine robustness. We\npropose OnePoseViaGen, a pipeline that tackles these challenges through two key\ncomponents. First, a coarse-to-fine alignment module jointly refines scale and\npose by combining multi-view feature matching with render-and-compare\nrefinement. Second, a text-guided generative domain randomization strategy\ndiversifies textures, enabling effective fine-tuning of pose estimators with\nsynthetic data. Together, these steps allow high-fidelity single-view 3D\ngeneration to support reliable one-shot 6D pose estimation. On challenging\nbenchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves\nstate-of-the-art performance far surpassing prior approaches. We further\ndemonstrate robust dexterous grasping with a real robot hand, validating the\npracticality of our method in real-world manipulation. Project page:\nhttps://gzwsama.github.io/OnePoseviaGen.github.io/",
      "published": "2025-09-09T17:59:02Z",
      "authors": [
        "Zheng Geng",
        "Nan Wang",
        "Shaocong Xu",
        "Chongjie Ye",
        "Bohan Li",
        "Zhaoxi Chen",
        "Sida Peng",
        "Hao Zhao"
      ],
      "categories": [
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.07978v1",
      "relevance_score": 5
    },
    {
      "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and\n  Manipulation",
      "link": "https://arxiv.org/abs/2508.17643v1",
      "pdf_link": "https://arxiv.org/pdf/2508.17643v1.pdf",
      "summary": "Event cameras offer microsecond latency, high dynamic range, and low power\nconsumption, making them ideal for real-time robotic perception under\nchallenging conditions such as motion blur, occlusion, and illumination\nchanges. However, despite their advantages, synthetic event-based vision\nremains largely unexplored in mainstream robotics simulators. This lack of\nsimulation setup hinders the evaluation of event-driven approaches for robotic\nmanipulation and navigation tasks. This work presents an open-source,\nuser-friendly v2e robotics operating system (ROS) package for Gazebo simulation\nthat enables seamless event stream generation from RGB camera feeds. The\npackage is used to investigate event-based robotic policies (ERP) for real-time\nnavigation and manipulation. Two representative scenarios are evaluated: (1)\nobject following with a mobile robot and (2) object detection and grasping with\na robotic manipulator. Transformer-based ERPs are trained by behavior cloning\nand compared to RGB-based counterparts under various operating conditions.\nExperimental results show that event-guided policies consistently deliver\ncompetitive advantages. The results highlight the potential of event-driven\nperception to improve real-time robotic navigation and manipulation, providing\na foundation for broader integration of event cameras into robotic policy\nlearning. The GitHub repo for the dataset and code:\nhttps://eventbasedvision.github.io/SEBVS/",
      "published": "2025-08-25T04:14:04Z",
      "authors": [
        "Krishna Vinod",
        "Prithvi Jai Ramesh",
        "Pavan Kumar B N",
        "Bharatesh Chakravarthi"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.17643v1",
      "relevance_score": 5
    },
    {
      "title": "Towards simulation-based optimization of compliant fingers for\n  high-speed connector assembly",
      "link": "https://arxiv.org/abs/2509.10012v1",
      "pdf_link": "https://arxiv.org/pdf/2509.10012v1.pdf",
      "summary": "Mechanical compliance is a key design parameter for dynamic contact-rich\nmanipulation, affecting task success and safety robustness over contact\ngeometry variation. Design of soft robotic structures, such as compliant\nfingers, requires choosing design parameters which affect geometry and\nstiffness, and therefore manipulation performance and robustness. Today, these\nparameters are chosen through either hardware iteration, which takes\nsignificant development time, or simplified models (e.g. planar), which can't\naddress complex manipulation task objectives. Improvements in dynamic\nsimulation, especially with contact and friction modeling, present a potential\ndesign tool for mechanical compliance. We propose a simulation-based design\ntool for compliant mechanisms which allows design with respect to task-level\nobjectives, such as success rate. This is applied to optimize design parameters\nof a structured compliant finger to reduce failure cases inside a tolerance\nwindow in insertion tasks. The improvement in robustness is then validated on a\nreal robot using tasks from the benchmark NIST task board. The finger stiffness\naffects the tolerance window: optimized parameters can increase tolerable\nranges by a factor of 2.29, with workpiece variation up to 8.6 mm being\ncompensated. However, the trends remain task-specific. In some tasks, the\nhighest stiffness yields the widest tolerable range, whereas in others the\nopposite is observed, motivating need for design tools which can consider\napplication-specific geometry and dynamics.",
      "published": "2025-09-12T07:14:34Z",
      "authors": [
        "Richard Matthias Hartisch",
        "Alexander Rother",
        "Jörg Krüger",
        "Kevin Haninger"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.10012v1",
      "relevance_score": 4
    },
    {
      "title": "BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object\n  Bagging",
      "link": "https://arxiv.org/abs/2509.09484v1",
      "pdf_link": "https://arxiv.org/pdf/2509.09484v1.pdf",
      "summary": "Bagging tasks, commonly found in industrial scenarios, are challenging\nconsidering deformable bags' complicated and unpredictable nature. This paper\npresents an automated bagging system from the proposed adaptive\nStructure-of-Interest (SOI) manipulation strategy for dual robot arms. The\nsystem dynamically adjusts its actions based on real-time visual feedback,\nremoving the need for pre-existing knowledge of bag properties. Our framework\nincorporates Gaussian Mixture Models (GMM) for estimating SOI states,\noptimization techniques for SOI generation, motion planning via Constrained\nBidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination\nusing Model Predictive Control (MPC). Extensive experiments validate the\ncapability of our system to perform precise and robust bagging across various\nobjects, showcasing its adaptability. This work offers a new solution for\nrobotic deformable object manipulation (DOM), particularly in automated bagging\ntasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.",
      "published": "2025-09-11T14:15:20Z",
      "authors": [
        "Peng Zhou",
        "Jiaming Qi",
        "Hongmin Wu",
        "Chen Wang",
        "Yizhou Chen",
        "Zeqing Zhang"
      ],
      "categories": [
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.09484v1",
      "relevance_score": 4
    },
    {
      "title": "Attribute-based Object Grounding and Robot Grasp Detection with Spatial\n  Reasoning",
      "link": "https://arxiv.org/abs/2509.08126v1",
      "pdf_link": "https://arxiv.org/pdf/2509.08126v1.pdf",
      "summary": "Enabling robots to grasp objects specified through natural language is\nessential for effective human-robot interaction, yet it remains a significant\nchallenge. Existing approaches often struggle with open-form language\nexpressions and typically assume unambiguous target objects without duplicates.\nMoreover, they frequently rely on costly, dense pixel-wise annotations for both\nobject grounding and grasp configuration. We present Attribute-based Object\nGrounding and Robotic Grasping (OGRG), a novel framework that interprets\nopen-form language expressions and performs spatial reasoning to ground target\nobjects and predict planar grasp poses, even in scenes containing duplicated\nobject instances. We investigate OGRG in two settings: (1) Referring Grasp\nSynthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp\nAffordance (RGA) using weakly supervised learning with only single-pixel grasp\nannotations. Key contributions include a bi-directional vision-language fusion\nmodule and the integration of depth information to enhance geometric reasoning,\nimproving both grounding and grasping performance. Experiment results show that\nOGRG outperforms strong baselines in tabletop scenes with diverse spatial\nlanguage instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX\n2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential\ngrasping, while delivering superior grounding and grasp prediction accuracy\ncompared to all the baselines considered. Under the weakly supervised RGA\nsetting, OGRG also surpasses baseline grasp-success rates in both simulation\nand real-robot trials, underscoring the effectiveness of its spatial reasoning\ndesign. Project page: https://z.umn.edu/ogrg",
      "published": "2025-09-09T20:07:51Z",
      "authors": [
        "Houjian Yu",
        "Zheming Zhou",
        "Min Sun",
        "Omid Ghasemalizadeh",
        "Yuyin Sun",
        "Cheng-Hao Kuo",
        "Arnie Sen",
        "Changhyun Choi"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.08126v1",
      "relevance_score": 4
    },
    {
      "title": "A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation",
      "link": "https://arxiv.org/abs/2508.20831v1",
      "pdf_link": "https://arxiv.org/pdf/2508.20831v1.pdf",
      "summary": "This paper presents a novel fabric-based thermal-haptic interface for virtual\nreality and teleoperation. It integrates pneumatic actuation and conductive\nfabric with an innovative ultra-lightweight design, achieving only 2~g for each\nfinger unit. By embedding heating elements within textile pneumatic chambers,\nthe system delivers modulated pressure and thermal stimuli to fingerpads\nthrough a fully soft, wearable interface.\n  Comprehensive characterization demonstrates rapid thermal modulation with\nheating rates up to 3$^{\\circ}$C/s, enabling dynamic thermal feedback for\nvirtual or teleoperation interactions. The pneumatic subsystem generates forces\nup to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance\nenhances cooling efficiency with minimal force reduction. Experimental\nvalidation conducted with two different user studies shows high temperature\nidentification accuracy (0.98 overall) across three thermal levels, and\nsignificant manipulation improvements in a virtual pick-and-place tasks.\nResults show enhanced success rates (88.5\\% to 96.4\\%, p = 0.029) and improved\nforce control precision (p = 0.013) when haptic feedback is enabled, validating\nthe effectiveness of the integrated thermal-haptic approach for advanced\nhuman-machine interaction applications.",
      "published": "2025-08-28T14:25:35Z",
      "authors": [
        "Rui Chen",
        "Domenico Chiaradia",
        "Antonio Frisoli",
        "Daniele Leonardis"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.20831v1",
      "relevance_score": 4
    },
    {
      "title": "ManiFlow: A General Robot Manipulation Policy via Consistency Flow\n  Training",
      "link": "https://arxiv.org/abs/2509.01819v1",
      "pdf_link": "https://arxiv.org/pdf/2509.01819v1.pdf",
      "summary": "This paper introduces ManiFlow, a visuomotor imitation learning policy for\ngeneral robot manipulation that generates precise, high-dimensional actions\nconditioned on diverse visual, language and proprioceptive inputs. We leverage\nflow matching with consistency training to enable high-quality dexterous action\ngeneration in just 1-2 inference steps. To handle diverse input modalities\nefficiently, we propose DiT-X, a diffusion transformer architecture with\nadaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained\nfeature interactions between action tokens and multi-modal observations.\nManiFlow demonstrates consistent improvements across diverse simulation\nbenchmarks and nearly doubles success rates on real-world tasks across\nsingle-arm, bimanual, and humanoid robot setups with increasing dexterity. The\nextensive evaluation further demonstrates the strong robustness and\ngeneralizability of ManiFlow to novel objects and background changes, and\nhighlights its strong scaling capability with larger-scale datasets. Our\nwebsite: maniflow-policy.github.io.",
      "published": "2025-09-01T22:50:55Z",
      "authors": [
        "Ge Yan",
        "Jiyue Zhu",
        "Yuquan Deng",
        "Shiqi Yang",
        "Ri-Zhao Qiu",
        "Xuxin Cheng",
        "Marius Memmel",
        "Ranjay Krishna",
        "Ankit Goyal",
        "Xiaolong Wang",
        "Dieter Fox"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.01819v1",
      "relevance_score": 4
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}