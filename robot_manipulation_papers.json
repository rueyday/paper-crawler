{
  "last_updated": "2025-07-21T08:34:23.394411",
  "total_papers": 25,
  "papers": [
    {
      "title": "Hierarchical Reinforcement Learning for Articulated Tool Manipulation\n  with Multifingered Hand",
      "link": "https://arxiv.org/abs/2507.06822v1",
      "pdf_link": "https://arxiv.org/pdf/2507.06822v1.pdf",
      "summary": "Manipulating articulated tools, such as tweezers or scissors, has rarely been\nexplored in previous research. Unlike rigid tools, articulated tools change\ntheir shape dynamically, creating unique challenges for dexterous robotic\nhands. In this work, we present a hierarchical, goal-conditioned reinforcement\nlearning (GCRL) framework to improve the manipulation capabilities of\nanthropomorphic robotic hands using articulated tools. Our framework comprises\ntwo policy layers: (1) a low-level policy that enables the dexterous hand to\nmanipulate the tool into various configurations for objects of different sizes,\nand (2) a high-level policy that defines the tool's goal state and controls the\nrobotic arm for object-picking tasks. We employ an encoder, trained on\nsynthetic pointclouds, to estimate the tool's affordance states--specifically,\nhow different tool configurations (e.g., tweezer opening angles) enable\ngrasping of objects of varying sizes--from input point clouds, thereby enabling\nprecise tool manipulation. We also utilize a privilege-informed heuristic\npolicy to generate replay buffer, improving the training efficiency of the\nhigh-level policy. We validate our approach through real-world experiments,\nshowing that the robot can effectively manipulate a tweezer-like tool to grasp\nobjects of diverse shapes and sizes with a 70.8 % success rate. This study\nhighlights the potential of RL to advance dexterous robotic manipulation of\narticulated tools.",
      "published": "2025-07-09T13:11:12Z",
      "authors": [
        "Wei Xu",
        "Yanchao Zhao",
        "Weichao Guo",
        "Xinjun Sheng"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.06822v1",
      "relevance_score": 8
    },
    {
      "title": "A segmented robot grasping perception neural network for edge AI",
      "link": "https://arxiv.org/abs/2507.13970v1",
      "pdf_link": "https://arxiv.org/pdf/2507.13970v1.pdf",
      "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "published": "2025-07-18T14:32:45Z",
      "authors": [
        "Casper Bröcheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico Möckel"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.13970v1",
      "relevance_score": 7
    },
    {
      "title": "Regrasp Maps for Sequential Manipulation Planning",
      "link": "https://arxiv.org/abs/2507.12407v1",
      "pdf_link": "https://arxiv.org/pdf/2507.12407v1.pdf",
      "summary": "We consider manipulation problems in constrained and cluttered settings,\nwhich require several regrasps at unknown locations. We propose to inform an\noptimization-based task and motion planning (TAMP) solver with possible regrasp\nareas and grasp sequences to speed up the search. Our main idea is to use a\nstate space abstraction, a regrasp map, capturing the combinations of available\ngrasps in different parts of the configuration space, and allowing us to\nprovide the solver with guesses for the mode switches and additional\nconstraints for the object placements. By interleaving the creation of regrasp\nmaps, their adaptation based on failed refinements, and solving TAMP\n(sub)problems, we are able to provide a robust search method for challenging\nregrasp manipulation problems.",
      "published": "2025-07-16T16:53:07Z",
      "authors": [
        "Svetlana Levit",
        "Marc Toussaint"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12407v1",
      "relevance_score": 7
    },
    {
      "title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with\n  On-Generator Training",
      "link": "https://arxiv.org/abs/2507.13097v1",
      "pdf_link": "https://arxiv.org/pdf/2507.13097v1.pdf",
      "summary": "Grasping is a fundamental robot skill, yet despite significant research\nadvancements, learning-based 6-DOF grasping approaches are still not turnkey\nand struggle to generalize across different embodiments and in-the-wild\nsettings. We build upon the recent success on modeling the object-centric grasp\ngeneration process as an iterative diffusion process. Our proposed framework,\nGraspGen, consists of a DiffusionTransformer architecture that enhances grasp\ngeneration, paired with an efficient discriminator to score and filter sampled\ngrasps. We introduce a novel and performant on-generator training recipe for\nthe discriminator. To scale GraspGen to both objects and grippers, we release a\nnew simulated dataset consisting of over 53 million grasps. We demonstrate that\nGraspGen outperforms prior methods in simulations with singulated objects\nacross different grippers, achieves state-of-the-art performance on the\nFetchBench grasping benchmark, and performs well on a real robot with noisy\nvisual observations.",
      "published": "2025-07-17T13:09:28Z",
      "authors": [
        "Adithyavairavan Murali",
        "Balakumar Sundaralingam",
        "Yu-Wei Chao",
        "Wentao Yuan",
        "Jun Yamada",
        "Mark Carlson",
        "Fabio Ramos",
        "Stan Birchfield",
        "Dieter Fox",
        "Clemens Eppner"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.13097v1",
      "relevance_score": 6
    },
    {
      "title": "A Multi-Level Similarity Approach for Single-View Object Grasping:\n  Matching, Planning, and Fine-Tuning",
      "link": "https://arxiv.org/abs/2507.11938v1",
      "pdf_link": "https://arxiv.org/pdf/2507.11938v1.pdf",
      "summary": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.",
      "published": "2025-07-16T06:07:57Z",
      "authors": [
        "Hao Chen",
        "Takuya Kiyokawa",
        "Zhengtao Hu",
        "Weiwei Wan",
        "Kensuke Harada"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.11938v1",
      "relevance_score": 6
    },
    {
      "title": "The Developments and Challenges towards Dexterous and Embodied Robotic\n  Manipulation: A Survey",
      "link": "https://arxiv.org/abs/2507.11840v1",
      "pdf_link": "https://arxiv.org/pdf/2507.11840v1.pdf",
      "summary": "Achieving human-like dexterous robotic manipulation remains a central goal\nand a pivotal challenge in robotics. The development of Artificial Intelligence\n(AI) has allowed rapid progress in robotic manipulation. This survey summarizes\nthe evolution of robotic manipulation from mechanical programming to embodied\nintelligence, alongside the transition from simple grippers to multi-fingered\ndexterous hands, outlining key characteristics and main challenges. Focusing on\nthe current stage of embodied dexterous manipulation, we highlight recent\nadvances in two critical areas: dexterous manipulation data collection (via\nsimulation, human demonstrations, and teleoperation) and skill-learning\nframeworks (imitation and reinforcement learning). Then, based on the overview\nof the existing data collection paradigm and learning framework, three key\nchallenges restricting the development of dexterous robotic manipulation are\nsummarized and discussed.",
      "published": "2025-07-16T02:09:31Z",
      "authors": [
        "Gaofeng Li",
        "Ruize Wang",
        "Peisen Xu",
        "Qi Ye",
        "Jiming Chen"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.11840v1",
      "relevance_score": 6
    },
    {
      "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model",
      "link": "https://arxiv.org/abs/2507.09985v1",
      "pdf_link": "https://arxiv.org/pdf/2507.09985v1.pdf",
      "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.",
      "published": "2025-07-14T07:05:36Z",
      "authors": [
        "Samson Yu",
        "Kelvin Lin",
        "Harold Soh"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.09985v1",
      "relevance_score": 6
    },
    {
      "title": "ConViTac: Aligning Visual-Tactile Fusion with Contrastive\n  Representations",
      "link": "https://arxiv.org/abs/2506.20757v1",
      "pdf_link": "https://arxiv.org/pdf/2506.20757v1.pdf",
      "summary": "Vision and touch are two fundamental sensory modalities for robots, offering\ncomplementary information that enhances perception and manipulation tasks.\nPrevious research has attempted to jointly learn visual-tactile representations\nto extract more meaningful information. However, these approaches often rely on\ndirect combination, such as feature addition and concatenation, for modality\nfusion, which tend to result in poor feature integration. In this paper, we\npropose ConViTac, a visual-tactile representation learning network designed to\nenhance the alignment of features during fusion using contrastive\nrepresentations. Our key contribution is a Contrastive Embedding Conditioning\n(CEC) mechanism that leverages a contrastive encoder pretrained through\nself-supervised contrastive learning to project visual and tactile inputs into\nunified latent embeddings. These embeddings are used to couple visual-tactile\nfeature fusion through cross-modal attention, aiming at aligning the unified\nrepresentations and enhancing performance on downstream tasks. We conduct\nextensive experiments to demonstrate the superiority of ConViTac in real world\nover current state-of-the-art methods and the effectiveness of our proposed CEC\nmechanism, which improves accuracy by up to 12.0% in material classification\nand grasping prediction tasks.",
      "published": "2025-06-25T18:43:35Z",
      "authors": [
        "Zhiyuan Wu",
        "Yongqiang Zhao",
        "Shan Luo"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2506.20757v1",
      "relevance_score": 6
    },
    {
      "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM\n  Reasoning",
      "link": "https://arxiv.org/abs/2507.00833v1",
      "pdf_link": "https://arxiv.org/pdf/2507.00833v1.pdf",
      "summary": "For robotic manipulation, existing robotics datasets and simulation\nbenchmarks predominantly cater to robot-arm platforms. However, for humanoid\nrobots equipped with dual arms and dexterous hands, simulation tasks and\nhigh-quality demonstrations are notably lacking. Bimanual dexterous\nmanipulation is inherently more complex, as it requires coordinated arm\nmovements and hand operations, making autonomous data collection challenging.\nThis paper presents HumanoidGen, an automated task creation and demonstration\ncollection framework that leverages atomic dexterous operations and LLM\nreasoning to generate relational constraints. Specifically, we provide spatial\nannotations for both assets and dexterous hands based on the atomic operations,\nand perform an LLM planner to generate a chain of actionable spatial\nconstraints for arm movements based on object affordances and scenes. To\nfurther improve planning ability, we employ a variant of Monte Carlo tree\nsearch to enhance LLM reasoning for long-horizon tasks and insufficient\nannotation. In experiments, we create a novel benchmark with augmented\nscenarios to evaluate the quality of the collected data. The results show that\nthe performance of the 2D and 3D diffusion policies can scale with the\ngenerated dataset. Project page is https://openhumanoidgen.github.io.",
      "published": "2025-07-01T15:04:38Z",
      "authors": [
        "Zhi Jing",
        "Siyuan Yang",
        "Jicong Ao",
        "Ting Xiao",
        "Yugang Jiang",
        "Chenjia Bai"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.00833v1",
      "relevance_score": 6
    },
    {
      "title": "Few-shot transfer of tool-use skills using human demonstrations with\n  proximity and tactile sensing",
      "link": "https://arxiv.org/abs/2507.13200v1",
      "pdf_link": "https://arxiv.org/pdf/2507.13200v1.pdf",
      "summary": "Tools extend the manipulation abilities of robots, much like they do for\nhumans. Despite human expertise in tool manipulation, teaching robots these\nskills faces challenges. The complexity arises from the interplay of two\nsimultaneous points of contact: one between the robot and the tool, and another\nbetween the tool and the environment. Tactile and proximity sensors play a\ncrucial role in identifying these complex contacts. However, learning tool\nmanipulation using these sensors remains challenging due to limited real-world\ndata and the large sim-to-real gap. To address this, we propose a few-shot\ntool-use skill transfer framework using multimodal sensing. The framework\ninvolves pre-training the base policy to capture contact states common in\ntool-use skills in simulation and fine-tuning it with human demonstrations\ncollected in the real-world target domain to bridge the domain gap. We validate\nthat this framework enables teaching surface-following tasks using tools with\ndiverse physical and geometric properties with a small number of demonstrations\non the Franka Emika robot arm. Our analysis suggests that the robot acquires\nnew tool-use skills by transferring the ability to recognise tool-environment\ncontact relationships from pre-trained to fine-tuned policies. Additionally,\ncombining proximity and tactile sensors enhances the identification of contact\nstates and environmental geometry.",
      "published": "2025-07-17T15:10:12Z",
      "authors": [
        "Marina Y. Aoyama",
        "Sethu Vijayakumar",
        "Tetsuya Narita"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.13200v1",
      "relevance_score": 5
    },
    {
      "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
      "link": "https://arxiv.org/abs/2507.12768v1",
      "pdf_link": "https://arxiv.org/pdf/2507.12768v1.pdf",
      "summary": "Vision-language-action (VLA) models have shown promise on task-conditioned\ncontrol in complex settings such as bimanual manipulation. However, the heavy\nreliance on task-specific human demonstrations limits their generalization and\nincurs high data acquisition costs. In this work, we present a new notion of\ntask-agnostic action paradigm that decouples action execution from\ntask-specific conditioning, enhancing scalability, efficiency, and\ncost-effectiveness. To address the data collection challenges posed by this\nparadigm -- such as low coverage density, behavioral redundancy, and safety\nrisks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a\nscalable self-supervised framework that accelerates collection by over $\n30\\times $ compared to human teleoperation. To further enable effective\nlearning from task-agnostic data, which often suffers from distribution\nmismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics\nmodel equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder\n(DAD). We additionally integrate a video-conditioned action validation module\nto verify the feasibility of learned policies across diverse manipulation\ntasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%\nimprovement in test accuracy and achieves 30-40% higher success rates in\ndownstream tasks such as lifting, pick-and-place, and clicking, using\nreplay-based video validation. Project Page:\nhttps://embodiedfoundation.github.io/vidar_anypos",
      "published": "2025-07-17T03:48:57Z",
      "authors": [
        "Hengkai Tan",
        "Yao Feng",
        "Xinyi Mao",
        "Shuhe Huang",
        "Guodong Liu",
        "Zhongkai Hao",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12768v1",
      "relevance_score": 5
    },
    {
      "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware\n  Diffusers",
      "link": "https://arxiv.org/abs/2507.11287v1",
      "pdf_link": "https://arxiv.org/pdf/2507.11287v1.pdf",
      "summary": "In this paper, we study task-oriented human grasp synthesis, a new grasp\nsynthesis task that demands both task and context awareness. At the core of our\nmethod is the task-aware contact maps. Unlike traditional contact maps that\nonly reason about the manipulated object and its relation with the hand, our\nenhanced maps take into account scene and task information. This comprehensive\nmap is critical for hand-object interaction, enabling accurate grasping poses\nthat align with the task. We propose a two-stage pipeline that first constructs\na task-aware contact map informed by the scene and task. In the subsequent\nstage, we use this contact map to synthesize task-oriented human grasps. We\nintroduce a new dataset and a metric for the proposed task to evaluate our\napproach. Our experiments validate the importance of modeling both scene and\ntask, demonstrating significant improvements over existing methods in both\ngrasp quality and task performance. See our project page for more details:\nhttps://hcis-lab.github.io/TOHGS/",
      "published": "2025-07-15T13:11:55Z",
      "authors": [
        "An-Lun Liu",
        "Yu-Wei Chao",
        "Yi-Ting Chen"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.11287v1",
      "relevance_score": 5
    },
    {
      "title": "Object-Centric Mobile Manipulation through SAM2-Guided Perception and\n  Imitation Learning",
      "link": "https://arxiv.org/abs/2507.10899v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10899v1.pdf",
      "summary": "Imitation learning for mobile manipulation is a key challenge in the field of\nrobotic manipulation. However, current mobile manipulation frameworks typically\ndecouple navigation and manipulation, executing manipulation only after\nreaching a certain location. This can lead to performance degradation when\nnavigation is imprecise, especially due to misalignment in approach angles. To\nenable a mobile manipulator to perform the same task from diverse orientations,\nan essential capability for building general-purpose robotic models, we propose\nan object-centric method based on SAM2, a foundation model towards solving\npromptable visual segmentation in images, which incorporates manipulation\norientation information into our model. Our approach enables consistent\nunderstanding of the same task from different orientations. We deploy the model\non a custom-built mobile manipulator and evaluate it on a pick-and-place task\nunder varied orientation angles. Compared to Action Chunking Transformer, our\nmodel maintains superior generalization when trained with demonstrations from\nvaried approach angles. This work significantly enhances the generalization and\nrobustness of imitation learning-based mobile manipulation systems.",
      "published": "2025-07-15T01:26:59Z",
      "authors": [
        "Wang Zhicheng",
        "Satoshi Yagi",
        "Satoshi Yamamori",
        "Jun Morimoto"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10899v1",
      "relevance_score": 5
    },
    {
      "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid\n  Manipulation",
      "link": "https://arxiv.org/abs/2506.22827v3",
      "pdf_link": "https://arxiv.org/pdf/2506.22827v3.pdf",
      "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 73% success rate in completing the full manipulation sequence. These\nexperiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.",
      "published": "2025-06-28T09:39:37Z",
      "authors": [
        "André Schakkal",
        "Ben Zandonati",
        "Zhutian Yang",
        "Navid Azizan"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2506.22827v3",
      "relevance_score": 5
    },
    {
      "title": "EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for\n  Spatially Generalizable Contact-rich Tasks",
      "link": "https://arxiv.org/abs/2507.10961v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10961v1.pdf",
      "summary": "This paper presents a framework for learning vision-based robotic policies\nfor contact-rich manipulation tasks that generalize spatially across task\nconfigurations. We focus on achieving robust spatial generalization of the\npolicy for the peg-in-hole (PiH) task trained from a small number of\ndemonstrations. We propose EquiContact, a hierarchical policy composed of a\nhigh-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)\nand a novel low-level compliant visuomotor policy (Geometric Compliant ACT,\nG-CompACT). G-CompACT operates using only localized observations (geometrically\nconsistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB\nimages) and produces actions defined in the end-effector frame. Through these\ndesign choices, we show that the entire EquiContact pipeline is\nSE(3)-equivariant, from perception to force control. We also outline three key\ncomponents for spatially generalizable contact-rich policies: compliance,\nlocalized policies, and induced equivariance. Real-world experiments on PiH\ntasks demonstrate a near-perfect success rate and robust generalization to\nunseen spatial configurations, validating the proposed framework and\nprinciples. The experimental videos can be found on the project website:\nhttps://sites.google.com/berkeley.edu/equicontact",
      "published": "2025-07-15T03:45:26Z",
      "authors": [
        "Joohwan Seo",
        "Arvind Kruthiventy",
        "Soomi Lee",
        "Megan Teng",
        "Xiang Zhang",
        "Seoyeon Choi",
        "Jongeun Choi",
        "Roberto Horowitz"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10961v1",
      "relevance_score": 5
    },
    {
      "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge\n  for Tactile Generalization",
      "link": "https://arxiv.org/abs/2507.09160v1",
      "pdf_link": "https://arxiv.org/pdf/2507.09160v1.pdf",
      "summary": "Vision-Language-Action (VLA) models have shown remarkable achievements,\ndriven by the rich implicit knowledge of their vision-language components.\nHowever, achieving generalist robotic agents demands precise grounding into\nphysical interactions, especially in contact-rich scenarios where fine-grained\nforce control is essential. We advance VLAs' implicit knowledge beyond\nidentifying what to do, towards guiding how to physically interact with real\nworld. This paper introduces Tactile-VLA, a novel framework that deeply fuses\nvision, language, action, and tactile sensing. This framework incorporates a\nhybrid position-force controller to translate the model's intentions into\nprecise physical actions and a reasoning module that allows the robot to adapt\nits strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's\neffectiveness and generalizability in three key aspects: (1) enabling\ntactile-aware instruction following, (2) utilizing tactile-relevant\ncommonsense, and (3) facilitating adaptive tactile-involved reasoning. A key\nfinding is that the VLM's prior knowledge already contains semantic\nunderstanding of physical interaction; by connecting it to the robot's tactile\nsensors with only a few demonstrations, we can activate this prior knowledge to\nachieve zero-shot generalization in contact-rich tasks.",
      "published": "2025-07-12T06:44:37Z",
      "authors": [
        "Jialei Huang",
        "Shuo Wang",
        "Fanqi Lin",
        "Yihang Hu",
        "Chuan Wen",
        "Yang Gao"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.09160v1",
      "relevance_score": 5
    },
    {
      "title": "DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View\n  Images for Robotic Manipulation",
      "link": "https://arxiv.org/abs/2507.05627v1",
      "pdf_link": "https://arxiv.org/pdf/2507.05627v1.pdf",
      "summary": "Partial-view 3D recognition -- reconstructing 3D geometry and identifying\nobject instances from a few sparse RGB images -- is an exceptionally\nchallenging yet practically essential task, particularly in cluttered, occluded\nreal-world settings where full-view or reliable depth data are often\nunavailable. Existing methods, whether based on strong symmetry priors or\nsupervised learning on curated datasets, fail to generalize to such scenarios.\nIn this work, we introduce DreamGrasp, a framework that leverages the\nimagination capability of large-scale pre-trained image generative models to\ninfer the unobserved parts of a scene. By combining coarse 3D reconstruction,\ninstance segmentation via contrastive learning, and text-guided instance-wise\nrefinement, DreamGrasp circumvents limitations of prior methods and enables\nrobust 3D reconstruction in complex, multi-object environments. Our experiments\nshow that DreamGrasp not only recovers accurate object geometry but also\nsupports downstream tasks like sequential decluttering and target retrieval\nwith high success rates.",
      "published": "2025-07-08T03:12:49Z",
      "authors": [
        "Young Hun Kim",
        "Seungyeon Kim",
        "Yonghyeon Lee",
        "Frank Chongwoo Park"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.05627v1",
      "relevance_score": 5
    },
    {
      "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable\n  Evaluation",
      "link": "https://arxiv.org/abs/2507.00435v1",
      "pdf_link": "https://arxiv.org/pdf/2507.00435v1.pdf",
      "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone.",
      "published": "2025-07-01T05:33:16Z",
      "authors": [
        "Yi Ru Wang",
        "Carter Ung",
        "Grant Tannert",
        "Jiafei Duan",
        "Josephine Li",
        "Amy Le",
        "Rishabh Oswal",
        "Markus Grotz",
        "Wilbert Pumacay",
        "Yuquan Deng",
        "Ranjay Krishna",
        "Dieter Fox",
        "Siddhartha Srinivasa"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.00435v1",
      "relevance_score": 5
    },
    {
      "title": "EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled\n  Videos via Embodiment-Centric Flow",
      "link": "https://arxiv.org/abs/2507.06224v1",
      "pdf_link": "https://arxiv.org/pdf/2507.06224v1.pdf",
      "summary": "Current language-guided robotic manipulation systems often require low-level\naction-labeled datasets for imitation learning. While object-centric flow\nprediction methods mitigate this issue, they remain limited to scenarios\ninvolving rigid objects with clear displacement and minimal occlusion. In this\nwork, we present Embodiment-Centric Flow (EC-Flow), a framework that directly\nlearns manipulation from action-unlabeled videos by predicting\nembodiment-centric flow. Our key insight is that incorporating the embodiment's\ninherent kinematics significantly enhances generalization to versatile\nmanipulation scenarios, including deformable object handling, occlusions, and\nnon-object-displacement tasks. To connect the EC-Flow with language\ninstructions and object interactions, we further introduce a goal-alignment\nmodule by jointly optimizing movement consistency and goal-image prediction.\nMoreover, translating EC-Flow to executable robot actions only requires a\nstandard robot URDF (Unified Robot Description Format) file to specify\nkinematic constraints across joints, which makes it easy to use in practice. We\nvalidate EC-Flow on both simulation (Meta-World) and real-world tasks,\ndemonstrating its state-of-the-art performance in occluded object handling (62%\nimprovement), deformable object manipulation (45% improvement), and\nnon-object-displacement tasks (80% improvement) than prior state-of-the-art\nobject-centric flow methods. For more information, see our project website at\nhttps://ec-flow1.github.io .",
      "published": "2025-07-08T17:57:03Z",
      "authors": [
        "Yixiang Chen",
        "Peiyan Li",
        "Yan Huang",
        "Jiabing Yang",
        "Kehan Chen",
        "Liang Wang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.06224v1",
      "relevance_score": 5
    },
    {
      "title": "Generalist Bimanual Manipulation via Foundation Video Diffusion Models",
      "link": "https://arxiv.org/abs/2507.12898v1",
      "pdf_link": "https://arxiv.org/pdf/2507.12898v1.pdf",
      "summary": "Bimanual robotic manipulation, which involves the coordinated control of two\nrobotic arms, is foundational for solving challenging tasks. Despite recent\nprogress in general-purpose manipulation, data scarcity and embodiment\nheterogeneity remain serious obstacles to further scaling up in bimanual\nsettings. In this paper, we introduce VIdeo Diffusion for Action Reasoning\n(VIDAR), a two-stage framework that leverages large-scale, diffusion-based\nvideo pre-training and a novel masked inverse dynamics model for action\nprediction. We pre-train the video diffusion model on 750K multi-view videos\nfrom three real-world bimanual robot platforms, utilizing a unified observation\nspace that encodes robot, camera, task, and scene contexts. Our masked inverse\ndynamics model learns masks to extract action-relevant information from\ngenerated trajectories without requiring pixel-level labels, and the masks can\neffectively generalize to unseen backgrounds. Our experiments demonstrate that\nwith only 20 minutes of human demonstrations on an unseen robot platform (only\n1% of typical data requirements), VIDAR generalizes to unseen tasks and\nbackgrounds with strong semantic understanding, surpassing state-of-the-art\nmethods. Our findings highlight the potential of video foundation models,\ncoupled with masked action prediction, to enable scalable and generalizable\nrobotic manipulation in diverse real-world settings.",
      "published": "2025-07-17T08:31:55Z",
      "authors": [
        "Yao Feng",
        "Hengkai Tan",
        "Xinyi Mao",
        "Guodong Liu",
        "Shuhe Huang",
        "Chendong Xiang",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12898v1",
      "relevance_score": 4
    },
    {
      "title": "Learning human-to-robot handovers through 3D scene reconstruction",
      "link": "https://arxiv.org/abs/2507.08726v1",
      "pdf_link": "https://arxiv.org/pdf/2507.08726v1.pdf",
      "summary": "Learning robot manipulation policies from raw, real-world image data requires\na large number of robot-action trials in the physical environment. Although\ntraining using simulations offers a cost-effective alternative, the visual\ndomain gap between simulation and robot workspace remains a major limitation.\nGaussian Splatting visual reconstruction methods have recently provided new\ndirections for robot manipulation by generating realistic environments. In this\npaper, we propose the first method for learning supervised-based robot\nhandovers solely from RGB images without the need of real-robot training or\nreal-robot data collection. The proposed policy learner, Human-to-Robot\nHandover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view\nGaussian Splatting reconstruction of human-to-robot handover scenes to generate\nrobot demonstrations containing image-action pairs captured with a camera\nmounted on the robot gripper. As a result, the simulated camera pose changes in\nthe reconstructed scene can be directly translated into gripper pose changes.\nWe train a robot policy on demonstrations collected with 16 household objects\nand {\\em directly} deploy this policy in the real environment. Experiments in\nboth Gaussian Splatting reconstructed scene and real-world human-to-robot\nhandover experiments demonstrate that H2RH-SGS serves as a new and effective\nrepresentation for the human-to-robot handover task.",
      "published": "2025-07-11T16:26:31Z",
      "authors": [
        "Yuekun Wu",
        "Yik Lung Pang",
        "Andrea Cavallaro",
        "Changjae Oh"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.08726v1",
      "relevance_score": 4
    },
    {
      "title": "UniTac: Whole-Robot Touch Sensing Without Tactile Sensors",
      "link": "https://arxiv.org/abs/2507.07980v1",
      "pdf_link": "https://arxiv.org/pdf/2507.07980v1.pdf",
      "summary": "Robots can better interact with humans and unstructured environments through\ntouch sensing. However, most commercial robots are not equipped with tactile\nskins, making it challenging to achieve even basic touch-sensing functions,\nsuch as contact localization. We present UniTac, a data-driven whole-body\ntouch-sensing approach that uses only proprioceptive joint sensors and does not\nrequire the installation of additional sensors. Our approach enables a robot\nequipped solely with joint sensors to localize contacts. Our goal is to\ndemocratize touch sensing and provide an off-the-shelf tool for HRI researchers\nto provide their robots with touch-sensing capabilities. We validate our\napproach on two platforms: the Franka robot arm and the Spot quadruped. On\nFranka, we can localize contact to within 8.0 centimeters, and on Spot, we can\nlocalize to within 7.2 centimeters at around 2,000 Hz on an RTX 3090 GPU\nwithout adding any additional sensors to the robot. Project website:\nhttps://ivl.cs.brown.edu/research/unitac.",
      "published": "2025-07-10T17:55:05Z",
      "authors": [
        "Wanjia Fu",
        "Hongyu Li",
        "Ivy X. He",
        "Stefanie Tellex",
        "Srinath Sridhar"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.07980v1",
      "relevance_score": 4
    },
    {
      "title": "Accurate Pose Estimation Using Contact Manifold Sampling for Safe\n  Peg-in-Hole Insertion of Complex Geometries",
      "link": "https://arxiv.org/abs/2507.03925v1",
      "pdf_link": "https://arxiv.org/pdf/2507.03925v1.pdf",
      "summary": "Robotic assembly of complex, non-convex geometries with tight clearances\nremains a challenging problem, demanding precise state estimation for\nsuccessful insertion. In this work, we propose a novel framework that relies\nsolely on contact states to estimate the full SE(3) pose of a peg relative to a\nhole. Our method constructs an online submanifold of contact states through\nprimitive motions with just 6 seconds of online execution, subsequently mapping\nit to an offline contact manifold for precise pose estimation. We demonstrate\nthat without such state estimation, robots risk jamming and excessive force\napplication, potentially causing damage. We evaluate our approach on five\nindustrially relevant, complex geometries with 0.1 to 1.0 mm clearances,\nachieving a 96.7% success rate - a 6x improvement over primitive-based\ninsertion without state estimation. Additionally, we analyze insertion forces,\nand overall insertion times, showing our method significantly reduces the\naverage wrench, enabling safer and more efficient assembly.",
      "published": "2025-07-05T07:18:07Z",
      "authors": [
        "Abhay Negi",
        "Omey M. Manyar",
        "Dhanush K. Penmetsa",
        "Satyandra K. Gupta"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.03925v1",
      "relevance_score": 4
    },
    {
      "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active\n  Instance-Level Object Understanding",
      "link": "https://arxiv.org/abs/2507.10776v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10776v1.pdf",
      "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.",
      "published": "2025-07-14T20:02:52Z",
      "authors": [
        "Howard H. Qian",
        "Yiting Chen",
        "Gaotian Wang",
        "Podshara Chanrungmaneekul",
        "Kaiyu Hang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10776v1",
      "relevance_score": 4
    },
    {
      "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An\n  Evaluation with Human-Inspired Constraints",
      "link": "https://arxiv.org/abs/2507.10131v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10131v1.pdf",
      "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.",
      "published": "2025-07-14T10:21:27Z",
      "authors": [
        "Cesar Alan Contreras",
        "Manolis Chiou",
        "Alireza Rastegarpanah",
        "Michal Szulik",
        "Rustam Stolkin"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10131v1",
      "relevance_score": 4
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}