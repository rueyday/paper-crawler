{
  "last_updated": "2025-08-04T08:33:47.174416",
  "total_papers": 25,
  "papers": [
    {
      "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation\n  Learning",
      "link": "https://arxiv.org/abs/2508.00491v1",
      "pdf_link": "https://arxiv.org/pdf/2508.00491v1.pdf",
      "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation",
      "published": "2025-08-01T10:09:38Z",
      "authors": [
        "Carlo Alessi",
        "Federico Vasile",
        "Federico Ceola",
        "Giulia Pasquale",
        "Nicol√≤ Boccardo",
        "Lorenzo Natale"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.00491v1",
      "relevance_score": 7
    },
    {
      "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark\n  towards General Grasping",
      "link": "https://arxiv.org/abs/2507.23734v1",
      "pdf_link": "https://arxiv.org/pdf/2507.23734v1.pdf",
      "summary": "General robotic grasping systems require accurate object affordance\nperception in diverse open-world scenarios following human instructions.\nHowever, current studies suffer from the problem of lacking reasoning-based\nlarge-scale affordance prediction data, leading to considerable concern about\nopen-world effectiveness. To address this limitation, we build a large-scale\ngrasping-oriented affordance segmentation benchmark with human-like\ninstructions, named RAGNet. It contains 273k images, 180 categories, and 26k\nreasoning instructions. The images cover diverse embodied data domains, such as\nwild, robot, ego-centric, and even simulation data. They are carefully\nannotated with an affordance map, while the difficulty of language instructions\nis largely increased by removing their category name and only providing\nfunctional descriptions. Furthermore, we propose a comprehensive\naffordance-based grasping framework, named AffordanceNet, which consists of a\nVLM pre-trained on our massive affordance data and a grasping network that\nconditions an affordance map to grasp the target. Extensive experiments on\naffordance segmentation benchmarks and real-robot manipulation tasks show that\nour model has a powerful open-world generalization ability. Our data and code\nis available at https://github.com/wudongming97/AffordanceNet.",
      "published": "2025-07-31T17:17:05Z",
      "authors": [
        "Dongming Wu",
        "Yanping Fu",
        "Saike Huang",
        "Yingfei Liu",
        "Fan Jia",
        "Nian Liu",
        "Feng Dai",
        "Tiancai Wang",
        "Rao Muhammad Anwer",
        "Fahad Shahbaz Khan",
        "Jianbing Shen"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.23734v1",
      "relevance_score": 7
    },
    {
      "title": "Fluidically Innervated Lattices Make Versatile and Durable Tactile\n  Sensors",
      "link": "https://arxiv.org/abs/2507.21225v1",
      "pdf_link": "https://arxiv.org/pdf/2507.21225v1.pdf",
      "summary": "Tactile sensing plays a fundamental role in enabling robots to navigate\ndynamic and unstructured environments, particularly in applications such as\ndelicate object manipulation, surface exploration, and human-robot interaction.\nIn this paper, we introduce a passive soft robotic fingertip with integrated\ntactile sensing, fabricated using a 3D-printed elastomer lattice with embedded\nair channels. This sensorization approach, termed fluidic innervation,\ntransforms the lattice into a tactile sensor by detecting pressure changes\nwithin sealed air channels, providing a simple yet robust solution to tactile\nsensing in robotics. Unlike conventional methods that rely on complex materials\nor designs, fluidic innervation offers a simple, scalable, single-material\nfabrication process. We characterize the sensors' response, develop a geometric\nmodel to estimate tip displacement, and train a neural network to accurately\npredict contact location and contact force. Additionally, we integrate the\nfingertip with an admittance controller to emulate spring-like behavior,\ndemonstrate its capability for environment exploration through tactile\nfeedback, and validate its durability under high impact and cyclic loading\nconditions. This tactile sensing technique offers advantages in terms of\nsimplicity, adaptability, and durability and opens up new opportunities for\nversatile robotic manipulation.",
      "published": "2025-07-28T18:00:04Z",
      "authors": [
        "Annan Zhang",
        "Miguel Flores-Acton",
        "Andy Yu",
        "Anshul Gupta",
        "Maggie Yao",
        "Daniela Rus"
      ],
      "categories": [
        "cs.RO",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.21225v1",
      "relevance_score": 7
    },
    {
      "title": "RAKOMO: Reachability-Aware K-Order Markov Path Optimization for\n  Quadrupedal Loco-Manipulation",
      "link": "https://arxiv.org/abs/2507.19652v1",
      "pdf_link": "https://arxiv.org/pdf/2507.19652v1.pdf",
      "summary": "Legged manipulators, such as quadrupeds equipped with robotic arms, require\nmotion planning techniques that account for their complex kinematic constraints\nin order to perform manipulation tasks both safely and effectively. However,\ntrajectory optimization methods often face challenges due to the hybrid\ndynamics introduced by contact discontinuities, and tend to neglect leg\nlimitations during planning for computational reasons. In this work, we propose\nRAKOMO, a path optimization technique that integrates the strengths of K-Order\nMarkov Optimization (KOMO) with a kinematically-aware criterion based on the\nreachable region defined as reachability margin. We leverage a neural-network\nto predict the margin and optimize it by incorporating it in the standard KOMO\nformulation. This approach enables rapid convergence of gradient-based motion\nplanning -- commonly tailored for continuous systems -- while adapting it\neffectively to legged manipulators, successfully executing loco-manipulation\ntasks. We benchmark RAKOMO against a baseline KOMO approach through a set of\nsimulations for pick-and-place tasks with the HyQReal quadruped robot equipped\nwith a Kinova Gen3 robotic arm.",
      "published": "2025-07-25T19:55:35Z",
      "authors": [
        "Mattia Risiglione",
        "Abdelrahman Abdalla",
        "Victor Barasuol",
        "Kim Tien Ly",
        "Ioannis Havoutis",
        "Claudio Semini"
      ],
      "categories": [
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.19652v1",
      "relevance_score": 7
    },
    {
      "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable\n  Visuo-Tactile Gripper",
      "link": "https://arxiv.org/abs/2507.15062v1",
      "pdf_link": "https://arxiv.org/pdf/2507.15062v1.pdf",
      "summary": "Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .",
      "published": "2025-07-20T17:53:59Z",
      "authors": [
        "Xinyue Zhu",
        "Binghao Huang",
        "Yunzhu Li"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.15062v1",
      "relevance_score": 7
    },
    {
      "title": "A Segmented Robot Grasping Perception Neural Network for Edge AI",
      "link": "https://arxiv.org/abs/2507.13970v2",
      "pdf_link": "https://arxiv.org/pdf/2507.13970v2.pdf",
      "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "published": "2025-07-18T14:32:45Z",
      "authors": [
        "Casper Br√∂cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M√∂ckel"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.13970v2",
      "relevance_score": 7
    },
    {
      "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation\n  Model Reasoning and Part Grounding",
      "link": "https://arxiv.org/abs/2507.18276v1",
      "pdf_link": "https://arxiv.org/pdf/2507.18276v1.pdf",
      "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.",
      "published": "2025-07-24T10:25:58Z",
      "authors": [
        "Xiaojie Zhang",
        "Yuanfei Wang",
        "Ruihai Wu",
        "Kunqi Xu",
        "Yu Li",
        "Liuyu Xiang",
        "Hao Dong",
        "Zhaofeng He"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.18276v1",
      "relevance_score": 6
    },
    {
      "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model",
      "link": "https://arxiv.org/abs/2507.09985v1",
      "pdf_link": "https://arxiv.org/pdf/2507.09985v1.pdf",
      "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.",
      "published": "2025-07-14T07:05:36Z",
      "authors": [
        "Samson Yu",
        "Kelvin Lin",
        "Harold Soh"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.09985v1",
      "relevance_score": 6
    },
    {
      "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
      "link": "https://arxiv.org/abs/2507.12768v1",
      "pdf_link": "https://arxiv.org/pdf/2507.12768v1.pdf",
      "summary": "Vision-language-action (VLA) models have shown promise on task-conditioned\ncontrol in complex settings such as bimanual manipulation. However, the heavy\nreliance on task-specific human demonstrations limits their generalization and\nincurs high data acquisition costs. In this work, we present a new notion of\ntask-agnostic action paradigm that decouples action execution from\ntask-specific conditioning, enhancing scalability, efficiency, and\ncost-effectiveness. To address the data collection challenges posed by this\nparadigm -- such as low coverage density, behavioral redundancy, and safety\nrisks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a\nscalable self-supervised framework that accelerates collection by over $\n30\\times $ compared to human teleoperation. To further enable effective\nlearning from task-agnostic data, which often suffers from distribution\nmismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics\nmodel equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder\n(DAD). We additionally integrate a video-conditioned action validation module\nto verify the feasibility of learned policies across diverse manipulation\ntasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%\nimprovement in test accuracy and achieves 30-40% higher success rates in\ndownstream tasks such as lifting, pick-and-place, and clicking, using\nreplay-based video validation. Project Page:\nhttps://embodiedfoundation.github.io/vidar_anypos",
      "published": "2025-07-17T03:48:57Z",
      "authors": [
        "Hengkai Tan",
        "Yao Feng",
        "Xinyi Mao",
        "Shuhe Huang",
        "Guodong Liu",
        "Zhongkai Hao",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12768v1",
      "relevance_score": 5
    },
    {
      "title": "Object-Centric Mobile Manipulation through SAM2-Guided Perception and\n  Imitation Learning",
      "link": "https://arxiv.org/abs/2507.10899v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10899v1.pdf",
      "summary": "Imitation learning for mobile manipulation is a key challenge in the field of\nrobotic manipulation. However, current mobile manipulation frameworks typically\ndecouple navigation and manipulation, executing manipulation only after\nreaching a certain location. This can lead to performance degradation when\nnavigation is imprecise, especially due to misalignment in approach angles. To\nenable a mobile manipulator to perform the same task from diverse orientations,\nan essential capability for building general-purpose robotic models, we propose\nan object-centric method based on SAM2, a foundation model towards solving\npromptable visual segmentation in images, which incorporates manipulation\norientation information into our model. Our approach enables consistent\nunderstanding of the same task from different orientations. We deploy the model\non a custom-built mobile manipulator and evaluate it on a pick-and-place task\nunder varied orientation angles. Compared to Action Chunking Transformer, our\nmodel maintains superior generalization when trained with demonstrations from\nvaried approach angles. This work significantly enhances the generalization and\nrobustness of imitation learning-based mobile manipulation systems.",
      "published": "2025-07-15T01:26:59Z",
      "authors": [
        "Wang Zhicheng",
        "Satoshi Yagi",
        "Satoshi Yamamori",
        "Jun Morimoto"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10899v1",
      "relevance_score": 5
    },
    {
      "title": "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level\n  Tactile Feedback",
      "link": "https://arxiv.org/abs/2507.17294v2",
      "pdf_link": "https://arxiv.org/pdf/2507.17294v2.pdf",
      "summary": "Tactile feedback is generally recognized to be crucial for effective\ninteraction with the physical world. However, state-of-the-art\nVision-Language-Action (VLA) models lack the ability to interpret and use\ntactile signals, limiting their effectiveness in contact-rich tasks.\nIncorporating tactile feedback into these systems is challenging due to the\nabsence of large multi-modal datasets. We present VLA-Touch, an approach that\nenhances generalist robot policies with tactile sensing \\emph{without\nfine-tuning} the base VLA. Our method introduces two key innovations: (1) a\npipeline that leverages a pretrained tactile-language model that provides\nsemantic tactile feedback for high-level task planning, and (2) a\ndiffusion-based controller that refines VLA-generated actions with tactile\nsignals for contact-rich manipulation. Through real-world experiments, we\ndemonstrate that our dual-level integration of tactile feedback improves task\nplanning efficiency while enhancing execution precision. Code is open-sourced\nat \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.",
      "published": "2025-07-23T07:54:10Z",
      "authors": [
        "Jianxin Bi",
        "Kevin Yuchen Ma",
        "Ce Hao",
        "Mike Zheng Shou",
        "Harold Soh"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.17294v2",
      "relevance_score": 5
    },
    {
      "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware\n  Diffusers",
      "link": "https://arxiv.org/abs/2507.11287v1",
      "pdf_link": "https://arxiv.org/pdf/2507.11287v1.pdf",
      "summary": "In this paper, we study task-oriented human grasp synthesis, a new grasp\nsynthesis task that demands both task and context awareness. At the core of our\nmethod is the task-aware contact maps. Unlike traditional contact maps that\nonly reason about the manipulated object and its relation with the hand, our\nenhanced maps take into account scene and task information. This comprehensive\nmap is critical for hand-object interaction, enabling accurate grasping poses\nthat align with the task. We propose a two-stage pipeline that first constructs\na task-aware contact map informed by the scene and task. In the subsequent\nstage, we use this contact map to synthesize task-oriented human grasps. We\nintroduce a new dataset and a metric for the proposed task to evaluate our\napproach. Our experiments validate the importance of modeling both scene and\ntask, demonstrating significant improvements over existing methods in both\ngrasp quality and task performance. See our project page for more details:\nhttps://hcis-lab.github.io/TOHGS/",
      "published": "2025-07-15T13:11:55Z",
      "authors": [
        "An-Lun Liu",
        "Yu-Wei Chao",
        "Yi-Ting Chen"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.11287v1",
      "relevance_score": 5
    },
    {
      "title": "DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View\n  Images for Robotic Manipulation",
      "link": "https://arxiv.org/abs/2507.05627v1",
      "pdf_link": "https://arxiv.org/pdf/2507.05627v1.pdf",
      "summary": "Partial-view 3D recognition -- reconstructing 3D geometry and identifying\nobject instances from a few sparse RGB images -- is an exceptionally\nchallenging yet practically essential task, particularly in cluttered, occluded\nreal-world settings where full-view or reliable depth data are often\nunavailable. Existing methods, whether based on strong symmetry priors or\nsupervised learning on curated datasets, fail to generalize to such scenarios.\nIn this work, we introduce DreamGrasp, a framework that leverages the\nimagination capability of large-scale pre-trained image generative models to\ninfer the unobserved parts of a scene. By combining coarse 3D reconstruction,\ninstance segmentation via contrastive learning, and text-guided instance-wise\nrefinement, DreamGrasp circumvents limitations of prior methods and enables\nrobust 3D reconstruction in complex, multi-object environments. Our experiments\nshow that DreamGrasp not only recovers accurate object geometry but also\nsupports downstream tasks like sequential decluttering and target retrieval\nwith high success rates.",
      "published": "2025-07-08T03:12:49Z",
      "authors": [
        "Young Hun Kim",
        "Seungyeon Kim",
        "Yonghyeon Lee",
        "Frank Chongwoo Park"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.05627v1",
      "relevance_score": 5
    },
    {
      "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for\n  Dexterous Teleoperation",
      "link": "https://arxiv.org/abs/2507.23592v1",
      "pdf_link": "https://arxiv.org/pdf/2507.23592v1.pdf",
      "summary": "Hand exoskeletons are critical tools for dexterous teleoperation and\nimmersive manipulation interfaces, but achieving accurate hand tracking remains\na challenge due to user-specific anatomical variability and donning\ninconsistencies. These issues lead to kinematic misalignments that degrade\ntracking performance and limit applicability in precision tasks. We propose a\nsubject-specific calibration framework for exoskeleton-based hand tracking that\nuses redundant joint sensing and a residual-weighted optimization strategy to\nestimate virtual link parameters. Implemented on the Maestro exoskeleton, our\nmethod improves joint angle and fingertip position estimation across users with\nvarying hand geometries. We introduce a data-driven approach to empirically\ntune cost function weights using motion capture ground truth, enabling more\naccurate and consistent calibration across participants. Quantitative results\nfrom seven subjects show substantial reductions in joint and fingertip tracking\nerrors compared to uncalibrated and evenly weighted models. Qualitative\nvisualizations using a Unity-based virtual hand further confirm improvements in\nmotion fidelity. The proposed framework generalizes across exoskeleton designs\nwith closed-loop kinematics and minimal sensing, and lays the foundation for\nhigh-fidelity teleoperation and learning-from-demonstration applications.",
      "published": "2025-07-31T14:29:38Z",
      "authors": [
        "Haiyun Zhang",
        "Stefano Dalla Gasperina",
        "Saad N. Yousaf",
        "Toshimitsu Tsuboi",
        "Tetsuya Narita",
        "Ashish D. Deshpande"
      ],
      "categories": [
        "cs.RO",
        "cs.HC",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.23592v1",
      "relevance_score": 4
    },
    {
      "title": "Quadratic Programming-Based Posture Manipulation and Thrust-vectoring\n  for Agile Dynamic Walking on Narrow Pathways",
      "link": "https://arxiv.org/abs/2507.23203v1",
      "pdf_link": "https://arxiv.org/pdf/2507.23203v1.pdf",
      "summary": "There has been significant advancement in legged robot's agility where they\ncan show impressive acrobatic maneuvers, such as parkour. These maneuvers rely\nheavily on posture manipulation. To expand the stability and locomotion\nplasticity, we use the multi-modal ability in our legged-aerial platform, the\nHusky Beta, to perform thruster-assisted walking. This robot has thrusters on\neach of its sagittal knee joints which can be used to stabilize its frontal\ndynamic as it walks. In this work, we perform a simulation study of quadruped\nnarrow-path walking with Husky $\\beta$, where the robot will utilize its\nthrusters to stably walk on a narrow path. The controller is designed based on\na centroidal dynamics model with thruster and foot ground contact forces as\ninputs. These inputs are regulated using a QP solver to be used in a model\npredictive control framework. In addition to narrow-path walking, we also\nperform a lateral push-recovery simulation to study how the thrusters can be\nused to stabilize the frontal dynamics.",
      "published": "2025-07-31T02:57:26Z",
      "authors": [
        "Chenghao Wang",
        "Eric Sihite",
        "Kaushik Venkatesh Krishnamurthy",
        "Shreyansh Pitroda",
        "Adarsh Salagame",
        "Alireza Ramezani",
        "Morteza Gharib"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.23203v1",
      "relevance_score": 4
    },
    {
      "title": "Whisker-based Active Tactile Perception for Contour Reconstruction",
      "link": "https://arxiv.org/abs/2507.23305v1",
      "pdf_link": "https://arxiv.org/pdf/2507.23305v1.pdf",
      "summary": "Perception using whisker-inspired tactile sensors currently faces a major\nchallenge: the lack of active control in robots based on direct contact\ninformation from the whisker. To accurately reconstruct object contours, it is\ncrucial for the whisker sensor to continuously follow and maintain an\nappropriate relative touch pose on the surface. This is especially important\nfor localization based on tip contact, which has a low tolerance for sharp\nsurfaces and must avoid slipping into tangential contact. In this paper, we\nfirst construct a magnetically transduced whisker sensor featuring a compact\nand robust suspension system composed of three flexible spiral arms. We develop\na method that leverages a characterized whisker deflection profile to directly\nextract the tip contact position using gradient descent, with a Bayesian filter\napplied to reduce fluctuations. We then propose an active motion control policy\nto maintain the optimal relative pose of the whisker sensor against the object\nsurface. A B-Spline curve is employed to predict the local surface curvature\nand determine the sensor orientation. Results demonstrate that our algorithm\ncan effectively track objects and reconstruct contours with sub-millimeter\naccuracy. Finally, we validate the method in simulations and real-world\nexperiments where a robot arm drives the whisker sensor to follow the surfaces\nof three different objects.",
      "published": "2025-07-31T07:38:42Z",
      "authors": [
        "Yixuan Dang",
        "Qinyang Xu",
        "Yu Zhang",
        "Xiangtong Yao",
        "Liding Zhang",
        "Zhenshan Bing",
        "Florian Roehrbein",
        "Alois Knoll"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.23305v1",
      "relevance_score": 4
    },
    {
      "title": "Skin-Machine Interface with Multimodal Contact Motion Classifier",
      "link": "https://arxiv.org/abs/2507.19760v1",
      "pdf_link": "https://arxiv.org/pdf/2507.19760v1.pdf",
      "summary": "This paper proposes a novel framework for utilizing skin sensors as a new\noperation interface of complex robots. The skin sensors employed in this study\npossess the capability to quantify multimodal tactile information at multiple\ncontact points. The time-series data generated from these sensors is\nanticipated to facilitate the classification of diverse contact motions\nexhibited by an operator. By mapping the classification results with robot\nmotion primitives, a diverse range of robot motions can be generated by\naltering the manner in which the skin sensors are interacted with. In this\npaper, we focus on a learning-based contact motion classifier employing\nrecurrent neural networks. This classifier is a pivotal factor in the success\nof this framework. Furthermore, we elucidate the requisite conditions for\nsoftware-hardware designs. Firstly, multimodal sensing and its comprehensive\nencoding significantly contribute to the enhancement of classification accuracy\nand learning stability. Utilizing all modalities simultaneously as inputs to\nthe classifier proves to be an effective approach. Secondly, it is essential to\nmount the skin sensors on a flexible and compliant support to enable the\nactivation of three-axis accelerometers. These accelerometers are capable of\nmeasuring horizontal tactile information, thereby enhancing the correlation\nwith other modalities. Furthermore, they serve to absorb the noises generated\nby the robot's movements during deployment. Through these discoveries, the\naccuracy of the developed classifier surpassed 95 %, enabling the dual-arm\nmobile manipulator to execute a diverse range of tasks via the Skin-Machine\nInterface. https://youtu.be/UjUXT4Z4BC8",
      "published": "2025-07-26T03:26:20Z",
      "authors": [
        "Alberto Confente",
        "Takanori Jin",
        "Taisuke Kobayashi",
        "Julio Rogelio Guadarrama-Olvera",
        "Gordon Cheng"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.19760v1",
      "relevance_score": 4
    },
    {
      "title": "Deformable Cluster Manipulation via Whole-Arm Policy Learning",
      "link": "https://arxiv.org/abs/2507.17085v1",
      "pdf_link": "https://arxiv.org/pdf/2507.17085v1.pdf",
      "summary": "Manipulating clusters of deformable objects presents a substantial challenge\nwith widespread applicability, but requires contact-rich whole-arm\ninteractions. A potential solution must address the limited capacity for\nrealistic model synthesis, high uncertainty in perception, and the lack of\nefficient spatial abstractions, among others. We propose a novel framework for\nlearning model-free policies integrating two modalities: 3D point clouds and\nproprioceptive touch indicators, emphasising manipulation with full body\ncontact awareness, going beyond traditional end-effector modes. Our\nreinforcement learning framework leverages a distributional state\nrepresentation, aided by kernel mean embeddings, to achieve improved training\nefficiency and real-time inference. Furthermore, we propose a novel\ncontext-agnostic occlusion heuristic to clear deformables from a target region\nfor exposure tasks. We deploy the framework in a power line clearance scenario\nand observe that the agent generates creative strategies leveraging multiple\narm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy\ntransfer, allowing the arm to clear real branches with unknown occlusion\npatterns, unseen topology, and uncertain dynamics.",
      "published": "2025-07-22T23:58:30Z",
      "authors": [
        "Jayadeep Jacob",
        "Wenzheng Zhang",
        "Houston Warren",
        "Paulo Borges",
        "Tirthankar Bandyopadhyay",
        "Fabio Ramos"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.17085v1",
      "relevance_score": 4
    },
    {
      "title": "SuperMag: Vision-based Tactile Data Guided High-resolution Tactile Shape\n  Reconstruction for Magnetic Tactile Sensors",
      "link": "https://arxiv.org/abs/2507.20002v1",
      "pdf_link": "https://arxiv.org/pdf/2507.20002v1.pdf",
      "summary": "Magnetic-based tactile sensors (MBTS) combine the advantages of compact\ndesign and high-frequency operation but suffer from limited spatial resolution\ndue to their sparse taxel arrays. This paper proposes SuperMag, a tactile shape\nreconstruction method that addresses this limitation by leveraging\nhigh-resolution vision-based tactile sensor (VBTS) data to supervise MBTS\nsuper-resolution. Co-designed, open-source VBTS and MBTS with identical contact\nmodules enable synchronized data collection of high-resolution shapes and\nmagnetic signals via a symmetric calibration setup. We frame tactile shape\nreconstruction as a conditional generative problem, employing a conditional\nvariational auto-encoder to infer high-resolution shapes from low-resolution\nMBTS inputs. The MBTS achieves a sampling frequency of 125 Hz, whereas the\nshape reconstruction sustains an inference time within 2.5 ms. This\ncross-modality synergy advances tactile perception of the MBTS, potentially\nunlocking its new capabilities in high-precision robotic tasks.",
      "published": "2025-07-26T16:44:25Z",
      "authors": [
        "Peiyao Hou",
        "Danning Sun",
        "Meng Wang",
        "Yuzhe Huang",
        "Zeyu Zhang",
        "Hangxin Liu",
        "Wanlin Li",
        "Ziyuan Jiao"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.20002v1",
      "relevance_score": 4
    },
    {
      "title": "PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion\n  Policy",
      "link": "https://arxiv.org/abs/2507.17846v1",
      "pdf_link": "https://arxiv.org/pdf/2507.17846v1.pdf",
      "summary": "Pottery creation is a complicated art form that requires dexterous, precise\nand delicate actions to slowly morph a block of clay to a meaningful, and often\nuseful 3D goal shape. In this work, we aim to create a robotic system that can\ncreate simple pottery goals with only pinch-based actions. This pinch pottery\ntask allows us to explore the challenges of a highly multi-modal and\nlong-horizon deformable manipulation task. To this end, we present PinchBot, a\ngoal-conditioned diffusion policy model that when combined with pre-trained 3D\npoint cloud embeddings, task progress prediction and collision-constrained\naction projection, is able to successfully create a variety of simple pottery\ngoals. For experimental videos and access to the demonstration dataset, please\nvisit our project website:\nhttps://sites.google.com/andrew.cmu.edu/pinchbot/home.",
      "published": "2025-07-23T18:13:41Z",
      "authors": [
        "Alison Bartsch",
        "Arvind Car",
        "Amir Barati Farimani"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.17846v1",
      "relevance_score": 4
    },
    {
      "title": "Vidar: Embodied Video Diffusion Model for Generalist Bimanual\n  Manipulation",
      "link": "https://arxiv.org/abs/2507.12898v2",
      "pdf_link": "https://arxiv.org/pdf/2507.12898v2.pdf",
      "summary": "Bimanual robotic manipulation, which involves the coordinated control of two\nrobotic arms, is foundational for solving challenging tasks. Despite recent\nprogress in general-purpose manipulation, data scarcity and embodiment\nheterogeneity remain serious obstacles to further scaling up in bimanual\nsettings. In this paper, we introduce Video Diffusion for Action Reasoning\n(Vidar), a two-stage framework that leverages large-scale, diffusion-based\nvideo pre-training and a novel masked inverse dynamics model for action\nprediction. We pre-train the video diffusion model on 750K multi-view videos\nfrom three real-world bimanual robot platforms, utilizing a unified observation\nspace that encodes robot, camera, task, and scene contexts. Our masked inverse\ndynamics model learns masks to extract action-relevant information from\ngenerated trajectories without requiring pixel-level labels, and the masks can\neffectively generalize to unseen backgrounds. Our experiments demonstrate that\nwith only 20 minutes of human demonstrations on an unseen robot platform (only\n1% of typical data requirements), Vidar generalizes to unseen tasks and\nbackgrounds with strong semantic understanding, surpassing state-of-the-art\nmethods. Our findings highlight the potential of video foundation models,\ncoupled with masked action prediction, to enable scalable and generalizable\nrobotic manipulation in diverse real-world settings.",
      "published": "2025-07-17T08:31:55Z",
      "authors": [
        "Yao Feng",
        "Hengkai Tan",
        "Xinyi Mao",
        "Guodong Liu",
        "Shuhe Huang",
        "Chendong Xiang",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12898v2",
      "relevance_score": 4
    },
    {
      "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active\n  Instance-Level Object Understanding",
      "link": "https://arxiv.org/abs/2507.10776v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10776v1.pdf",
      "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.",
      "published": "2025-07-14T20:02:52Z",
      "authors": [
        "Howard H. Qian",
        "Yiting Chen",
        "Gaotian Wang",
        "Podshara Chanrungmaneekul",
        "Kaiyu Hang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10776v1",
      "relevance_score": 4
    },
    {
      "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An\n  Evaluation with Human-Inspired Constraints",
      "link": "https://arxiv.org/abs/2507.10131v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10131v1.pdf",
      "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.",
      "published": "2025-07-14T10:21:27Z",
      "authors": [
        "Cesar Alan Contreras",
        "Manolis Chiou",
        "Alireza Rastegarpanah",
        "Michal Szulik",
        "Rustam Stolkin"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10131v1",
      "relevance_score": 4
    },
    {
      "title": "Learning human-to-robot handovers through 3D scene reconstruction",
      "link": "https://arxiv.org/abs/2507.08726v1",
      "pdf_link": "https://arxiv.org/pdf/2507.08726v1.pdf",
      "summary": "Learning robot manipulation policies from raw, real-world image data requires\na large number of robot-action trials in the physical environment. Although\ntraining using simulations offers a cost-effective alternative, the visual\ndomain gap between simulation and robot workspace remains a major limitation.\nGaussian Splatting visual reconstruction methods have recently provided new\ndirections for robot manipulation by generating realistic environments. In this\npaper, we propose the first method for learning supervised-based robot\nhandovers solely from RGB images without the need of real-robot training or\nreal-robot data collection. The proposed policy learner, Human-to-Robot\nHandover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view\nGaussian Splatting reconstruction of human-to-robot handover scenes to generate\nrobot demonstrations containing image-action pairs captured with a camera\nmounted on the robot gripper. As a result, the simulated camera pose changes in\nthe reconstructed scene can be directly translated into gripper pose changes.\nWe train a robot policy on demonstrations collected with 16 household objects\nand {\\em directly} deploy this policy in the real environment. Experiments in\nboth Gaussian Splatting reconstructed scene and real-world human-to-robot\nhandover experiments demonstrate that H2RH-SGS serves as a new and effective\nrepresentation for the human-to-robot handover task.",
      "published": "2025-07-11T16:26:31Z",
      "authors": [
        "Yuekun Wu",
        "Yik Lung Pang",
        "Andrea Cavallaro",
        "Changjae Oh"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.08726v1",
      "relevance_score": 4
    },
    {
      "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation",
      "link": "https://arxiv.org/abs/2508.00697v1",
      "pdf_link": "https://arxiv.org/pdf/2508.00697v1.pdf",
      "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.",
      "published": "2025-08-01T15:14:39Z",
      "authors": [
        "Yiming Wu",
        "Huan Wang",
        "Zhenghao Chen",
        "Jianxin Pang",
        "Dong Xu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.00697v1",
      "relevance_score": 3
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}