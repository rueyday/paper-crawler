{
  "last_updated": "2025-07-28T08:33:49.487278",
  "total_papers": 25,
  "papers": [
    {
      "title": "Hierarchical Reinforcement Learning for Articulated Tool Manipulation\n  with Multifingered Hand",
      "link": "https://arxiv.org/abs/2507.06822v1",
      "pdf_link": "https://arxiv.org/pdf/2507.06822v1.pdf",
      "summary": "Manipulating articulated tools, such as tweezers or scissors, has rarely been\nexplored in previous research. Unlike rigid tools, articulated tools change\ntheir shape dynamically, creating unique challenges for dexterous robotic\nhands. In this work, we present a hierarchical, goal-conditioned reinforcement\nlearning (GCRL) framework to improve the manipulation capabilities of\nanthropomorphic robotic hands using articulated tools. Our framework comprises\ntwo policy layers: (1) a low-level policy that enables the dexterous hand to\nmanipulate the tool into various configurations for objects of different sizes,\nand (2) a high-level policy that defines the tool's goal state and controls the\nrobotic arm for object-picking tasks. We employ an encoder, trained on\nsynthetic pointclouds, to estimate the tool's affordance states--specifically,\nhow different tool configurations (e.g., tweezer opening angles) enable\ngrasping of objects of varying sizes--from input point clouds, thereby enabling\nprecise tool manipulation. We also utilize a privilege-informed heuristic\npolicy to generate replay buffer, improving the training efficiency of the\nhigh-level policy. We validate our approach through real-world experiments,\nshowing that the robot can effectively manipulate a tweezer-like tool to grasp\nobjects of diverse shapes and sizes with a 70.8 % success rate. This study\nhighlights the potential of RL to advance dexterous robotic manipulation of\narticulated tools.",
      "published": "2025-07-09T13:11:12Z",
      "authors": [
        "Wei Xu",
        "Yanchao Zhao",
        "Weichao Guo",
        "Xinjun Sheng"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.06822v1",
      "relevance_score": 8
    },
    {
      "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable\n  Visuo-Tactile Gripper",
      "link": "https://arxiv.org/abs/2507.15062v1",
      "pdf_link": "https://arxiv.org/pdf/2507.15062v1.pdf",
      "summary": "Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .",
      "published": "2025-07-20T17:53:59Z",
      "authors": [
        "Xinyue Zhu",
        "Binghao Huang",
        "Yunzhu Li"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.15062v1",
      "relevance_score": 7
    },
    {
      "title": "Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp\n  Constraining Walls",
      "link": "https://arxiv.org/abs/2507.14721v1",
      "pdf_link": "https://arxiv.org/pdf/2507.14721v1.pdf",
      "summary": "This study addresses the problem of occluded grasping, where primary grasp\nconfigurations of an object are not available due to occlusion with\nenvironment. Simple parallel grippers often struggle with such tasks due to\nlimited dexterity and actuation constraints. Prior works have explored object\npose reorientation such as pivoting by utilizing extrinsic contacts between an\nobject and an environment feature like a wall, to make the object graspable.\nHowever, such works often assume the presence of a short wall, and this\nassumption may not always hold in real-world scenarios. If the wall available\nfor interaction is too large or too tall, the robot may still fail to grasp the\nobject even after pivoting, and the robot must combine different types of\nactions to grasp. To address this, we propose a hierarchical reinforcement\nlearning (RL) framework. We use Q-learning to train a high-level policy that\nselects the type of action expected to yield the highest reward. The selected\nlow-level skill then samples a specific robot action in continuous space. To\nguide the robot to an appropriate location for executing the selected action,\nwe adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on\nthe object point cloud and the skill ID, enabling it to infer a suitable\nlocation based on the object geometry and the selected skill. To promote\ngeneralization, we apply domain randomization during the training of low-level\nskills. The RL policy is trained entirely in simulation with a box-like object\nand deployed to six objects in real world. We conduct experiments to evaluate\nour method and demonstrate both its generalizability and robust sim-to-real\ntransfer performance with promising success rates.",
      "published": "2025-07-19T18:49:47Z",
      "authors": [
        "Keita Kobashi",
        "Masayoshi Tomizuka"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.14721v1",
      "relevance_score": 7
    },
    {
      "title": "A segmented robot grasping perception neural network for edge AI",
      "link": "https://arxiv.org/abs/2507.13970v1",
      "pdf_link": "https://arxiv.org/pdf/2507.13970v1.pdf",
      "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "published": "2025-07-18T14:32:45Z",
      "authors": [
        "Casper Bröcheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico Möckel"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.13970v1",
      "relevance_score": 7
    },
    {
      "title": "Foundation Model-Driven Grasping of Unknown Objects via Center of\n  Gravity Estimation",
      "link": "https://arxiv.org/abs/2507.19242v1",
      "pdf_link": "https://arxiv.org/pdf/2507.19242v1.pdf",
      "summary": "This study presents a grasping method for objects with uneven mass\ndistribution by leveraging diffusion models to localize the center of gravity\n(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to\npostural instability, where existing keypoint-based or affordance-driven\nmethods exhibit limitations. We constructed a dataset of 790 images featuring\nunevenly distributed objects with keypoint annotations for CoG localization. A\nvision-driven framework based on foundation models was developed to achieve\nCoG-aware grasping. Experimental evaluations across real-world scenarios\ndemonstrate that our method achieves a 49\\% higher success rate compared to\nconventional keypoint-based approaches and an 11\\% improvement over\nstate-of-the-art affordance-driven methods. The system exhibits strong\ngeneralization with a 76\\% CoG localization accuracy on unseen objects,\nproviding a novel solution for precise and stable grasping tasks.",
      "published": "2025-07-25T13:09:11Z",
      "authors": [
        "Kang Xiangli",
        "Yage He",
        "Xianwu Gong",
        "Zehan Liu",
        "Yuru Bai"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.19242v1",
      "relevance_score": 6
    },
    {
      "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation\n  Model Reasoning and Part Grounding",
      "link": "https://arxiv.org/abs/2507.18276v1",
      "pdf_link": "https://arxiv.org/pdf/2507.18276v1.pdf",
      "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.",
      "published": "2025-07-24T10:25:58Z",
      "authors": [
        "Xiaojie Zhang",
        "Yuanfei Wang",
        "Ruihai Wu",
        "Kunqi Xu",
        "Yu Li",
        "Liuyu Xiang",
        "Hao Dong",
        "Zhaofeng He"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.18276v1",
      "relevance_score": 6
    },
    {
      "title": "Heterogeneous object manipulation on nonlinear soft surface through\n  linear controller",
      "link": "https://arxiv.org/abs/2507.14967v1",
      "pdf_link": "https://arxiv.org/pdf/2507.14967v1.pdf",
      "summary": "Manipulation surfaces indirectly control and reposition objects by actively\nmodifying their shape or properties rather than directly gripping objects.\nThese surfaces, equipped with dense actuator arrays, generate dynamic\ndeformations. However, a high-density actuator array introduces considerable\ncomplexity due to increased degrees of freedom (DOF), complicating control\ntasks. High DOF restrict the implementation and utilization of manipulation\nsurfaces in real-world applications as the maintenance and control of such\nsystems exponentially increase with array/surface size. Learning-based control\napproaches may ease the control complexity, but they require extensive training\nsamples and struggle to generalize for heterogeneous objects. In this study, we\nintroduce a simple, precise and robust PID-based linear close-loop feedback\ncontrol strategy for heterogeneous object manipulation on MANTA-RAY\n(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation\ndensity). Our approach employs a geometric transformation-driven PID\ncontroller, directly mapping tilt angle control outputs(1D/2D) to actuator\ncommands to eliminate the need for extensive black-box training. We validate\nthe proposed method through simulations and experiments on a physical system,\nsuccessfully manipulating objects with diverse geometries, weights and\ntextures, including fragile objects like eggs and apples. The outcomes\ndemonstrate that our approach is highly generalized and offers a practical and\nreliable solution for object manipulation on soft robotic manipulation,\nfacilitating real-world implementation without prohibitive training demands.",
      "published": "2025-07-20T13:53:35Z",
      "authors": [
        "Pratik Ingle",
        "Kasper Støy",
        "Andres Faiña"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.14967v1",
      "relevance_score": 6
    },
    {
      "title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with\n  On-Generator Training",
      "link": "https://arxiv.org/abs/2507.13097v1",
      "pdf_link": "https://arxiv.org/pdf/2507.13097v1.pdf",
      "summary": "Grasping is a fundamental robot skill, yet despite significant research\nadvancements, learning-based 6-DOF grasping approaches are still not turnkey\nand struggle to generalize across different embodiments and in-the-wild\nsettings. We build upon the recent success on modeling the object-centric grasp\ngeneration process as an iterative diffusion process. Our proposed framework,\nGraspGen, consists of a DiffusionTransformer architecture that enhances grasp\ngeneration, paired with an efficient discriminator to score and filter sampled\ngrasps. We introduce a novel and performant on-generator training recipe for\nthe discriminator. To scale GraspGen to both objects and grippers, we release a\nnew simulated dataset consisting of over 53 million grasps. We demonstrate that\nGraspGen outperforms prior methods in simulations with singulated objects\nacross different grippers, achieves state-of-the-art performance on the\nFetchBench grasping benchmark, and performs well on a real robot with noisy\nvisual observations.",
      "published": "2025-07-17T13:09:28Z",
      "authors": [
        "Adithyavairavan Murali",
        "Balakumar Sundaralingam",
        "Yu-Wei Chao",
        "Wentao Yuan",
        "Jun Yamada",
        "Mark Carlson",
        "Fabio Ramos",
        "Stan Birchfield",
        "Dieter Fox",
        "Clemens Eppner"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.13097v1",
      "relevance_score": 6
    },
    {
      "title": "The Developments and Challenges towards Dexterous and Embodied Robotic\n  Manipulation: A Survey",
      "link": "https://arxiv.org/abs/2507.11840v1",
      "pdf_link": "https://arxiv.org/pdf/2507.11840v1.pdf",
      "summary": "Achieving human-like dexterous robotic manipulation remains a central goal\nand a pivotal challenge in robotics. The development of Artificial Intelligence\n(AI) has allowed rapid progress in robotic manipulation. This survey summarizes\nthe evolution of robotic manipulation from mechanical programming to embodied\nintelligence, alongside the transition from simple grippers to multi-fingered\ndexterous hands, outlining key characteristics and main challenges. Focusing on\nthe current stage of embodied dexterous manipulation, we highlight recent\nadvances in two critical areas: dexterous manipulation data collection (via\nsimulation, human demonstrations, and teleoperation) and skill-learning\nframeworks (imitation and reinforcement learning). Then, based on the overview\nof the existing data collection paradigm and learning framework, three key\nchallenges restricting the development of dexterous robotic manipulation are\nsummarized and discussed.",
      "published": "2025-07-16T02:09:31Z",
      "authors": [
        "Gaofeng Li",
        "Ruize Wang",
        "Peisen Xu",
        "Qi Ye",
        "Jiming Chen"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.11840v1",
      "relevance_score": 6
    },
    {
      "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model",
      "link": "https://arxiv.org/abs/2507.09985v1",
      "pdf_link": "https://arxiv.org/pdf/2507.09985v1.pdf",
      "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.",
      "published": "2025-07-14T07:05:36Z",
      "authors": [
        "Samson Yu",
        "Kelvin Lin",
        "Harold Soh"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.09985v1",
      "relevance_score": 6
    },
    {
      "title": "Equivariant Volumetric Grasping",
      "link": "https://arxiv.org/abs/2507.18847v1",
      "pdf_link": "https://arxiv.org/pdf/2507.18847v1.pdf",
      "summary": "We propose a new volumetric grasp model that is equivariant to rotations\naround the vertical axis, leading to a significant improvement in sample\nefficiency. Our model employs a tri-plane volumetric feature representation --\ni.e., the projection of 3D features onto three canonical planes. We introduce a\nnovel tri-plane feature design in which features on the horizontal plane are\nequivariant to 90{\\deg} rotations, while the sum of features from the other two\nplanes remains invariant to the same transformations. This design is enabled by\na new deformable steerable convolution, which combines the adaptability of\ndeformable convolutions with the rotational equivariance of steerable ones.\nThis allows the receptive field to adapt to local object geometry while\npreserving equivariance properties. We further develop equivariant adaptations\nof two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,\nwe derive a new equivariant formulation of IGD's deformable attention mechanism\nand propose an equivariant generative model of grasp orientations based on flow\nmatching. We provide a detailed analytical justification of the proposed\nequivariance properties and validate our approach through extensive simulated\nand real-world experiments. Our results demonstrate that the proposed\nprojection-based design significantly reduces both computational and memory\ncosts. Moreover, the equivariant grasp models built on top of our tri-plane\nfeatures consistently outperform their non-equivariant counterparts, achieving\nhigher performance with only a modest computational overhead. Video and code\ncan be viewed in: https://mousecpn.github.io/evg-page/",
      "published": "2025-07-24T23:18:32Z",
      "authors": [
        "Pinhao Song",
        "Yutong Hu",
        "Pengteng Li",
        "Renaud Detry"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.18847v1",
      "relevance_score": 5
    },
    {
      "title": "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level\n  Tactile Feedback",
      "link": "https://arxiv.org/abs/2507.17294v1",
      "pdf_link": "https://arxiv.org/pdf/2507.17294v1.pdf",
      "summary": "Tactile feedback is generally recognized to be crucial for effective\ninteraction with the physical world. However, state-of-the-art\nVision-Language-Action (VLA) models lack the ability to interpret and use\ntactile signals, limiting their effectiveness in contact-rich tasks.\nIncorporating tactile feedback into these systems is challenging due to the\nabsence of large multi-modal datasets. We present VLA-Touch, an approach that\nenhances generalist robot policies with tactile sensing \\emph{without\nfine-tuning} the base VLA. Our method introduces two key innovations: (1) a\npipeline that leverages a pretrained tactile-language model that provides\nsemantic tactile feedback for high-level task planning, and (2) a\ndiffusion-based controller that refines VLA-generated actions with tactile\nsignals for contact-rich manipulation. Through real-world experiments, we\ndemonstrate that our dual-level integration of tactile feedback improves task\nplanning efficiency while enhancing execution precision. Code is open-sourced\nat \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.",
      "published": "2025-07-23T07:54:10Z",
      "authors": [
        "Jianxin Bi",
        "Kevin Yuchen Ma",
        "Ce Hao",
        "Mike Zheng Shou",
        "Harold Soh"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.17294v1",
      "relevance_score": 5
    },
    {
      "title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D\n  Correspondence Learning",
      "link": "https://arxiv.org/abs/2507.14820v1",
      "pdf_link": "https://arxiv.org/pdf/2507.14820v1.pdf",
      "summary": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.",
      "published": "2025-07-20T04:35:31Z",
      "authors": [
        "Bingran Chen",
        "Baorun Li",
        "Jian Yang",
        "Yong Liu",
        "Guangyao Zhai"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.14820v1",
      "relevance_score": 5
    },
    {
      "title": "Few-shot transfer of tool-use skills using human demonstrations with\n  proximity and tactile sensing",
      "link": "https://arxiv.org/abs/2507.13200v1",
      "pdf_link": "https://arxiv.org/pdf/2507.13200v1.pdf",
      "summary": "Tools extend the manipulation abilities of robots, much like they do for\nhumans. Despite human expertise in tool manipulation, teaching robots these\nskills faces challenges. The complexity arises from the interplay of two\nsimultaneous points of contact: one between the robot and the tool, and another\nbetween the tool and the environment. Tactile and proximity sensors play a\ncrucial role in identifying these complex contacts. However, learning tool\nmanipulation using these sensors remains challenging due to limited real-world\ndata and the large sim-to-real gap. To address this, we propose a few-shot\ntool-use skill transfer framework using multimodal sensing. The framework\ninvolves pre-training the base policy to capture contact states common in\ntool-use skills in simulation and fine-tuning it with human demonstrations\ncollected in the real-world target domain to bridge the domain gap. We validate\nthat this framework enables teaching surface-following tasks using tools with\ndiverse physical and geometric properties with a small number of demonstrations\non the Franka Emika robot arm. Our analysis suggests that the robot acquires\nnew tool-use skills by transferring the ability to recognise tool-environment\ncontact relationships from pre-trained to fine-tuned policies. Additionally,\ncombining proximity and tactile sensors enhances the identification of contact\nstates and environmental geometry.",
      "published": "2025-07-17T15:10:12Z",
      "authors": [
        "Marina Y. Aoyama",
        "Sethu Vijayakumar",
        "Tetsuya Narita"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.13200v1",
      "relevance_score": 5
    },
    {
      "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
      "link": "https://arxiv.org/abs/2507.12768v1",
      "pdf_link": "https://arxiv.org/pdf/2507.12768v1.pdf",
      "summary": "Vision-language-action (VLA) models have shown promise on task-conditioned\ncontrol in complex settings such as bimanual manipulation. However, the heavy\nreliance on task-specific human demonstrations limits their generalization and\nincurs high data acquisition costs. In this work, we present a new notion of\ntask-agnostic action paradigm that decouples action execution from\ntask-specific conditioning, enhancing scalability, efficiency, and\ncost-effectiveness. To address the data collection challenges posed by this\nparadigm -- such as low coverage density, behavioral redundancy, and safety\nrisks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a\nscalable self-supervised framework that accelerates collection by over $\n30\\times $ compared to human teleoperation. To further enable effective\nlearning from task-agnostic data, which often suffers from distribution\nmismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics\nmodel equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder\n(DAD). We additionally integrate a video-conditioned action validation module\nto verify the feasibility of learned policies across diverse manipulation\ntasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%\nimprovement in test accuracy and achieves 30-40% higher success rates in\ndownstream tasks such as lifting, pick-and-place, and clicking, using\nreplay-based video validation. Project Page:\nhttps://embodiedfoundation.github.io/vidar_anypos",
      "published": "2025-07-17T03:48:57Z",
      "authors": [
        "Hengkai Tan",
        "Yao Feng",
        "Xinyi Mao",
        "Shuhe Huang",
        "Guodong Liu",
        "Zhongkai Hao",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12768v1",
      "relevance_score": 5
    },
    {
      "title": "Object-Centric Mobile Manipulation through SAM2-Guided Perception and\n  Imitation Learning",
      "link": "https://arxiv.org/abs/2507.10899v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10899v1.pdf",
      "summary": "Imitation learning for mobile manipulation is a key challenge in the field of\nrobotic manipulation. However, current mobile manipulation frameworks typically\ndecouple navigation and manipulation, executing manipulation only after\nreaching a certain location. This can lead to performance degradation when\nnavigation is imprecise, especially due to misalignment in approach angles. To\nenable a mobile manipulator to perform the same task from diverse orientations,\nan essential capability for building general-purpose robotic models, we propose\nan object-centric method based on SAM2, a foundation model towards solving\npromptable visual segmentation in images, which incorporates manipulation\norientation information into our model. Our approach enables consistent\nunderstanding of the same task from different orientations. We deploy the model\non a custom-built mobile manipulator and evaluate it on a pick-and-place task\nunder varied orientation angles. Compared to Action Chunking Transformer, our\nmodel maintains superior generalization when trained with demonstrations from\nvaried approach angles. This work significantly enhances the generalization and\nrobustness of imitation learning-based mobile manipulation systems.",
      "published": "2025-07-15T01:26:59Z",
      "authors": [
        "Wang Zhicheng",
        "Satoshi Yagi",
        "Satoshi Yamamori",
        "Jun Morimoto"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10899v1",
      "relevance_score": 5
    },
    {
      "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware\n  Diffusers",
      "link": "https://arxiv.org/abs/2507.11287v1",
      "pdf_link": "https://arxiv.org/pdf/2507.11287v1.pdf",
      "summary": "In this paper, we study task-oriented human grasp synthesis, a new grasp\nsynthesis task that demands both task and context awareness. At the core of our\nmethod is the task-aware contact maps. Unlike traditional contact maps that\nonly reason about the manipulated object and its relation with the hand, our\nenhanced maps take into account scene and task information. This comprehensive\nmap is critical for hand-object interaction, enabling accurate grasping poses\nthat align with the task. We propose a two-stage pipeline that first constructs\na task-aware contact map informed by the scene and task. In the subsequent\nstage, we use this contact map to synthesize task-oriented human grasps. We\nintroduce a new dataset and a metric for the proposed task to evaluate our\napproach. Our experiments validate the importance of modeling both scene and\ntask, demonstrating significant improvements over existing methods in both\ngrasp quality and task performance. See our project page for more details:\nhttps://hcis-lab.github.io/TOHGS/",
      "published": "2025-07-15T13:11:55Z",
      "authors": [
        "An-Lun Liu",
        "Yu-Wei Chao",
        "Yi-Ting Chen"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.11287v1",
      "relevance_score": 5
    },
    {
      "title": "DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View\n  Images for Robotic Manipulation",
      "link": "https://arxiv.org/abs/2507.05627v1",
      "pdf_link": "https://arxiv.org/pdf/2507.05627v1.pdf",
      "summary": "Partial-view 3D recognition -- reconstructing 3D geometry and identifying\nobject instances from a few sparse RGB images -- is an exceptionally\nchallenging yet practically essential task, particularly in cluttered, occluded\nreal-world settings where full-view or reliable depth data are often\nunavailable. Existing methods, whether based on strong symmetry priors or\nsupervised learning on curated datasets, fail to generalize to such scenarios.\nIn this work, we introduce DreamGrasp, a framework that leverages the\nimagination capability of large-scale pre-trained image generative models to\ninfer the unobserved parts of a scene. By combining coarse 3D reconstruction,\ninstance segmentation via contrastive learning, and text-guided instance-wise\nrefinement, DreamGrasp circumvents limitations of prior methods and enables\nrobust 3D reconstruction in complex, multi-object environments. Our experiments\nshow that DreamGrasp not only recovers accurate object geometry but also\nsupports downstream tasks like sequential decluttering and target retrieval\nwith high success rates.",
      "published": "2025-07-08T03:12:49Z",
      "authors": [
        "Young Hun Kim",
        "Seungyeon Kim",
        "Yonghyeon Lee",
        "Frank Chongwoo Park"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.05627v1",
      "relevance_score": 5
    },
    {
      "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable\n  Evaluation",
      "link": "https://arxiv.org/abs/2507.00435v1",
      "pdf_link": "https://arxiv.org/pdf/2507.00435v1.pdf",
      "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone.",
      "published": "2025-07-01T05:33:16Z",
      "authors": [
        "Yi Ru Wang",
        "Carter Ung",
        "Grant Tannert",
        "Jiafei Duan",
        "Josephine Li",
        "Amy Le",
        "Rishabh Oswal",
        "Markus Grotz",
        "Wilbert Pumacay",
        "Yuquan Deng",
        "Ranjay Krishna",
        "Dieter Fox",
        "Siddhartha Srinivasa"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.00435v1",
      "relevance_score": 5
    },
    {
      "title": "EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled\n  Videos via Embodiment-Centric Flow",
      "link": "https://arxiv.org/abs/2507.06224v1",
      "pdf_link": "https://arxiv.org/pdf/2507.06224v1.pdf",
      "summary": "Current language-guided robotic manipulation systems often require low-level\naction-labeled datasets for imitation learning. While object-centric flow\nprediction methods mitigate this issue, they remain limited to scenarios\ninvolving rigid objects with clear displacement and minimal occlusion. In this\nwork, we present Embodiment-Centric Flow (EC-Flow), a framework that directly\nlearns manipulation from action-unlabeled videos by predicting\nembodiment-centric flow. Our key insight is that incorporating the embodiment's\ninherent kinematics significantly enhances generalization to versatile\nmanipulation scenarios, including deformable object handling, occlusions, and\nnon-object-displacement tasks. To connect the EC-Flow with language\ninstructions and object interactions, we further introduce a goal-alignment\nmodule by jointly optimizing movement consistency and goal-image prediction.\nMoreover, translating EC-Flow to executable robot actions only requires a\nstandard robot URDF (Unified Robot Description Format) file to specify\nkinematic constraints across joints, which makes it easy to use in practice. We\nvalidate EC-Flow on both simulation (Meta-World) and real-world tasks,\ndemonstrating its state-of-the-art performance in occluded object handling (62%\nimprovement), deformable object manipulation (45% improvement), and\nnon-object-displacement tasks (80% improvement) than prior state-of-the-art\nobject-centric flow methods. For more information, see our project website at\nhttps://ec-flow1.github.io .",
      "published": "2025-07-08T17:57:03Z",
      "authors": [
        "Yixiang Chen",
        "Peiyan Li",
        "Yan Huang",
        "Jiabing Yang",
        "Kehan Chen",
        "Liang Wang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.06224v1",
      "relevance_score": 5
    },
    {
      "title": "PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion\n  Policy",
      "link": "https://arxiv.org/abs/2507.17846v1",
      "pdf_link": "https://arxiv.org/pdf/2507.17846v1.pdf",
      "summary": "Pottery creation is a complicated art form that requires dexterous, precise\nand delicate actions to slowly morph a block of clay to a meaningful, and often\nuseful 3D goal shape. In this work, we aim to create a robotic system that can\ncreate simple pottery goals with only pinch-based actions. This pinch pottery\ntask allows us to explore the challenges of a highly multi-modal and\nlong-horizon deformable manipulation task. To this end, we present PinchBot, a\ngoal-conditioned diffusion policy model that when combined with pre-trained 3D\npoint cloud embeddings, task progress prediction and collision-constrained\naction projection, is able to successfully create a variety of simple pottery\ngoals. For experimental videos and access to the demonstration dataset, please\nvisit our project website:\nhttps://sites.google.com/andrew.cmu.edu/pinchbot/home.",
      "published": "2025-07-23T18:13:41Z",
      "authors": [
        "Alison Bartsch",
        "Arvind Car",
        "Amir Barati Farimani"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.17846v1",
      "relevance_score": 4
    },
    {
      "title": "Deformable Cluster Manipulation via Whole-Arm Policy Learning",
      "link": "https://arxiv.org/abs/2507.17085v1",
      "pdf_link": "https://arxiv.org/pdf/2507.17085v1.pdf",
      "summary": "Manipulating clusters of deformable objects presents a substantial challenge\nwith widespread applicability, but requires contact-rich whole-arm\ninteractions. A potential solution must address the limited capacity for\nrealistic model synthesis, high uncertainty in perception, and the lack of\nefficient spatial abstractions, among others. We propose a novel framework for\nlearning model-free policies integrating two modalities: 3D point clouds and\nproprioceptive touch indicators, emphasising manipulation with full body\ncontact awareness, going beyond traditional end-effector modes. Our\nreinforcement learning framework leverages a distributional state\nrepresentation, aided by kernel mean embeddings, to achieve improved training\nefficiency and real-time inference. Furthermore, we propose a novel\ncontext-agnostic occlusion heuristic to clear deformables from a target region\nfor exposure tasks. We deploy the framework in a power line clearance scenario\nand observe that the agent generates creative strategies leveraging multiple\narm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy\ntransfer, allowing the arm to clear real branches with unknown occlusion\npatterns, unseen topology, and uncertain dynamics.",
      "published": "2025-07-22T23:58:30Z",
      "authors": [
        "Jayadeep Jacob",
        "Wenzheng Zhang",
        "Houston Warren",
        "Paulo Borges",
        "Tirthankar Bandyopadhyay",
        "Fabio Ramos"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.17085v1",
      "relevance_score": 4
    },
    {
      "title": "Generalist Bimanual Manipulation via Foundation Video Diffusion Models",
      "link": "https://arxiv.org/abs/2507.12898v1",
      "pdf_link": "https://arxiv.org/pdf/2507.12898v1.pdf",
      "summary": "Bimanual robotic manipulation, which involves the coordinated control of two\nrobotic arms, is foundational for solving challenging tasks. Despite recent\nprogress in general-purpose manipulation, data scarcity and embodiment\nheterogeneity remain serious obstacles to further scaling up in bimanual\nsettings. In this paper, we introduce VIdeo Diffusion for Action Reasoning\n(VIDAR), a two-stage framework that leverages large-scale, diffusion-based\nvideo pre-training and a novel masked inverse dynamics model for action\nprediction. We pre-train the video diffusion model on 750K multi-view videos\nfrom three real-world bimanual robot platforms, utilizing a unified observation\nspace that encodes robot, camera, task, and scene contexts. Our masked inverse\ndynamics model learns masks to extract action-relevant information from\ngenerated trajectories without requiring pixel-level labels, and the masks can\neffectively generalize to unseen backgrounds. Our experiments demonstrate that\nwith only 20 minutes of human demonstrations on an unseen robot platform (only\n1% of typical data requirements), VIDAR generalizes to unseen tasks and\nbackgrounds with strong semantic understanding, surpassing state-of-the-art\nmethods. Our findings highlight the potential of video foundation models,\ncoupled with masked action prediction, to enable scalable and generalizable\nrobotic manipulation in diverse real-world settings.",
      "published": "2025-07-17T08:31:55Z",
      "authors": [
        "Yao Feng",
        "Hengkai Tan",
        "Xinyi Mao",
        "Guodong Liu",
        "Shuhe Huang",
        "Chendong Xiang",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.12898v1",
      "relevance_score": 4
    },
    {
      "title": "Accurate Pose Estimation Using Contact Manifold Sampling for Safe\n  Peg-in-Hole Insertion of Complex Geometries",
      "link": "https://arxiv.org/abs/2507.03925v1",
      "pdf_link": "https://arxiv.org/pdf/2507.03925v1.pdf",
      "summary": "Robotic assembly of complex, non-convex geometries with tight clearances\nremains a challenging problem, demanding precise state estimation for\nsuccessful insertion. In this work, we propose a novel framework that relies\nsolely on contact states to estimate the full SE(3) pose of a peg relative to a\nhole. Our method constructs an online submanifold of contact states through\nprimitive motions with just 6 seconds of online execution, subsequently mapping\nit to an offline contact manifold for precise pose estimation. We demonstrate\nthat without such state estimation, robots risk jamming and excessive force\napplication, potentially causing damage. We evaluate our approach on five\nindustrially relevant, complex geometries with 0.1 to 1.0 mm clearances,\nachieving a 96.7% success rate - a 6x improvement over primitive-based\ninsertion without state estimation. Additionally, we analyze insertion forces,\nand overall insertion times, showing our method significantly reduces the\naverage wrench, enabling safer and more efficient assembly.",
      "published": "2025-07-05T07:18:07Z",
      "authors": [
        "Abhay Negi",
        "Omey M. Manyar",
        "Dhanush K. Penmetsa",
        "Satyandra K. Gupta"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.03925v1",
      "relevance_score": 4
    },
    {
      "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active\n  Instance-Level Object Understanding",
      "link": "https://arxiv.org/abs/2507.10776v1",
      "pdf_link": "https://arxiv.org/pdf/2507.10776v1.pdf",
      "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.",
      "published": "2025-07-14T20:02:52Z",
      "authors": [
        "Howard H. Qian",
        "Yiting Chen",
        "Gaotian Wang",
        "Podshara Chanrungmaneekul",
        "Kaiyu Hang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2507.10776v1",
      "relevance_score": 4
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}