{
  "last_updated": "2025-09-29T08:30:30.640785",
  "total_papers": 25,
  "papers": [
    {
      "title": "D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects",
      "link": "https://arxiv.org/abs/2509.19892v1",
      "pdf_link": "https://arxiv.org/pdf/2509.19892v1.pdf",
      "summary": "Achieving diverse and stable dexterous grasping for general and deformable\nobjects remains a fundamental challenge in robotics, due to high-dimensional\naction spaces and uncertainty in perception. In this paper, we present D3Grasp,\na multimodal perception-guided reinforcement learning framework designed to\nenable Diverse and Deformable Dexterous Grasping. We firstly introduce a\nunified multimodal representation that integrates visual and tactile perception\nto robustly grasp common objects with diverse properties. Second, we propose an\nasymmetric reinforcement learning architecture that exploits privileged\ninformation during training while preserving deployment realism, enhancing both\ngeneralization and sample efficiency. Third, we meticulously design a training\nstrategy to synthesize contact-rich, penetration-free, and kinematically\nfeasible grasps with enhanced adaptability to deformable and contact-sensitive\nobjects. Extensive evaluations confirm that D3Grasp delivers highly robust\nperformance across large-scale and diverse object categories, and substantially\nadvances the state of the art in dexterous grasping for deformable and\ncompliant objects, even under perceptual uncertainty and real-world\ndisturbances. D3Grasp achieves an average success rate of 95.1% in real-world\ntrials,outperforming prior methods on both rigid and deformable objects\nbenchmarks.",
      "published": "2025-09-24T08:42:22Z",
      "authors": [
        "Keyu Wang",
        "Bingcong Lu",
        "Zhengxue Cheng",
        "Hengdi Zhang",
        "Li Song"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19892v1",
      "relevance_score": 11
    },
    {
      "title": "TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic\n  Manipulation",
      "link": "https://arxiv.org/abs/2509.16550v1",
      "pdf_link": "https://arxiv.org/pdf/2509.16550v1.pdf",
      "summary": "Robotic manipulation tasks such as inserting a key into a lock or plugging a\nUSB device into a port can fail when visual perception is insufficient to\ndetect misalignment. In these situations, touch sensing is crucial for the\nrobot to monitor the task's states and make precise, timely adjustments.\nCurrent touch sensing solutions are either insensitive to detect subtle changes\nor demand excessive sensor data. Here, we introduce TranTac, a data-efficient\nand low-cost tactile sensing and control framework that integrates a single\ncontact-sensitive 6-axis inertial measurement unit within the elastomeric tips\nof a robotic gripper for completing fine insertion tasks. Our customized\nsensing system can detect dynamic translational and torsional deformations at\nthe micrometer scale, enabling the tracking of visually imperceptible pose\nchanges of the grasped object. By leveraging transformer-based encoders and\ndiffusion policy, TranTac can imitate human insertion behaviors using transient\ntactile cues detected at the gripper's tip during insertion processes. These\ncues enable the robot to dynamically control and correct the 6-DoF pose of the\ngrasped object. When combined with vision, TranTac achieves an average success\nrate of 79% on object grasping and insertion tasks, outperforming both\nvision-only policy and the one augmented with end-effector 6D force/torque\nsensing. Contact localization performance is also validated through\ntactile-only misaligned insertion tasks, achieving an average success rate of\n88%. We assess the generalizability by training TranTac on a single prism-slot\npair and testing it on unseen data, including a USB plug and a metal key, and\nfind that the insertion tasks can still be completed with an average success\nrate of nearly 70%. The proposed framework may inspire new robotic tactile\nsensing systems for delicate manipulation tasks.",
      "published": "2025-09-20T06:25:59Z",
      "authors": [
        "Yinghao Wu",
        "Shuhong Hou",
        "Haowen Zheng",
        "Yichen Li",
        "Weiyi Lu",
        "Xun Zhou",
        "Yitian Shao"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.16550v1",
      "relevance_score": 11
    },
    {
      "title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration",
      "link": "https://arxiv.org/abs/2509.22149v1",
      "pdf_link": "https://arxiv.org/pdf/2509.22149v1.pdf",
      "summary": "Universal grasping with multi-fingered dexterous hands is a fundamental\nchallenge in robotic manipulation. While recent approaches successfully learn\nclosed-loop grasping policies using reinforcement learning (RL), the inherent\ndifficulty of high-dimensional, long-horizon exploration necessitates complex\nreward and curriculum design, often resulting in suboptimal solutions across\ndiverse objects. We propose DemoGrasp, a simple yet effective method for\nlearning universal dexterous grasping. We start from a single successful\ndemonstration trajectory of grasping a specific object and adapt to novel\nobjects and poses by editing the robot actions in this trajectory: changing the\nwrist pose determines where to grasp, and changing the hand joint angles\ndetermines how to grasp. We formulate this trajectory editing as a single-step\nMarkov Decision Process (MDP) and use RL to optimize a universal policy across\nhundreds of objects in parallel in simulation, with a simple reward consisting\nof a binary success term and a robot-table collision penalty. In simulation,\nDemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow\nHand, outperforming previous state-of-the-art methods. It also shows strong\ntransferability, achieving an average success rate of 84.6% across diverse\ndexterous hand embodiments on six unseen object datasets, while being trained\non only 175 objects. Through vision-based imitation learning, our policy\nsuccessfully grasps 110 unseen real-world objects, including small, thin items.\nIt generalizes to spatial, background, and lighting changes, supports both RGB\nand depth inputs, and extends to language-guided grasping in cluttered scenes.",
      "published": "2025-09-26T10:09:51Z",
      "authors": [
        "Haoqi Yuan",
        "Ziye Huang",
        "Ye Wang",
        "Chuan Mao",
        "Chaoyi Xu",
        "Zongqing Lu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.22149v1",
      "relevance_score": 10
    },
    {
      "title": "Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive\n  Grasping",
      "link": "https://arxiv.org/abs/2509.22421v1",
      "pdf_link": "https://arxiv.org/pdf/2509.22421v1.pdf",
      "summary": "Grasping is a core task in robotics with various applications. However, most\ncurrent implementations are primarily designed for rigid items, and their\nperformance drops considerably when handling fragile or deformable materials\nthat require real-time feedback. Meanwhile, tactile-reactive grasping focuses\non a single agent, which limits their ability to grasp and manipulate large,\nheavy objects. To overcome this, we propose a learning-based, tactile-reactive\nmulti-agent Model Predictive Controller (MPC) for grasping a wide range of\nobjects with different softness and shapes, beyond the capabilities of\npreexisting single-agent implementations. Our system uses two Gelsight Mini\ntactile sensors [1] to extract real-time information on object texture and\nstiffness. This rich tactile feedback is used to estimate contact dynamics and\nobject compliance in real time, enabling the system to adapt its control policy\nto diverse object geometries and stiffness profiles. The learned controller\noperates in a closed loop, leveraging tactile encoding to predict grasp\nstability and adjust force and position accordingly. Our key technical\ncontributions include a multi-agent MPC formulation trained on real contact\ninteractions, a tactile-data driven method for inferring grasping states, and a\ncoordination strategy that enables collaborative control. By combining tactile\nsensing and a learning-based multi-agent MPC, our method offers a robust,\nintelligent solution for collaborative grasping in complex environments,\nsignificantly advancing the capabilities of multi-agent systems. Our approach\nis validated through extensive experiments against independent PD and MPC\nbaselines. Our pipeline outperforms the baselines regarding success rates in\nachieving and maintaining stable grasps across objects of varying sizes and\nstiffness.",
      "published": "2025-09-24T18:03:12Z",
      "authors": [
        "Leonel Giacobbe",
        "Jingdao Chen",
        "Chuangchuang Sun"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.22421v1",
      "relevance_score": 10
    },
    {
      "title": "EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation",
      "link": "https://arxiv.org/abs/2509.20656v1",
      "pdf_link": "https://arxiv.org/pdf/2509.20656v1.pdf",
      "summary": "Reliable brain-computer interface (BCI) control of robots provides an\nintuitive and accessible means of human-robot interaction, particularly\nvaluable for individuals with motor impairments. However, existing BCI-Robot\nsystems face major limitations: electroencephalography (EEG) signals are noisy\nand unstable, target selection is often predefined and inflexible, and most\nstudies remain restricted to simulation without closed-loop validation. These\nissues hinder real-world deployment in assistive scenarios. To address them, we\npropose a closed-loop BCI-AR-Robot system that integrates motor imagery\n(MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic\ngrasping for zero-touch operation. A 14-channel EEG headset enabled\nindividualized MI calibration, a smartphone-based AR interface supported\nmulti-target navigation with direction-congruent feedback to enhance stability,\nand the robotic arm combined decision outputs with vision-based pose estimation\nfor autonomous grasping. Experiments are conducted to validate the framework:\nMI training achieved 93.1 percent accuracy with an average information transfer\nrate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained\ncontrol (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with\nstatic, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2\npercent success rate with good efficiency and strong user-reported control.\nThese results show that AR feedback substantially stabilizes EEG-based control\nand that the proposed framework enables robust zero-touch grasping, advancing\nassistive robotic applications and future modes of human-robot interaction.",
      "published": "2025-09-25T01:28:40Z",
      "authors": [
        "Junzhe Wang",
        "Jiarui Xie",
        "Pengfei Hao",
        "Zheng Li",
        "Yi Cai"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.20656v1",
      "relevance_score": 9
    },
    {
      "title": "FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque\n  Control for Contact-Rich Manipulation Tasks",
      "link": "https://arxiv.org/abs/2509.17053v1",
      "pdf_link": "https://arxiv.org/pdf/2509.17053v1.pdf",
      "summary": "Contact-rich manipulation is crucial for robots to perform tasks requiring\nprecise force control, such as insertion, assembly, and in-hand manipulation.\nHowever, most imitation learning (IL) policies remain position-centric and lack\nexplicit force awareness, and adding force/torque sensors to collaborative\nrobot arms is often costly and requires additional hardware design. To overcome\nthese issues, we propose FILIC, a Force-guided Imitation Learning framework\nwith impedance torque control. FILIC integrates a Transformer-based IL policy\nwith an impedance controller in a dual-loop structure, enabling compliant\nforce-informed, force-executed manipulation. For robots without force/torque\nsensors, we introduce a cost-effective end-effector force estimator using joint\ntorque measurements through analytical Jacobian-based inversion while\ncompensating with model-predicted torques from a digital twin. We also design\ncomplementary force feedback frameworks via handheld haptics and VR\nvisualization to improve demonstration quality. Experiments show that FILIC\nsignificantly outperforms vision-only and joint-torque-based methods, achieving\nsafer, more compliant, and adaptable contact-rich manipulation. Our code can be\nfound in https://github.com/TATP-233/FILIC.",
      "published": "2025-09-21T12:17:20Z",
      "authors": [
        "Haizhou Ge",
        "Yufei Jia",
        "Zheng Li",
        "Yue Li",
        "Zhixing Chen",
        "Ruqi Huang",
        "Guyue Zhou"
      ],
      "categories": [
        "cs.RO",
        "68T40, 93C85",
        "I.2.9"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.17053v1",
      "relevance_score": 9
    },
    {
      "title": "The Role of Touch: Towards Optimal Tactile Sensing Distribution in\n  Anthropomorphic Hands for Dexterous In-Hand Manipulation",
      "link": "https://arxiv.org/abs/2509.14984v1",
      "pdf_link": "https://arxiv.org/pdf/2509.14984v1.pdf",
      "summary": "In-hand manipulation tasks, particularly in human-inspired robotic systems,\nmust rely on distributed tactile sensing to achieve precise control across a\nwide variety of tasks. However, the optimal configuration of this network of\nsensors is a complex problem, and while the fingertips are a common choice for\nplacing sensors, the contribution of tactile information from other regions of\nthe hand is often overlooked. This work investigates the impact of tactile\nfeedback from various regions of the fingers and palm in performing in-hand\nobject reorientation tasks. We analyze how sensory feedback from different\nparts of the hand influences the robustness of deep reinforcement learning\ncontrol policies and investigate the relationship between object\ncharacteristics and optimal sensor placement. We identify which tactile sensing\nconfigurations contribute to improving the efficiency and accuracy of\nmanipulation. Our results provide valuable insights for the design and use of\nanthropomorphic end-effectors with enhanced manipulation capabilities.",
      "published": "2025-09-18T14:13:26Z",
      "authors": [
        "João Damião Almeida",
        "Egidio Falotico",
        "Cecilia Laschi",
        "José Santos-Victor"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.14984v1",
      "relevance_score": 9
    },
    {
      "title": "Soft Regrasping Tool Inspired by Jamming Gripper",
      "link": "https://arxiv.org/abs/2509.13815v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13815v1.pdf",
      "summary": "Regrasping on fixtures is a promising approach to reduce pose uncertainty in\nrobotic assembly, but conventional rigid fixtures lack adaptability and require\ndedicated designs for each part. To overcome this limitation, we propose a soft\njig inspired by the jamming transition phenomenon, which can be continuously\ndeformed to accommodate diverse object geometries. By pressing a\ntriangular-pyramid-shaped tool into the membrane and evacuating the enclosed\nair, a stable cavity is formed as a placement space. We further optimize the\nstamping depth to balance placement stability and gripper accessibility. In\nsoft-jig-based regrasping, the key challenge lies in optimizing the cavity size\nto achieve precise dropping; once the part is reliably placed, subsequent\ngrasping can be performed with reduced uncertainty. Accordingly, we conducted\ndrop experiments on ten mechanical parts of varying shapes, which achieved\nplacement success rates exceeding 80% for most objects and above 90% for\ncylindrical ones, while failures were mainly caused by geometric constraints\nand membrane properties. These results demonstrate that the proposed jig\nenables general-purpose, accurate, and repeatable regrasping, while also\nclarifying its current limitations and future potential as a practical\nalternative to rigid fixtures in assembly automation.",
      "published": "2025-09-17T08:30:21Z",
      "authors": [
        "Takuya Kiyokawa",
        "Zhengtao Hu",
        "Weiwei Wan",
        "Kensuke Harada"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13815v1",
      "relevance_score": 8
    },
    {
      "title": "Collaborative Loco-Manipulation for Pick-and-Place Tasks with Dynamic\n  Reward Curriculum",
      "link": "https://arxiv.org/abs/2509.13239v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13239v1.pdf",
      "summary": "We present a hierarchical RL pipeline for training one-armed legged robots to\nperform pick-and-place (P&P) tasks end-to-end -- from approaching the payload\nto releasing it at a target area -- in both single-robot and cooperative\ndual-robot settings. We introduce a novel dynamic reward curriculum that\nenables a single policy to efficiently learn long-horizon P&P operations by\nprogressively guiding the agents through payload-centered sub-objectives.\nCompared to state-of-the-art approaches for long-horizon RL tasks, our method\nimproves training efficiency by 55% and reduces execution time by 18.6% in\nsimulation experiments. In the dual-robot case, we show that our policy enables\neach robot to attend to different components of its observation space at\ndistinct task stages, promoting effective coordination via autonomous attention\nshifts. We validate our method through real-world experiments using ANYmal D\nplatforms in both single- and dual-robot scenarios. To our knowledge, this is\nthe first RL pipeline that tackles the full scope of collaborative P&P with two\nlegged manipulators.",
      "published": "2025-09-16T16:45:24Z",
      "authors": [
        "Tianxu An",
        "Flavio De Vincenti",
        "Yuntao Ma",
        "Marco Hutter",
        "Stelian Coros"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13239v1",
      "relevance_score": 8
    },
    {
      "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning\n  Contact-Rich Manipulation",
      "link": "https://arxiv.org/abs/2509.18830v1",
      "pdf_link": "https://arxiv.org/pdf/2509.18830v1.pdf",
      "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.",
      "published": "2025-09-23T09:16:34Z",
      "authors": [
        "Suzannah Wistreich",
        "Baiyu Shi",
        "Stephen Tian",
        "Samuel Clarke",
        "Michael Nath",
        "Chengyi Xu",
        "Zhenan Bao",
        "Jiajun Wu"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.18830v1",
      "relevance_score": 8
    },
    {
      "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration",
      "link": "https://arxiv.org/abs/2509.09671v1",
      "pdf_link": "https://arxiv.org/pdf/2509.09671v1.pdf",
      "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation.",
      "published": "2025-09-11T17:59:07Z",
      "authors": [
        "Sirui Xu",
        "Yu-Wei Chao",
        "Liuyu Bian",
        "Arsalan Mousavian",
        "Yu-Xiong Wang",
        "Liang-Yan Gui",
        "Wei Yang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.09671v1",
      "relevance_score": 8
    },
    {
      "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware\n  Representation for Contact-Rich Manipulation",
      "link": "https://arxiv.org/abs/2509.19047v1",
      "pdf_link": "https://arxiv.org/pdf/2509.19047v1.pdf",
      "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.",
      "published": "2025-09-23T14:15:19Z",
      "authors": [
        "Geonhyup Lee",
        "Yeongjin Lee",
        "Kangmin Kim",
        "Seongju Lee",
        "Sangjun Noh",
        "Seunghyeok Back",
        "Kyoobin Lee"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19047v1",
      "relevance_score": 7
    },
    {
      "title": "SHaRe-RL: Structured, Interactive Reinforcement Learning for\n  Contact-Rich Industrial Assembly Tasks",
      "link": "https://arxiv.org/abs/2509.13949v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13949v1.pdf",
      "summary": "High-mix low-volume (HMLV) industrial assembly, common in small and\nmedium-sized enterprises (SMEs), requires the same precision, safety, and\nreliability as high-volume automation while remaining flexible to product\nvariation and environmental uncertainty. Current robotic systems struggle to\nmeet these demands. Manual programming is brittle and costly to adapt, while\nlearning-based methods suffer from poor sample efficiency and unsafe\nexploration in contact-rich tasks. To address this, we present SHaRe-RL, a\nreinforcement learning framework that leverages multiple sources of prior\nknowledge. By (i) structuring skills into manipulation primitives, (ii)\nincorporating human demonstrations and online corrections, and (iii) bounding\ninteraction forces with per-axis compliance, SHaRe-RL enables efficient and\nsafe online learning for long-horizon, contact-rich industrial assembly tasks.\nExperiments on the insertion of industrial Harting connector modules with\n0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance\nwithin practical time budgets. Our results show that process expertise, without\nrequiring robotics or RL knowledge, can meaningfully contribute to learning,\nenabling safer, more robust, and more economically viable deployment of RL for\nindustrial assembly.",
      "published": "2025-09-17T13:19:59Z",
      "authors": [
        "Jannick Stranghöner",
        "Philipp Hartmann",
        "Marco Braun",
        "Sebastian Wrede",
        "Klaus Neumann"
      ],
      "categories": [
        "cs.RO",
        "I.2.9"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13949v1",
      "relevance_score": 7
    },
    {
      "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
      "link": "https://arxiv.org/abs/2509.21006v1",
      "pdf_link": "https://arxiv.org/pdf/2509.21006v1.pdf",
      "summary": "We address natural language pick-and-place in unseen, unpredictable indoor\nenvironments with AnywhereVLA, a modular framework for mobile manipulation. A\nuser text prompt serves as an entry point and is parsed into a structured task\ngraph that conditions classical SLAM with LiDAR and cameras, metric semantic\nmapping, and a task-aware frontier exploration policy. An approach planner then\nselects visibility and reachability aware pre grasp base poses. For\ninteraction, a compact SmolVLA manipulation head is fine tuned on platform pick\nand place trajectories for the SO-101 by TheRobotStudio, grounding local visual\ncontext and sub-goals into grasp and place proposals. The full system runs\nfully onboard on consumer-level hardware, with Jetson Orin NX for perception\nand VLA and an Intel NUC for SLAM, exploration, and control, sustaining\nreal-time operation. We evaluated AnywhereVLA in a multi-room lab under static\nscenes and normal human motion. In this setting, the system achieves a $46\\%$\noverall task success rate while maintaining throughput on embedded compute. By\ncombining a classical stack with a fine-tuned VLA manipulation, the system\ninherits the reliability of geometry-based navigation with the agility and task\ngeneralization of language-conditioned manipulation.",
      "published": "2025-09-25T11:04:44Z",
      "authors": [
        "Konstantin Gubernatorov",
        "Artem Voronov",
        "Roman Voronov",
        "Sergei Pasynkov",
        "Stepan Perminov",
        "Ziang Guo",
        "Dzmitry Tsetserukou"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.21006v1",
      "relevance_score": 6
    },
    {
      "title": "GraspFactory: A Large Object-Centric Grasping Dataset",
      "link": "https://arxiv.org/abs/2509.20550v1",
      "pdf_link": "https://arxiv.org/pdf/2509.20550v1.pdf",
      "summary": "Robotic grasping is a crucial task in industrial automation, where robots are\nincreasingly expected to handle a wide range of objects. However, a significant\nchallenge arises when robot grasping models trained on limited datasets\nencounter novel objects. In real-world environments such as warehouses or\nmanufacturing plants, the diversity of objects can be vast, and grasping models\nneed to generalize to this diversity. Training large, generalizable\nrobot-grasping models requires geometrically diverse datasets. In this paper,\nwe introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps\ncollectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85\ngrippers (with 33,710 objects). GraspFactory is designed for training\ndata-intensive models, and we demonstrate the generalization capabilities of\none such model trained on a subset of GraspFactory in both simulated and\nreal-world settings. The dataset and tools are made available for download at\nhttps://graspfactory.github.io/.",
      "published": "2025-09-24T20:29:46Z",
      "authors": [
        "Srinidhi Kalgundi Srinivas",
        "Yash Shukla",
        "Adam Arnold",
        "Sachin Chitta"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.20550v1",
      "relevance_score": 6
    },
    {
      "title": "Simultaneous estimation of contact position and tool shape with\n  high-dimensional parameters using force measurements and particle filtering",
      "link": "https://arxiv.org/abs/2509.19732v1",
      "pdf_link": "https://arxiv.org/pdf/2509.19732v1.pdf",
      "summary": "Estimating the contact state between a grasped tool and the environment is\nessential for performing contact tasks such as assembly and object\nmanipulation. Force signals are valuable for estimating the contact state, as\nthey can be utilized even when the contact location is obscured by the tool.\nPrevious studies proposed methods for estimating contact positions using\nforce/torque signals; however, most methods require the geometry of the tool\nsurface to be known. Although several studies have proposed methods that do not\nrequire the tool shape, these methods require considerable time for estimation\nor are limited to tools with low-dimensional shape parameters. Here, we propose\na method for simultaneously estimating the contact position and tool shape,\nwhere the tool shape is represented by a grid, which is high-dimensional (more\nthan 1000 dimensional). The proposed method uses a particle filter in which\neach particle has individual tool shape parameters, thereby to avoid directly\nhandling a high-dimensional parameter space. The proposed method is evaluated\nthrough simulations and experiments using tools with curved shapes on a plane.\nConsequently, the proposed method can estimate the shape of the tool\nsimultaneously with the contact positions, making the contact-position\nestimation more accurate.",
      "published": "2025-09-24T03:24:01Z",
      "authors": [
        "Kyo Kutsuzawa",
        "Mitsuhiro Hayashibe"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19732v1",
      "relevance_score": 6
    },
    {
      "title": "Object Pose Estimation through Dexterous Touch",
      "link": "https://arxiv.org/abs/2509.13591v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13591v1.pdf",
      "summary": "Robust object pose estimation is essential for manipulation and interaction\ntasks in robotics, particularly in scenarios where visual data is limited or\nsensitive to lighting, occlusions, and appearances. Tactile sensors often offer\nlimited and local contact information, making it challenging to reconstruct the\npose from partial data. Our approach uses sensorimotor exploration to actively\ncontrol a robot hand to interact with the object. We train with Reinforcement\nLearning (RL) to explore and collect tactile data. The collected 3D point\nclouds are used to iteratively refine the object's shape and pose. In our\nsetup, one hand holds the object steady while the other performs active\nexploration. We show that our method can actively explore an object's surface\nto identify critical pose features without prior knowledge of the object's\ngeometry. Supplementary material and more demonstrations will be provided at\nhttps://amirshahid.github.io/BimanualTactilePose .",
      "published": "2025-09-16T23:25:05Z",
      "authors": [
        "Amir-Hossein Shahidzadeh",
        "Jiyue Zhu",
        "Kezhou Chen",
        "Sha Yi",
        "Cornelia Fermüller",
        "Yiannis Aloimonos",
        "Xiaolong Wang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13591v1",
      "relevance_score": 6
    },
    {
      "title": "Generating Stable Placements via Physics-guided Diffusion Models",
      "link": "https://arxiv.org/abs/2509.21664v1",
      "pdf_link": "https://arxiv.org/pdf/2509.21664v1.pdf",
      "summary": "Stably placing an object in a multi-object scene is a fundamental challenge\nin robotic manipulation, as placements must be penetration-free, establish\nprecise surface contact, and result in a force equilibrium. To assess\nstability, existing methods rely on running a simulation engine or resort to\nheuristic, appearance-based assessments. In contrast, our approach integrates\nstability directly into the sampling process of a diffusion model. To this end,\nwe query an offline sampling-based planner to gather multi-modal placement\nlabels and train a diffusion model to generate stable placements. The diffusion\nmodel is conditioned on scene and object point clouds, and serves as a\ngeometry-aware prior. We leverage the compositional nature of score-based\ngenerative models to combine this learned prior with a stability-aware loss,\nthereby increasing the likelihood of sampling from regions of high stability.\nImportantly, this strategy requires no additional re-training or fine-tuning,\nand can be directly applied to off-the-shelf models. We evaluate our method on\nfour benchmark scenes where stability can be accurately computed. Our\nphysics-guided models achieve placements that are 56% more robust to forceful\nperturbations while reducing runtime by 47% compared to a state-of-the-art\ngeometric method.",
      "published": "2025-09-25T22:32:35Z",
      "authors": [
        "Philippe Nadeau",
        "Miguel Rogel",
        "Ivan Bilić",
        "Ivan Petrović",
        "Jonathan Kelly"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.21664v1",
      "relevance_score": 5
    },
    {
      "title": "BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation\n  Primitives",
      "link": "https://arxiv.org/abs/2509.21256v1",
      "pdf_link": "https://arxiv.org/pdf/2509.21256v1.pdf",
      "summary": "Non-prehensile manipulation, encompassing ungraspable actions such as\npushing, poking, and pivoting, represents a critical yet underexplored domain\nin robotics due to its contact-rich and analytically intractable nature. In\nthis work, we revisit this problem from two novel perspectives. First, we move\nbeyond the usual single-arm setup and the strong assumption of favorable\nexternal dexterity such as walls, ramps, or edges. Instead, we advocate a\ngeneralizable dual-arm configuration and establish a suite of Bimanual\nNon-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the\nprevailing RL-based paradigm and propose a three-stage, RL-free framework to\nlearn non-prehensile skills. Specifically, we begin by extracting bimanual hand\nmotion trajectories from video demonstrations. Due to visual inaccuracies and\nmorphological gaps, these coarse trajectories are difficult to transfer\ndirectly to robotic end-effectors. To address this, we propose a geometry-aware\npost-optimization algorithm that refines raw motions into executable\nmanipulation primitives that conform to specific motion patterns. Beyond\ninstance-level reproduction, we further enable category-level generalization by\nparameterizing the learned primitives with object-relevant geometric\nattributes, particularly size, resulting in adaptable and general parameterized\nmanipulation primitives. We validate BiNoMaP across a range of representative\nbimanual tasks and diverse object categories, demonstrating its effectiveness,\nefficiency, versatility, and superior generalization capability.",
      "published": "2025-09-25T14:49:48Z",
      "authors": [
        "Huayi Zhou",
        "Kui Jia"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.21256v1",
      "relevance_score": 5
    },
    {
      "title": "Multi-Robot Vision-Based Task and Motion Planning for EV Battery\n  Disassembly and Sorting",
      "link": "https://arxiv.org/abs/2509.21020v1",
      "pdf_link": "https://arxiv.org/pdf/2509.21020v1.pdf",
      "summary": "Electric-vehicle (EV) battery disassembly requires precise multi-robot\ncoordination, short and reliable motions, and robust collision safety in\ncluttered, dynamic scenes. We propose a four-layer task-and-motion planning\n(TAMP) framework that couples symbolic task planning and cost- and\naccessibility-aware allocation with a TP-GMM-guided motion planner learned from\ndemonstrations. Stereo vision with YOLOv8 provides real-time component\nlocalization, while OctoMap-based 3D mapping and FCL(Flexible Collision\nLibrary) checks in MoveIt unify predictive digital-twin collision checking with\nreactive, vision-based avoidance. Validated on two UR10e robots across cable,\nbusbar, service plug, and three leaf-cell removals, the approach yields\nsubstantially more compact and safer motions than a default RRTConnect baseline\nunder identical perception and task assignments: average end-effector path\nlength drops by $-63.3\\%$ and makespan by $-8.1\\%$; per-arm swept volumes\nshrink (R1: $0.583\\rightarrow0.139\\,\\mathrm{m}^3$; R2:\n$0.696\\rightarrow0.252\\,\\mathrm{m}^3$), and mutual overlap decreases by $47\\%$\n($0.064\\rightarrow0.034\\,\\mathrm{m}^3$). These results highlight improved\nautonomy, precision, and safety for multi-robot EV battery disassembly in\nunstructured, dynamic environments.",
      "published": "2025-09-25T11:30:45Z",
      "authors": [
        "Abdelaziz Shaarawy",
        "Cansu Erdogan",
        "Rustam Stolkin",
        "Alireza Rastegarpanah"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.21020v1",
      "relevance_score": 5
    },
    {
      "title": "Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking",
      "link": "https://arxiv.org/abs/2509.14530v1",
      "pdf_link": "https://arxiv.org/pdf/2509.14530v1.pdf",
      "summary": "Strawberries naturally grow in clusters, interwoven with leaves, stems, and\nother fruits, which frequently leads to occlusion. This inherent growth habit\npresents a significant challenge for robotic picking, as traditional\npercept-plan-control systems struggle to reach fruits amid the clutter.\nEffectively picking an occluded strawberry demands dexterous manipulation to\ncarefully bypass or gently move the surrounding soft objects and precisely\naccess the ideal picking point located at the stem just above the calyx. To\naddress this challenge, we introduce a strawberry-picking robotic system that\nlearns from human demonstrations. Our system features a 4-DoF SCARA arm paired\nwith a human teleoperation interface for efficient data collection and\nleverages an End Pose Assisted Action Chunking Transformer (ACT) to develop a\nfine-grained visuomotor picking policy. Experiments under various occlusion\nscenarios demonstrate that our modified approach significantly outperforms the\ndirect implementation of ACT, underscoring its potential for practical\napplication in occluded strawberry picking.",
      "published": "2025-09-18T01:55:13Z",
      "authors": [
        "Zhenghao Fei",
        "Wenwu Lu",
        "Linsheng Hou",
        "Chen Peng"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.14530v1",
      "relevance_score": 5
    },
    {
      "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks",
      "link": "https://arxiv.org/abs/2509.19696v1",
      "pdf_link": "https://arxiv.org/pdf/2509.19696v1.pdf",
      "summary": "Learning methods excel at motion generation in the information domain but are\nnot primarily designed for physical interaction in the energy domain. Impedance\nControl shapes physical interaction but requires task-aware tuning by selecting\nfeasible impedance parameters. We present Diffusion-Based Impedance Learning, a\nframework that combines both domains. A Transformer-based Diffusion Model with\ncross-attention to external wrenches reconstructs a simulated Zero-Force\nTrajectory (sZFT). This captures both translational and rotational task-space\nbehavior. For rotations, we introduce a novel SLERP-based quaternion noise\nscheduler that ensures geometric consistency. The reconstructed sZFT is then\npassed to an energy-based estimator that updates stiffness and damping\nparameters. A directional rule is applied that reduces impedance along non task\naxes while preserving rigidity along task directions. Training data were\ncollected for a parkour scenario and robotic-assisted therapy tasks using\nteleoperation with Apple Vision Pro. With only tens of thousands of samples,\nthe model achieved sub-millimeter positional accuracy and sub-degree rotational\naccuracy. Its compact model size enabled real-time torque control and\nautonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller\nachieved smooth parkour traversal within force and velocity limits and 30/30\nsuccess rates for cylindrical, square, and star peg insertions without any\npeg-specific demonstrations in the training data set. All code for the\nTransformer-based Diffusion Model, the robot controller, and the Apple Vision\nPro telemanipulation framework is publicly available. These results mark an\nimportant step towards Physical AI, fusing model-based control for physical\ninteraction with learning-based methods for trajectory generation.",
      "published": "2025-09-24T02:07:17Z",
      "authors": [
        "Noah Geiger",
        "Tamim Asfour",
        "Neville Hogan",
        "Johannes Lachner"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19696v1",
      "relevance_score": 5
    },
    {
      "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under\n  Changing External Forces",
      "link": "https://arxiv.org/abs/2509.19261v1",
      "pdf_link": "https://arxiv.org/pdf/2509.19261v1.pdf",
      "summary": "Robotic manipulation in dynamic environments often requires seamless\ntransitions between different grasp types to maintain stability and efficiency.\nHowever, achieving smooth and adaptive grasp transitions remains a challenge,\nparticularly when dealing with external forces and complex motion constraints.\nExisting grasp transition strategies often fail to account for varying external\nforces and do not optimize motion performance effectively. In this work, we\npropose an Imitation-Guided Bimanual Planning Framework that integrates\nefficient grasp transition strategies and motion performance optimization to\nenhance stability and dexterity in robotic manipulation. Our approach\nintroduces Strategies for Sampling Stable Intersections in Grasp Manifolds for\nseamless transitions between uni-manual and bi-manual grasps, reducing\ncomputational costs and regrasping inefficiencies. Additionally, a Hierarchical\nDual-Stage Motion Architecture combines an Imitation Learning-based Global Path\nGenerator with a Quadratic Programming-driven Local Planner to ensure real-time\nmotion feasibility, obstacle avoidance, and superior manipulability. The\nproposed method is evaluated through a series of force-intensive tasks,\ndemonstrating significant improvements in grasp transition efficiency and\nmotion performance. A video demonstrating our simulation results can be viewed\nat\n\\href{https://youtu.be/3DhbUsv4eDo}{\\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.",
      "published": "2025-09-23T17:19:25Z",
      "authors": [
        "Kuanqi Cai",
        "Chunfeng Wang",
        "Zeqi Li",
        "Haowen Yao",
        "Weinan Chen",
        "Luis Figueredo",
        "Aude Billard",
        "Arash Ajoudani"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19261v1",
      "relevance_score": 5
    },
    {
      "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
      "link": "https://arxiv.org/abs/2509.19142v1",
      "pdf_link": "https://arxiv.org/pdf/2509.19142v1.pdf",
      "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer",
      "published": "2025-09-23T15:26:04Z",
      "authors": [
        "Kangmin Kim",
        "Seunghyeok Back",
        "Geonhyup Lee",
        "Sangbeom Lee",
        "Sangjun Noh",
        "Kyoobin Lee"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.19142v1",
      "relevance_score": 5
    },
    {
      "title": "One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain\n  Randomization for One-Shot 6D Pose Estimation",
      "link": "https://arxiv.org/abs/2509.07978v1",
      "pdf_link": "https://arxiv.org/pdf/2509.07978v1.pdf",
      "summary": "Estimating the 6D pose of arbitrary unseen objects from a single reference\nimage is critical for robotics operating in the long-tail of real-world\ninstances. However, this setting is notoriously challenging: 3D models are\nrarely available, single-view reconstructions lack metric scale, and domain\ngaps between generated models and real-world images undermine robustness. We\npropose OnePoseViaGen, a pipeline that tackles these challenges through two key\ncomponents. First, a coarse-to-fine alignment module jointly refines scale and\npose by combining multi-view feature matching with render-and-compare\nrefinement. Second, a text-guided generative domain randomization strategy\ndiversifies textures, enabling effective fine-tuning of pose estimators with\nsynthetic data. Together, these steps allow high-fidelity single-view 3D\ngeneration to support reliable one-shot 6D pose estimation. On challenging\nbenchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves\nstate-of-the-art performance far surpassing prior approaches. We further\ndemonstrate robust dexterous grasping with a real robot hand, validating the\npracticality of our method in real-world manipulation. Project page:\nhttps://gzwsama.github.io/OnePoseviaGen.github.io/",
      "published": "2025-09-09T17:59:02Z",
      "authors": [
        "Zheng Geng",
        "Nan Wang",
        "Shaocong Xu",
        "Chongjie Ye",
        "Bohan Li",
        "Zhaoxi Chen",
        "Sida Peng",
        "Hao Zhao"
      ],
      "categories": [
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.07978v1",
      "relevance_score": 5
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}