{
  "last_updated": "2025-09-08T08:29:29.695753",
  "total_papers": 25,
  "papers": [
    {
      "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
      "link": "https://arxiv.org/abs/2509.04441v1",
      "pdf_link": "https://arxiv.org/pdf/2509.04441v1.pdf",
      "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.",
      "published": "2025-09-04T17:57:13Z",
      "authors": [
        "Hao-Shu Fang",
        "Branden Romero",
        "Yichen Xie",
        "Arthur Hu",
        "Bo-Ruei Huang",
        "Juan Alvarez",
        "Matthew Kim",
        "Gabriel Margolis",
        "Kavya Anbarasu",
        "Masayoshi Tomizuka",
        "Edward Adelson",
        "Pulkit Agrawal"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.04441v1",
      "relevance_score": 9
    },
    {
      "title": "Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator",
      "link": "https://arxiv.org/abs/2509.03859v2",
      "pdf_link": "https://arxiv.org/pdf/2509.03859v2.pdf",
      "summary": "Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM.",
      "published": "2025-09-04T03:36:07Z",
      "authors": [
        "Haichao Zhang",
        "Haonan Yu",
        "Le Zhao",
        "Andrew Choi",
        "Qinxun Bai",
        "Yiqing Yang",
        "Wei Xu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.03859v2",
      "relevance_score": 9
    },
    {
      "title": "Optimizing Grasping in Legged Robots: A Deep Learning Approach to\n  Loco-Manipulation",
      "link": "https://arxiv.org/abs/2508.17466v1",
      "pdf_link": "https://arxiv.org/pdf/2508.17466v1.pdf",
      "summary": "Quadruped robots have emerged as highly efficient and versatile platforms,\nexcelling in navigating complex and unstructured terrains where traditional\nwheeled robots might fail. Equipping these robots with manipulator arms unlocks\nthe advanced capability of loco-manipulation to perform complex physical\ninteraction tasks in areas ranging from industrial automation to\nsearch-and-rescue missions. However, achieving precise and adaptable grasping\nin such dynamic scenarios remains a significant challenge, often hindered by\nthe need for extensive real-world calibration and pre-programmed grasp\nconfigurations. This paper introduces a deep learning framework designed to\nenhance the grasping capabilities of quadrupeds equipped with arms, focusing on\nimproved precision and adaptability. Our approach centers on a sim-to-real\nmethodology that minimizes reliance on physical data collection. We developed a\npipeline within the Genesis simulation environment to generate a synthetic\ndataset of grasp attempts on common objects. By simulating thousands of\ninteractions from various perspectives, we created pixel-wise annotated\ngrasp-quality maps to serve as the ground truth for our model. This dataset was\nused to train a custom CNN with a U-Net-like architecture that processes\nmulti-modal input from an onboard RGB and depth cameras, including RGB images,\ndepth maps, segmentation masks, and surface normal maps. The trained model\noutputs a grasp-quality heatmap to identify the optimal grasp point. We\nvalidated the complete framework on a four-legged robot. The system\nsuccessfully executed a full loco-manipulation task: autonomously navigating to\na target object, perceiving it with its sensors, predicting the optimal grasp\npose using our model, and performing a precise grasp. This work proves that\nleveraging simulated training with advanced sensing offers a scalable and\neffective solution for object handling.",
      "published": "2025-08-24T17:47:56Z",
      "authors": [
        "Dilermando Almeida",
        "Guilherme Lazzarini",
        "Juliano Negri",
        "Thiago H. Segreto",
        "Ricardo V. Godoy",
        "Marcelo Becker"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.17466v1",
      "relevance_score": 9
    },
    {
      "title": "Autonomous Aggregate Sorting in Construction and Mining via Computer\n  Vision-Aided Robotic Arm Systems",
      "link": "https://arxiv.org/abs/2509.00339v1",
      "pdf_link": "https://arxiv.org/pdf/2509.00339v1.pdf",
      "summary": "Traditional aggregate sorting methods, whether manual or mechanical, often\nsuffer from low precision, limited flexibility, and poor adaptability to\ndiverse material properties such as size, shape, and lithology. To address\nthese limitations, this study presents a computer vision-aided robotic arm\nsystem designed for autonomous aggregate sorting in construction and mining\napplications. The system integrates a six-degree-of-freedom robotic arm, a\nbinocular stereo camera for 3D perception, and a ROS-based control framework.\nCore techniques include an attention-augmented YOLOv8 model for aggregate\ndetection, stereo matching for 3D localization, Denavit-Hartenberg kinematic\nmodeling for arm motion control, minimum enclosing rectangle analysis for size\nestimation, and hand-eye calibration for precise coordinate alignment.\nExperimental validation with four aggregate types achieved an average grasping\nand sorting success rate of 97.5%, with comparable classification accuracy.\nRemaining challenges include the reliable handling of small aggregates and\ntexture-based misclassification. Overall, the proposed system demonstrates\nsignificant potential to enhance productivity, reduce operational costs, and\nimprove safety in aggregate handling, while providing a scalable framework for\nadvancing smart automation in construction, mining, and recycling industries.",
      "published": "2025-08-30T03:44:11Z",
      "authors": [
        "Md. Taherul Islam Shawon",
        "Yuan Li",
        "Yincai Cai",
        "Junjie Niu",
        "Ting Peng"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.00339v1",
      "relevance_score": 8
    },
    {
      "title": "No Need to Look! Locating and Grasping Objects by a Robot Arm Covered\n  with Sensitive Skin",
      "link": "https://arxiv.org/abs/2508.17986v1",
      "pdf_link": "https://arxiv.org/pdf/2508.17986v1.pdf",
      "summary": "Locating and grasping of objects by robots is typically performed using\nvisual sensors. Haptic feedback from contacts with the environment is only\nsecondary if present at all. In this work, we explored an extreme case of\nsearching for and grasping objects in complete absence of visual input, relying\non haptic feedback only. The main novelty lies in the use of contacts over the\ncomplete surface of a robot manipulator covered with sensitive skin. The search\nis divided into two phases: (1) coarse workspace exploration with the complete\nrobot surface, followed by (2) precise localization using the end-effector\nequipped with a force/torque sensor. We systematically evaluated this method in\nsimulation and on the real robot, demonstrating that diverse objects can be\nlocated, grasped, and put in a basket. The overall success rate on the real\nrobot for one object was 85.7\\% with failures mainly while grasping specific\nobjects. The method using whole-body contacts is six times faster compared to a\nbaseline that uses haptic feedback only on the end-effector. We also show\nlocating and grasping multiple objects on the table. This method is not\nrestricted to our specific setup and can be deployed on any platform with the\nability of sensing contacts over the entire body surface. This work holds\npromise for diverse applications in areas with challenging visual perception\n(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or\nvegetables need to be located inside foliage and picked.",
      "published": "2025-08-25T12:55:38Z",
      "authors": [
        "Karel Bartunek",
        "Lukas Rustler",
        "Matej Hoffmann"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.17986v1",
      "relevance_score": 8
    },
    {
      "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense\n  Correspondence and Visuotactile Affordance",
      "link": "https://arxiv.org/abs/2509.03889v1",
      "pdf_link": "https://arxiv.org/pdf/2509.03889v1.pdf",
      "summary": "Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.",
      "published": "2025-09-04T05:16:56Z",
      "authors": [
        "Neha Sunil",
        "Megha Tippur",
        "Arnau Saumell",
        "Edward Adelson",
        "Alberto Rodriguez"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.03889v1",
      "relevance_score": 7
    },
    {
      "title": "LaGarNet: Goal-Conditioned Recurrent State-Space Models for\n  Pick-and-Place Garment Flattening",
      "link": "https://arxiv.org/abs/2508.17070v1",
      "pdf_link": "https://arxiv.org/pdf/2508.17070v1.pdf",
      "summary": "We present a novel goal-conditioned recurrent state space (GC-RSSM) model\ncapable of learning latent dynamics of pick-and-place garment manipulation. Our\nproposed method LaGarNet matches the state-of-the-art performance of mesh-based\nmethods, marking the first successful application of state-space models on\ncomplex garments. LaGarNet trains on a coverage-alignment reward and a dataset\ncollected through a general procedure supported by a random policy and a\ndiffusion policy learned from few human demonstrations; it substantially\nreduces the inductive biases introduced in the previous similar methods. We\ndemonstrate that a single-policy LaGarNet achieves flattening on four different\ntypes of garments in both real-world and simulation settings.",
      "published": "2025-08-23T15:52:48Z",
      "authors": [
        "Halid Abdulrahim Kadi",
        "Kasim TerziÄ‡"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.17070v1",
      "relevance_score": 7
    },
    {
      "title": "GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System",
      "link": "https://arxiv.org/abs/2508.15990v1",
      "pdf_link": "https://arxiv.org/pdf/2508.15990v1.pdf",
      "summary": "Accurately perceiving an object's pose and shape is essential for precise\ngrasping and manipulation. Compared to common vision-based methods, tactile\nsensing offers advantages in precision and immunity to occlusion when tracking\nand reconstructing objects in contact. This makes it particularly valuable for\nin-hand and other high-precision manipulation tasks. In this work, we present\nGelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to\nestimate object pose over long periods and reconstruct object shapes with high\nfidelity. Unlike traditional point cloud-based approaches, GelSLAM uses\ntactile-derived surface normals and curvatures for robust tracking and loop\nclosure. It can track object motion in real time with low error and minimal\ndrift, and reconstruct shapes with submillimeter accuracy, even for low-texture\nobjects such as wooden tools. GelSLAM extends tactile sensing beyond local\ncontact to enable global, long-horizon spatial perception, and we believe it\nwill serve as a foundation for many precise manipulation tasks involving\ninteraction with objects in hand. The video demo is available on our website:\nhttps://joehjhuang.github.io/gelslam.",
      "published": "2025-08-21T22:20:43Z",
      "authors": [
        "Hung-Jui Huang",
        "Mohammad Amin Mirzaee",
        "Michael Kaess",
        "Wenzhen Yuan"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.15990v1",
      "relevance_score": 7
    },
    {
      "title": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent\n  Detection",
      "link": "https://arxiv.org/abs/2509.04324v1",
      "pdf_link": "https://arxiv.org/pdf/2509.04324v1.pdf",
      "summary": "Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.",
      "published": "2025-09-04T15:42:36Z",
      "authors": [
        "Chen Hu",
        "Shan Luo",
        "Letizia Gionfrida"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.04324v1",
      "relevance_score": 6
    },
    {
      "title": "Cooperative Grasping for Collective Object Transport in Constrained\n  Environments",
      "link": "https://arxiv.org/abs/2509.03638v1",
      "pdf_link": "https://arxiv.org/pdf/2509.03638v1.pdf",
      "summary": "We propose a novel framework for decision-making in cooperative grasping for\ntwo-robot object transport in constrained environments. The core of the\nframework is a Conditional Embedding (CE) model consisting of two neural\nnetworks that map grasp configuration information into an embedding space. The\nresulting embedding vectors are then used to identify feasible grasp\nconfigurations that allow two robots to collaboratively transport an object. To\nensure generalizability across diverse environments and object geometries, the\nneural networks are trained on a dataset comprising a range of environment maps\nand object shapes. We employ a supervised learning approach with negative\nsampling to ensure that the learned embeddings effectively distinguish between\nfeasible and infeasible grasp configurations. Evaluation results across a wide\nrange of environments and objects in simulations demonstrate the model's\nability to reliably identify feasible grasp configurations. We further validate\nthe framework through experiments on a physical robotic platform, confirming\nits practical applicability.",
      "published": "2025-09-03T18:44:42Z",
      "authors": [
        "David Alvear",
        "George Turkiyyah",
        "Shinkyu Park"
      ],
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.03638v1",
      "relevance_score": 6
    },
    {
      "title": "A Reactive Grasping Framework for Multi-DoF Grippers via Task Space\n  Velocity Fields and Joint Space QP",
      "link": "https://arxiv.org/abs/2509.01044v1",
      "pdf_link": "https://arxiv.org/pdf/2509.01044v1.pdf",
      "summary": "We present a fast and reactive grasping framework for multi-DoF grippers that\ncombines task-space velocity fields with a joint-space Quadratic Program (QP)\nin a hierarchical structure. Reactive, collision-free global motion planning is\nparticularly challenging for high-DoF systems, since simultaneous increases in\nstate dimensionality and planning horizon trigger a combinatorial explosion of\nthe search space, making real-time planning intractable. To address this, we\nplan globally in a lower-dimensional task space, such as fingertip positions,\nand track locally in the full joint space while enforcing all constraints. This\napproach is realized by constructing velocity fields in multiple task-space\ncoordinates (or in some cases a subset of joint coordinates) and solving a\nweighted joint-space QP to compute joint velocities that track these fields\nwith appropriately assigned priorities. Through simulation experiments with\nprivileged knowledge and real-world tests using the recent pose-tracking\nalgorithm FoundationPose, we verify that our method enables high-DoF arm-hand\nsystems to perform real-time, collision-free reaching motions while adapting to\ndynamic environments and external disturbances.",
      "published": "2025-09-01T00:52:01Z",
      "authors": [
        "Yonghyeon Lee",
        "Tzu-Yuan Lin",
        "Alexander Alexiev",
        "Sangbae Kim"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.01044v1",
      "relevance_score": 6
    },
    {
      "title": "Inference of Human-derived Specifications of Object Placement via\n  Demonstration",
      "link": "https://arxiv.org/abs/2508.19367v1",
      "pdf_link": "https://arxiv.org/pdf/2508.19367v1.pdf",
      "summary": "As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,\nobject packing, sorting, and kitting), methods focused on understanding\nhuman-acceptable object configurations remain limited expressively with regard\nto capturing spatial relationships important to humans. To advance robotic\nunderstanding of human rules for object arrangement, we introduce\npositionally-augmented RCC (PARCC), a formal logic framework based on region\nconnection calculus (RCC) for describing the relative position of objects in\nspace. Additionally, we introduce an inference algorithm for learning PARCC\nspecifications via demonstrations. Finally, we present the results from a human\nstudy, which demonstrate our framework's ability to capture a human's intended\nspecification and the benefits of learning from demonstration approaches over\nhuman-provided specifications.",
      "published": "2025-08-26T18:57:21Z",
      "authors": [
        "Alex Cuellar",
        "Ho Chit Siu",
        "Julie A Shah"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.19367v1",
      "relevance_score": 6
    },
    {
      "title": "Taming VR Teleoperation and Learning from Demonstration for Multi-Task\n  Bimanual Table Service Manipulation",
      "link": "https://arxiv.org/abs/2508.14542v2",
      "pdf_link": "https://arxiv.org/pdf/2508.14542v2.pdf",
      "summary": "This technical report presents the champion solution of the Table Service\nTrack in the ICRA 2025 What Bimanuals Can Do (WBCD) competition. We tackled a\nseries of demanding tasks under strict requirements for speed, precision, and\nreliability: unfolding a tablecloth (deformable-object manipulation), placing a\npizza into the container (pick-and-place), and opening and closing a food\ncontainer with the lid. Our solution combines VR-based teleoperation and\nLearning from Demonstrations (LfD) to balance robustness and autonomy. Most\nsubtasks were executed through high-fidelity remote teleoperation, while the\npizza placement was handled by an ACT-based policy trained from 100 in-person\nteleoperated demonstrations with randomized initial configurations. By\ncarefully integrating scoring rules, task characteristics, and current\ntechnical capabilities, our approach achieved both high efficiency and\nreliability, ultimately securing the first place in the competition.",
      "published": "2025-08-20T08:47:40Z",
      "authors": [
        "Weize Li",
        "Zhengxiao Han",
        "Lixin Xu",
        "Xiangyu Chen",
        "Harrison Bounds",
        "Chenrui Zhang",
        "Yifan Xu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.14542v2",
      "relevance_score": 6
    },
    {
      "title": "Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile\n  Sensing",
      "link": "https://arxiv.org/abs/2508.20959v1",
      "pdf_link": "https://arxiv.org/pdf/2508.20959v1.pdf",
      "summary": "Scaling tactile sensing for robust whole-body manipulation is a significant\nchallenge, often limited by wiring complexity, data throughput, and system\nreliability. This paper presents a complete architecture designed to overcome\nthese barriers. Our approach pairs open-source, fabric-based sensors with\ncustom readout electronics that reduce signal crosstalk to less than 3.3%\nthrough hardware-based mitigation. Critically, we introduce a novel,\ndaisy-chained SPI bus topology that avoids the practical limitations of common\nwireless protocols and the prohibitive wiring complexity of USB hub-based\nsystems. This architecture streams synchronized data from over 8,000 taxels\nacross 1 square meter of sensing area at update rates exceeding 50 FPS,\nconfirming its suitability for real-time control. We validate the system's\nefficacy in a whole-body grasping task where, without feedback, the robot's\nopen-loop trajectory results in an uncontrolled application of force that\nslowly crushes a deformable cardboard box. With real-time tactile feedback, the\nrobot transforms this motion into a gentle, stable grasp, successfully\nmanipulating the object without causing structural damage. This work provides a\nrobust and well-characterized platform to enable future research in advanced\nwhole-body control and physical human-robot interaction.",
      "published": "2025-08-28T16:19:16Z",
      "authors": [
        "Curtis C. Johnson",
        "Daniel Webb",
        "David Hill",
        "Marc D. Killpack"
      ],
      "categories": [
        "cs.RO",
        "eess.SP"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.20959v1",
      "relevance_score": 6
    },
    {
      "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation",
      "link": "https://arxiv.org/abs/2508.20085v3",
      "pdf_link": "https://arxiv.org/pdf/2508.20085v3.pdf",
      "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.",
      "published": "2025-08-27T17:53:46Z",
      "authors": [
        "Zhecheng Yuan",
        "Tianming Wei",
        "Langzhe Gu",
        "Pu Hua",
        "Tianhai Liang",
        "Yuanpei Chen",
        "Huazhe Xu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.20085v3",
      "relevance_score": 6
    },
    {
      "title": "Classification of Vision-Based Tactile Sensors: A Review",
      "link": "https://arxiv.org/abs/2509.02478v2",
      "pdf_link": "https://arxiv.org/pdf/2509.02478v2.pdf",
      "summary": "Vision-based tactile sensors (VBTS) have gained widespread application in\nrobotic hands, grippers and prosthetics due to their high spatial resolution,\nlow manufacturing costs, and ease of customization. While VBTSs have common\ndesign features, such as a camera module, they can differ in a rich diversity\nof sensing principles, material compositions, multimodal approaches, and data\ninterpretation methods. Here, we propose a novel classification of VBTS that\ncategorizes the technology into two primary sensing principles based on the\nunderlying transduction of contact into a tactile image: the Marker-Based\nTransduction Principle and the Intensity-Based Transduction Principle.\nMarker-Based Transduction interprets tactile information by detecting marker\ndisplacement and changes in marker density. In contrast, Intensity-Based\nTransduction maps external disturbances with variations in pixel values.\nDepending on the design of the contact module, Marker-Based Transduction can be\nfurther divided into two subtypes: Simple Marker-Based (SMB) and Morphological\nMarker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction\nPrinciple encompasses the Reflective Layer-based (RLB) and Transparent\nLayer-Based (TLB) mechanisms. This paper provides a comparative study of the\nhardware characteristics of these four types of sensors including various\ncombination types, and discusses the commonly used methods for interpreting\ntactile information. This~comparison reveals some current challenges faced by\nVBTS technology and directions for future research.",
      "published": "2025-09-02T16:29:06Z",
      "authors": [
        "Haoran Li",
        "Yijiong Lin",
        "Chenghua Lu",
        "Max Yang",
        "Efi Psomopoulou",
        "Nathan F Lepora"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.02478v2",
      "relevance_score": 5
    },
    {
      "title": "TARA: A Low-Cost 3D-Printed Robotic Arm for Accessible Robotics\n  Education",
      "link": "https://arxiv.org/abs/2509.01043v1",
      "pdf_link": "https://arxiv.org/pdf/2509.01043v1.pdf",
      "summary": "The high cost of robotic platforms limits students' ability to gain practical\nskills directly applicable in real-world scenarios. To address this challenge,\nthis paper presents TARA, a low-cost, 3D-printed robotic arm designed for\naccessible robotics education. TARA includes an open-source repository with\ndesign files, assembly instructions, and baseline code, enabling users to build\nand customize the platform. The system balances affordability and\nfunctionality, offering a highly capable robotic arm for approximately 200 USD,\nsignificantly lower than industrial systems that often cost thousands of\ndollars. Experimental validation confirmed accurate performance in basic\nmanipulation tasks. Rather than focusing on performance benchmarking, this work\nprioritizes educational reproducibility, providing a platform that students and\neducators can reliably replicate and extend.",
      "published": "2025-09-01T00:50:20Z",
      "authors": [
        "Thays Leach Mitre"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.01043v1",
      "relevance_score": 5
    },
    {
      "title": "Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe\n  ZYZ Regrasp on a Doosan M0609",
      "link": "https://arxiv.org/abs/2508.21272v1",
      "pdf_link": "https://arxiv.org/pdf/2508.21272v1.pdf",
      "summary": "This paper presents the first comprehensive application of legal-action\nmasked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated\ngripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly\nlearning. Our approach represents the first systematic integration of\nconstraint-aware reinforcement learning with singularity-safe motion planning\non a Doosan M0609 collaborative robot. We address critical challenges in\nrobotic manipulation: combinatorial action space explosion, unsafe motion\nplanning, and systematic assembly strategy learning. Our system integrates a\nlegal-action masked DQN with hierarchical architecture that decomposes\nQ-function estimation into orientation and position components, reducing\ncomputational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining\nsolution completeness. The robot-friendly reward function encourages\nground-first, vertically accessible assembly sequences aligned with\nmanipulation constraints. Curriculum learning across three progressive\ndifficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training\nefficiency: 100\\% success rate for Level 1 within 500 episodes, 92.9\\% for\nLevel 2, and 39.9\\% for Level 3 over 105,300 total training episodes.",
      "published": "2025-08-29T00:27:03Z",
      "authors": [
        "Jaehong Oh",
        "Seungjun Jung",
        "Sawoong Kim"
      ],
      "categories": [
        "cs.RO",
        "stat.CO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.21272v1",
      "relevance_score": 5
    },
    {
      "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation",
      "link": "https://arxiv.org/abs/2508.21063v1",
      "pdf_link": "https://arxiv.org/pdf/2508.21063v1.pdf",
      "summary": "Creating assembly products demands significant manual effort and expert\nknowledge in 1) designing the assembly and 2) constructing the product. This\npaper introduces Prompt-to-Product, an automated pipeline that generates\nreal-world assembly products from natural language prompts. Specifically, we\nleverage LEGO bricks as the assembly platform and automate the process of\ncreating brick assembly structures. Given the user design requirements,\nPrompt-to-Product generates physically buildable brick designs, and then\nleverages a bimanual robotic system to construct the real assembly products,\nbringing user imaginations into the real world. We conduct a comprehensive user\nstudy, and the results demonstrate that Prompt-to-Product significantly lowers\nthe barrier and reduces manual effort in creating assembly products from\nimaginative ideas.",
      "published": "2025-08-28T17:59:05Z",
      "authors": [
        "Ruixuan Liu",
        "Philip Huang",
        "Ava Pun",
        "Kangle Deng",
        "Shobhit Aggarwal",
        "Kevin Tang",
        "Michelle Liu",
        "Deva Ramanan",
        "Jun-Yan Zhu",
        "Jiaoyang Li",
        "Changliu Liu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.21063v1",
      "relevance_score": 5
    },
    {
      "title": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation",
      "link": "https://arxiv.org/abs/2508.19391v1",
      "pdf_link": "https://arxiv.org/pdf/2508.19391v1.pdf",
      "summary": "Visual-textual understanding is essential for language-guided robot\nmanipulation. Recent works leverage pre-trained vision-language models to\nmeasure the similarity between encoded visual observations and textual\ninstructions, and then train a model to map this similarity to robot actions.\nHowever, this two-step approach limits the model to capture the relationship\nbetween visual observations and textual instructions, leading to reduced\nprecision in manipulation tasks. We propose to learn visual-textual\nassociations through a self-supervised pretext task: reconstructing a masked\ngoal image conditioned on an input image and textual instructions. This\nformulation allows the model to learn visual-action representations without\nrobot action supervision. The learned representations can then be fine-tuned\nfor manipulation tasks with only a few demonstrations. We also introduce the\n\\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot\ntabletop manipulation episodes, including 180 object classes and 3,200\ninstances with corresponding textual instructions. This dataset enables the\nmodel to acquire diverse object priors and allows for a more comprehensive\nevaluation of its generalisation capability across object instances.\nExperimental results on the five benchmarks, including both simulated and\nreal-robot validations, demonstrate that our method outperforms prior art.",
      "published": "2025-08-26T19:34:54Z",
      "authors": [
        "Chaoran Zhu",
        "Hengyi Wang",
        "Yik Lung Pang",
        "Changjae Oh"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.19391v1",
      "relevance_score": 5
    },
    {
      "title": "SimShear: Sim-to-Real Shear-based Tactile Servoing",
      "link": "https://arxiv.org/abs/2508.20561v1",
      "pdf_link": "https://arxiv.org/pdf/2508.20561v1.pdf",
      "summary": "We present SimShear, a sim-to-real pipeline for tactile control that enables\nthe use of shear information without explicitly modeling shear dynamics in\nsimulation. Shear, arising from lateral movements across contact surfaces, is\ncritical for tasks involving dynamic object interactions but remains\nchallenging to simulate. To address this, we introduce shPix2pix, a\nshear-conditioned U-Net GAN that transforms simulated tactile images absent of\nshear, together with a vector encoding shear information, into realistic\nequivalents with shear deformations. This method outperforms baseline pix2pix\napproaches in simulating tactile images and in pose/shear prediction. We apply\nSimShear to two control tasks using a pair of low-cost desktop robotic arms\nequipped with a vision-based tactile sensor: (i) a tactile tracking task, where\na follower arm tracks a surface moved by a leader arm, and (ii) a collaborative\nco-lifting task, where both arms jointly hold an object while the leader\nfollows a prescribed trajectory. Our method maintains contact errors within 1\nto 2 mm across varied trajectories where shear sensing is essential, validating\nthe feasibility of sim-to-real shear modeling with rigid-body simulators and\nopening new directions for simulation in tactile robotics.",
      "published": "2025-08-28T08:54:06Z",
      "authors": [
        "Kipp McAdam Freud",
        "Yijiong Lin",
        "Nathan F. Lepora"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.20561v1",
      "relevance_score": 5
    },
    {
      "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and\n  Manipulation",
      "link": "https://arxiv.org/abs/2508.17643v1",
      "pdf_link": "https://arxiv.org/pdf/2508.17643v1.pdf",
      "summary": "Event cameras offer microsecond latency, high dynamic range, and low power\nconsumption, making them ideal for real-time robotic perception under\nchallenging conditions such as motion blur, occlusion, and illumination\nchanges. However, despite their advantages, synthetic event-based vision\nremains largely unexplored in mainstream robotics simulators. This lack of\nsimulation setup hinders the evaluation of event-driven approaches for robotic\nmanipulation and navigation tasks. This work presents an open-source,\nuser-friendly v2e robotics operating system (ROS) package for Gazebo simulation\nthat enables seamless event stream generation from RGB camera feeds. The\npackage is used to investigate event-based robotic policies (ERP) for real-time\nnavigation and manipulation. Two representative scenarios are evaluated: (1)\nobject following with a mobile robot and (2) object detection and grasping with\na robotic manipulator. Transformer-based ERPs are trained by behavior cloning\nand compared to RGB-based counterparts under various operating conditions.\nExperimental results show that event-guided policies consistently deliver\ncompetitive advantages. The results highlight the potential of event-driven\nperception to improve real-time robotic navigation and manipulation, providing\na foundation for broader integration of event cameras into robotic policy\nlearning. The GitHub repo for the dataset and code:\nhttps://eventbasedvision.github.io/SEBVS/",
      "published": "2025-08-25T04:14:04Z",
      "authors": [
        "Krishna Vinod",
        "Prithvi Jai Ramesh",
        "Pavan Kumar B N",
        "Bharatesh Chakravarthi"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.17643v1",
      "relevance_score": 5
    },
    {
      "title": "U-ARM : Ultra low-cost general teleoperation interface for robot\n  manipulation",
      "link": "https://arxiv.org/abs/2509.02437v1",
      "pdf_link": "https://arxiv.org/pdf/2509.02437v1.pdf",
      "summary": "We propose U-Arm, a low-cost and rapidly adaptable leader-follower\nteleoperation framework designed to interface with most of commercially\navailable robotic arms. Our system supports teleoperation through three\nstructurally distinct 3D-printed leader arms that share consistent control\nlogic, enabling seamless compatibility with diverse commercial robot\nconfigurations. Compared with previous open-source leader-follower interfaces,\nwe further optimized both the mechanical design and servo selection, achieving\na bill of materials (BOM) cost of only \\$50.5 for the 6-DoF leader arm and\n\\$56.8 for the 7-DoF version. To enhance usability, we mitigate the common\nchallenge in controlling redundant degrees of freedom by %engineering methods\nmechanical and control optimizations. Experimental results demonstrate that\nU-Arm achieves 39\\% higher data collection efficiency and comparable task\nsuccess rates across multiple manipulation scenarios compared with Joycon,\nanother low-cost teleoperation interface. We have open-sourced all CAD models\nof three configs and also provided simulation support for validating\nteleoperation workflows. We also open-sourced real-world manipulation data\ncollected with U-Arm. The project website is\nhttps://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.",
      "published": "2025-09-02T15:39:38Z",
      "authors": [
        "Yanwen Zou",
        "Zhaoye Zhou",
        "Chenyang Shi",
        "Zewei Ye",
        "Junda Huang",
        "Yan Ding",
        "Bo Zhao"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.02437v1",
      "relevance_score": 4
    },
    {
      "title": "Language-Guided Long Horizon Manipulation with LLM-based Planning and\n  Visual Perception",
      "link": "https://arxiv.org/abs/2509.02324v1",
      "pdf_link": "https://arxiv.org/pdf/2509.02324v1.pdf",
      "summary": "Language-guided long-horizon manipulation of deformable objects presents\nsignificant challenges due to high degrees of freedom, complex dynamics, and\nthe need for accurate vision-language grounding. In this work, we focus on\nmulti-step cloth folding, a representative deformable-object manipulation task\nthat requires both structured long-horizon planning and fine-grained visual\nperception. To this end, we propose a unified framework that integrates a Large\nLanguage Model (LLM)-based planner, a Vision-Language Model (VLM)-based\nperception system, and a task execution module. Specifically, the LLM-based\nplanner decomposes high-level language instructions into low-level action\nprimitives, bridging the semantic-execution gap, aligning perception with\naction, and enhancing generalization. The VLM-based perception module employs a\nSigLIP2-driven architecture with a bidirectional cross-attention fusion\nmechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to\nachieve language-conditioned fine-grained visual grounding. Experiments in both\nsimulation and real-world settings demonstrate the method's effectiveness. In\nsimulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3\non seen instructions, unseen instructions, and unseen tasks, respectively. On a\nreal robot, it robustly executes multi-step folding sequences from language\ninstructions across diverse cloth materials and configurations, demonstrating\nstrong generalization in practical scenarios. Project page:\nhttps://language-guided.netlify.app/",
      "published": "2025-09-02T13:50:45Z",
      "authors": [
        "Changshi Zhou",
        "Haichuan Xu",
        "Ningquan Gu",
        "Zhipeng Wang",
        "Bin Cheng",
        "Pengpeng Zhang",
        "Yanchao Dong",
        "Mitsuhiro Hayashibe",
        "Yanmin Zhou",
        "Bin He"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.02324v1",
      "relevance_score": 4
    },
    {
      "title": "ManiFlow: A General Robot Manipulation Policy via Consistency Flow\n  Training",
      "link": "https://arxiv.org/abs/2509.01819v1",
      "pdf_link": "https://arxiv.org/pdf/2509.01819v1.pdf",
      "summary": "This paper introduces ManiFlow, a visuomotor imitation learning policy for\ngeneral robot manipulation that generates precise, high-dimensional actions\nconditioned on diverse visual, language and proprioceptive inputs. We leverage\nflow matching with consistency training to enable high-quality dexterous action\ngeneration in just 1-2 inference steps. To handle diverse input modalities\nefficiently, we propose DiT-X, a diffusion transformer architecture with\nadaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained\nfeature interactions between action tokens and multi-modal observations.\nManiFlow demonstrates consistent improvements across diverse simulation\nbenchmarks and nearly doubles success rates on real-world tasks across\nsingle-arm, bimanual, and humanoid robot setups with increasing dexterity. The\nextensive evaluation further demonstrates the strong robustness and\ngeneralizability of ManiFlow to novel objects and background changes, and\nhighlights its strong scaling capability with larger-scale datasets. Our\nwebsite: maniflow-policy.github.io.",
      "published": "2025-09-01T22:50:55Z",
      "authors": [
        "Ge Yan",
        "Jiyue Zhu",
        "Yuquan Deng",
        "Shiqi Yang",
        "Ri-Zhao Qiu",
        "Xuxin Cheng",
        "Marius Memmel",
        "Ranjay Krishna",
        "Ankit Goyal",
        "Xiaolong Wang",
        "Dieter Fox"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.01819v1",
      "relevance_score": 4
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}