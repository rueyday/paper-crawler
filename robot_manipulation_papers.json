{
  "last_updated": "2025-09-22T08:28:45.683853",
  "total_papers": 25,
  "papers": [
    {
      "title": "The Role of Touch: Towards Optimal Tactile Sensing Distribution in\n  Anthropomorphic Hands for Dexterous In-Hand Manipulation",
      "link": "https://arxiv.org/abs/2509.14984v1",
      "pdf_link": "https://arxiv.org/pdf/2509.14984v1.pdf",
      "summary": "In-hand manipulation tasks, particularly in human-inspired robotic systems,\nmust rely on distributed tactile sensing to achieve precise control across a\nwide variety of tasks. However, the optimal configuration of this network of\nsensors is a complex problem, and while the fingertips are a common choice for\nplacing sensors, the contribution of tactile information from other regions of\nthe hand is often overlooked. This work investigates the impact of tactile\nfeedback from various regions of the fingers and palm in performing in-hand\nobject reorientation tasks. We analyze how sensory feedback from different\nparts of the hand influences the robustness of deep reinforcement learning\ncontrol policies and investigate the relationship between object\ncharacteristics and optimal sensor placement. We identify which tactile sensing\nconfigurations contribute to improving the efficiency and accuracy of\nmanipulation. Our results provide valuable insights for the design and use of\nanthropomorphic end-effectors with enhanced manipulation capabilities.",
      "published": "2025-09-18T14:13:26Z",
      "authors": [
        "João Damião Almeida",
        "Egidio Falotico",
        "Cecilia Laschi",
        "José Santos-Victor"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.14984v1",
      "relevance_score": 9
    },
    {
      "title": "\\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by\n  Harnessing Generated Video",
      "link": "https://arxiv.org/abs/2509.14178v1",
      "pdf_link": "https://arxiv.org/pdf/2509.14178v1.pdf",
      "summary": "Dexterous manipulation remains a challenging robotics problem, largely due to\nthe difficulty of collecting extensive human demonstrations for learning. In\nthis paper, we introduce \\textsc{Gen2Real}, which replaces costly human demos\nwith one generated video and drives robot skill from it: it combines\ndemonstration generation that leverages video generation with pose and depth\nestimation to yield hand-object trajectories, trajectory optimization that uses\nPhysics-aware Interaction Optimization Model (PIOM) to impose physics\nconsistency, and demonstration learning that retargets human motions to a robot\nhand and stabilizes control with an anchor-based residual Proximal Policy\nOptimization (PPO) policy. Using only generated videos, the learned policy\nachieves a 77.3\\% success rate on grasping tasks in simulation and demonstrates\ncoherent executions on a real robot. We also conduct ablation studies to\nvalidate the contribution of each component and demonstrate the ability to\ndirectly specify tasks using natural language, highlighting the flexibility and\nrobustness of \\textsc{Gen2Real} in generalizing grasping skills from imagined\nvideos to real-world execution.",
      "published": "2025-09-16T06:17:34Z",
      "authors": [
        "Kai Ye",
        "Yuhang Wu",
        "Shuyuan Hu",
        "Junliang Li",
        "Meng Liu",
        "Yongquan Chen",
        "Rui Huang"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.14178v1",
      "relevance_score": 9
    },
    {
      "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
      "link": "https://arxiv.org/abs/2509.04441v2",
      "pdf_link": "https://arxiv.org/pdf/2509.04441v2.pdf",
      "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.",
      "published": "2025-09-04T17:57:13Z",
      "authors": [
        "Hao-Shu Fang",
        "Branden Romero",
        "Yichen Xie",
        "Arthur Hu",
        "Bo-Ruei Huang",
        "Juan Alvarez",
        "Matthew Kim",
        "Gabriel Margolis",
        "Kavya Anbarasu",
        "Masayoshi Tomizuka",
        "Edward Adelson",
        "Pulkit Agrawal"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.04441v2",
      "relevance_score": 9
    },
    {
      "title": "Optimizing Grasping in Legged Robots: A Deep Learning Approach to\n  Loco-Manipulation",
      "link": "https://arxiv.org/abs/2508.17466v1",
      "pdf_link": "https://arxiv.org/pdf/2508.17466v1.pdf",
      "summary": "Quadruped robots have emerged as highly efficient and versatile platforms,\nexcelling in navigating complex and unstructured terrains where traditional\nwheeled robots might fail. Equipping these robots with manipulator arms unlocks\nthe advanced capability of loco-manipulation to perform complex physical\ninteraction tasks in areas ranging from industrial automation to\nsearch-and-rescue missions. However, achieving precise and adaptable grasping\nin such dynamic scenarios remains a significant challenge, often hindered by\nthe need for extensive real-world calibration and pre-programmed grasp\nconfigurations. This paper introduces a deep learning framework designed to\nenhance the grasping capabilities of quadrupeds equipped with arms, focusing on\nimproved precision and adaptability. Our approach centers on a sim-to-real\nmethodology that minimizes reliance on physical data collection. We developed a\npipeline within the Genesis simulation environment to generate a synthetic\ndataset of grasp attempts on common objects. By simulating thousands of\ninteractions from various perspectives, we created pixel-wise annotated\ngrasp-quality maps to serve as the ground truth for our model. This dataset was\nused to train a custom CNN with a U-Net-like architecture that processes\nmulti-modal input from an onboard RGB and depth cameras, including RGB images,\ndepth maps, segmentation masks, and surface normal maps. The trained model\noutputs a grasp-quality heatmap to identify the optimal grasp point. We\nvalidated the complete framework on a four-legged robot. The system\nsuccessfully executed a full loco-manipulation task: autonomously navigating to\na target object, perceiving it with its sensors, predicting the optimal grasp\npose using our model, and performing a precise grasp. This work proves that\nleveraging simulated training with advanced sensing offers a scalable and\neffective solution for object handling.",
      "published": "2025-08-24T17:47:56Z",
      "authors": [
        "Dilermando Almeida",
        "Guilherme Lazzarini",
        "Juliano Negri",
        "Thiago H. Segreto",
        "Ricardo V. Godoy",
        "Marcelo Becker"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "source": "ArXiv",
      "arxiv_id": "2508.17466v1",
      "relevance_score": 9
    },
    {
      "title": "Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from\n  Human Proprioceptive Sensorimotor Integration",
      "link": "https://arxiv.org/abs/2509.08354v1",
      "pdf_link": "https://arxiv.org/pdf/2509.08354v1.pdf",
      "summary": "Tactile and kinesthetic perceptions are crucial for human dexterous\nmanipulation, enabling reliable grasping of objects via proprioceptive\nsensorimotor integration. For robotic hands, even though acquiring such tactile\nand kinesthetic feedback is feasible, establishing a direct mapping from this\nsensory feedback to motor actions remains challenging. In this paper, we\npropose a novel glove-mediated tactile-kinematic perception-prediction\nframework for grasp skill transfer from human intuitive and natural operation\nto robotic execution based on imitation learning, and its effectiveness is\nvalidated through generalized grasping tasks, including those involving\ndeformable objects. Firstly, we integrate a data glove to capture tactile and\nkinesthetic data at the joint level. The glove is adaptable for both human and\nrobotic hands, allowing data collection from natural human hand demonstrations\nacross different scenarios. It ensures consistency in the raw data format,\nenabling evaluation of grasping for both human and robotic hands. Secondly, we\nestablish a unified representation of multi-modal inputs based on graph\nstructures with polar coordinates. We explicitly integrate the morphological\ndifferences into the designed representation, enhancing the compatibility\nacross different demonstrators and robotic hands. Furthermore, we introduce the\nTactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage\nmultidimensional subgraph convolutions and attention-based LSTM layers to\nextract spatio-temporal features from graph inputs to predict node-based states\nfor each hand joint. These predictions are then mapped to final commands\nthrough a force-position hybrid mapping.",
      "published": "2025-09-10T07:44:12Z",
      "authors": [
        "Ce Guo",
        "Xieyuanli Chen",
        "Zhiwen Zeng",
        "Zirui Guo",
        "Yihong Li",
        "Haoran Xiao",
        "Dewen Hu",
        "Huimin Lu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.08354v1",
      "relevance_score": 9
    },
    {
      "title": "High-Bandwidth Tactile-Reactive Control for Grasp Adjustment",
      "link": "https://arxiv.org/abs/2509.15876v1",
      "pdf_link": "https://arxiv.org/pdf/2509.15876v1.pdf",
      "summary": "Vision-only grasping systems are fundamentally constrained by calibration\nerrors, sensor noise, and grasp pose prediction inaccuracies, leading to\nunavoidable contact uncertainty in the final stage of grasping. High-bandwidth\ntactile feedback, when paired with a well-designed tactile-reactive controller,\ncan significantly improve robustness in the presence of perception errors. This\npaper contributes to controller design by proposing a purely tactile-feedback\ngrasp-adjustment algorithm. The proposed controller requires neither prior\nknowledge of the object's geometry nor an accurate grasp pose, and is capable\nof refining a grasp even when starting from a crude, imprecise initial\nconfiguration and uncertain contact points. Through simulation studies and\nreal-world experiments on a 15-DoF arm-hand system (featuring an 8-DoF hand)\nequipped with fingertip tactile sensors operating at 200 Hz, we demonstrate\nthat our tactile-reactive grasping framework effectively improves grasp\nstability.",
      "published": "2025-09-19T11:22:18Z",
      "authors": [
        "Yonghyeon Lee",
        "Tzu-Yuan Lin",
        "Alexander Alexiev",
        "Sangbae Kim"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.15876v1",
      "relevance_score": 8
    },
    {
      "title": "Soft Regrasping Tool Inspired by Jamming Gripper",
      "link": "https://arxiv.org/abs/2509.13815v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13815v1.pdf",
      "summary": "Regrasping on fixtures is a promising approach to reduce pose uncertainty in\nrobotic assembly, but conventional rigid fixtures lack adaptability and require\ndedicated designs for each part. To overcome this limitation, we propose a soft\njig inspired by the jamming transition phenomenon, which can be continuously\ndeformed to accommodate diverse object geometries. By pressing a\ntriangular-pyramid-shaped tool into the membrane and evacuating the enclosed\nair, a stable cavity is formed as a placement space. We further optimize the\nstamping depth to balance placement stability and gripper accessibility. In\nsoft-jig-based regrasping, the key challenge lies in optimizing the cavity size\nto achieve precise dropping; once the part is reliably placed, subsequent\ngrasping can be performed with reduced uncertainty. Accordingly, we conducted\ndrop experiments on ten mechanical parts of varying shapes, which achieved\nplacement success rates exceeding 80% for most objects and above 90% for\ncylindrical ones, while failures were mainly caused by geometric constraints\nand membrane properties. These results demonstrate that the proposed jig\nenables general-purpose, accurate, and repeatable regrasping, while also\nclarifying its current limitations and future potential as a practical\nalternative to rigid fixtures in assembly automation.",
      "published": "2025-09-17T08:30:21Z",
      "authors": [
        "Takuya Kiyokawa",
        "Zhengtao Hu",
        "Weiwei Wan",
        "Kensuke Harada"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13815v1",
      "relevance_score": 8
    },
    {
      "title": "Collaborative Loco-Manipulation for Pick-and-Place Tasks with Dynamic\n  Reward Curriculum",
      "link": "https://arxiv.org/abs/2509.13239v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13239v1.pdf",
      "summary": "We present a hierarchical RL pipeline for training one-armed legged robots to\nperform pick-and-place (P&P) tasks end-to-end -- from approaching the payload\nto releasing it at a target area -- in both single-robot and cooperative\ndual-robot settings. We introduce a novel dynamic reward curriculum that\nenables a single policy to efficiently learn long-horizon P&P operations by\nprogressively guiding the agents through payload-centered sub-objectives.\nCompared to state-of-the-art approaches for long-horizon RL tasks, our method\nimproves training efficiency by 55% and reduces execution time by 18.6% in\nsimulation experiments. In the dual-robot case, we show that our policy enables\neach robot to attend to different components of its observation space at\ndistinct task stages, promoting effective coordination via autonomous attention\nshifts. We validate our method through real-world experiments using ANYmal D\nplatforms in both single- and dual-robot scenarios. To our knowledge, this is\nthe first RL pipeline that tackles the full scope of collaborative P&P with two\nlegged manipulators.",
      "published": "2025-09-16T16:45:24Z",
      "authors": [
        "Tianxu An",
        "Flavio De Vincenti",
        "Yuntao Ma",
        "Marco Hutter",
        "Stelian Coros"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13239v1",
      "relevance_score": 8
    },
    {
      "title": "Beyond Anthropomorphism: Enhancing Grasping and Eliminating a Degree of\n  Freedom by Fusing the Abduction of Digits Four and Five",
      "link": "https://arxiv.org/abs/2509.13074v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13074v1.pdf",
      "summary": "This paper presents the SABD hand, a 16-degree-of-freedom (DoF) robotic hand\nthat departs from purely anthropomorphic designs to achieve an expanded grasp\nenvelope, enable manipulation poses beyond human capability, and reduce the\nrequired number of actuators. This is achieved by combining the\nadduction/abduction (Add/Abd) joint of digits four and five into a single joint\nwith a large range of motion. The combined joint increases the workspace of the\ndigits by 400\\% and reduces the required DoFs while retaining dexterity.\nExperimental results demonstrate that the combined Add/Abd joint enables the\nhand to grasp objects with a side distance of up to 200 mm. Reinforcement\nlearning-based investigations show that the design enables grasping policies\nthat are effective not only for handling larger objects but also for achieving\nenhanced grasp stability. In teleoperated trials, the hand successfully\nperformed 86\\% of attempted grasps on suitable YCB objects, including\nchallenging non-anthropomorphic configurations. These findings validate the\ndesign's ability to enhance grasp stability, flexibility, and dexterous\nmanipulation without added complexity, making it well-suited for a wide range\nof applications.",
      "published": "2025-09-16T13:30:33Z",
      "authors": [
        "Simon Fritsch",
        "Liam Achenbach",
        "Riccardo Bianco",
        "Nicola Irmiger",
        "Gawain Marti",
        "Samuel Visca",
        "Chenyu Yang",
        "Davide Liconti",
        "Barnabas Gavin Cangan",
        "Robert Jomar Malate",
        "Ronan J. Hinchet",
        "Robert K. Katzschmann"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13074v1",
      "relevance_score": 8
    },
    {
      "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration",
      "link": "https://arxiv.org/abs/2509.09671v1",
      "pdf_link": "https://arxiv.org/pdf/2509.09671v1.pdf",
      "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation.",
      "published": "2025-09-11T17:59:07Z",
      "authors": [
        "Sirui Xu",
        "Yu-Wei Chao",
        "Liuyu Bian",
        "Arsalan Mousavian",
        "Yu-Xiong Wang",
        "Liang-Yan Gui",
        "Wei Yang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.09671v1",
      "relevance_score": 8
    },
    {
      "title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic\n  Grasping",
      "link": "https://arxiv.org/abs/2509.14143v1",
      "pdf_link": "https://arxiv.org/pdf/2509.14143v1.pdf",
      "summary": "Vision-language-action (VLA) models have recently emerged as a promising\nparadigm for robotic control, enabling end-to-end policies that ground natural\nlanguage instructions into visuomotor actions. However, current VLAs often\nstruggle to satisfy precise task constraints, such as stopping based on numeric\nthresholds, since their observation-to-action mappings are implicitly shaped by\ntraining data and lack explicit mechanisms for condition monitoring. In this\nwork, we propose CLAW (CLIP-Language-Action for Weight), a framework that\ndecouples condition evaluation from action generation. CLAW leverages a\nfine-tuned CLIP model as a lightweight prompt generator, which continuously\nmonitors the digital readout of a scale and produces discrete directives based\non task-specific weight thresholds. These prompts are then consumed by $\\pi_0$,\na flow-based VLA policy, which integrates the prompts with multi-view camera\nobservations to produce continuous robot actions. This design enables CLAW to\ncombine symbolic weight reasoning with high-frequency visuomotor control. We\nvalidate CLAW on three experimental setups: single-object grasping and\nmixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW\nreliably executes weight-aware behaviors and outperforms both raw-$\\pi_0$ and\nfine-tuned $\\pi_0$ models. We have uploaded the videos as supplementary\nmaterials.",
      "published": "2025-09-17T16:22:25Z",
      "authors": [
        "Zijian An",
        "Ran Yang",
        "Yiming Feng",
        "Lifeng Zhou"
      ],
      "categories": [
        "cs.RO",
        "68T40"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.14143v1",
      "relevance_score": 7
    },
    {
      "title": "MoiréTac: A Dual-Mode Visuotactile Sensor for Multidimensional\n  Perception Using Moiré Pattern Amplification",
      "link": "https://arxiv.org/abs/2509.12714v1",
      "pdf_link": "https://arxiv.org/pdf/2509.12714v1.pdf",
      "summary": "Visuotactile sensors typically employ sparse marker arrays that limit spatial\nresolution and lack clear analytical force-to-image relationships. To solve\nthis problem, we present \\textbf{Moir\\'eTac}, a dual-mode sensor that generates\ndense interference patterns via overlapping micro-gratings within a transparent\narchitecture. When two gratings overlap with misalignment, they create moir\\'e\npatterns that amplify microscopic deformations. The design preserves optical\nclarity for vision tasks while producing continuous moir\\'e fields for tactile\nsensing, enabling simultaneous 6-axis force/torque measurement, contact\nlocalization, and visual perception. We combine physics-based features\n(brightness, phase gradient, orientation, and period) from moir\\'e patterns\nwith deep spatial features. These are mapped to 6-axis force/torque\nmeasurements, enabling interpretable regression through end-to-end learning.\nExperimental results demonstrate three capabilities: force/torque measurement\nwith R^2 > 0.98 across tested axes; sensitivity tuning through geometric\nparameters (threefold gain adjustment); and vision functionality for object\nclassification despite moir\\'e overlay. Finally, we integrate the sensor into a\nrobotic arm for cap removal with coordinated force and torque control,\nvalidating its potential for dexterous manipulation.",
      "published": "2025-09-16T06:09:43Z",
      "authors": [
        "Kit-Wa Sou",
        "Junhao Gong",
        "Shoujie Li",
        "Chuqiao Lyu",
        "Ziwu Song",
        "Shilong Mu",
        "Wenbo Ding"
      ],
      "categories": [
        "cs.RO",
        "eess.SP"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.12714v1",
      "relevance_score": 7
    },
    {
      "title": "SHaRe-RL: Structured, Interactive Reinforcement Learning for\n  Contact-Rich Industrial Assembly Tasks",
      "link": "https://arxiv.org/abs/2509.13949v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13949v1.pdf",
      "summary": "High-mix low-volume (HMLV) industrial assembly, common in small and\nmedium-sized enterprises (SMEs), requires the same precision, safety, and\nreliability as high-volume automation while remaining flexible to product\nvariation and environmental uncertainty. Current robotic systems struggle to\nmeet these demands. Manual programming is brittle and costly to adapt, while\nlearning-based methods suffer from poor sample efficiency and unsafe\nexploration in contact-rich tasks. To address this, we present SHaRe-RL, a\nreinforcement learning framework that leverages multiple sources of prior\nknowledge. By (i) structuring skills into manipulation primitives, (ii)\nincorporating human demonstrations and online corrections, and (iii) bounding\ninteraction forces with per-axis compliance, SHaRe-RL enables efficient and\nsafe online learning for long-horizon, contact-rich industrial assembly tasks.\nExperiments on the insertion of industrial Harting connector modules with\n0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance\nwithin practical time budgets. Our results show that process expertise, without\nrequiring robotics or RL knowledge, can meaningfully contribute to learning,\nenabling safer, more robust, and more economically viable deployment of RL for\nindustrial assembly.",
      "published": "2025-09-17T13:19:59Z",
      "authors": [
        "Jannick Stranghöner",
        "Philipp Hartmann",
        "Marco Braun",
        "Sebastian Wrede",
        "Klaus Neumann"
      ],
      "categories": [
        "cs.RO",
        "I.2.9"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13949v1",
      "relevance_score": 7
    },
    {
      "title": "Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm\n  Robotic Manipulation",
      "link": "https://arxiv.org/abs/2509.07957v1",
      "pdf_link": "https://arxiv.org/pdf/2509.07957v1.pdf",
      "summary": "Acquiring dexterous robotic skills from human video demonstrations remains a\nsignificant challenge, largely due to conventional reliance on low-level\ntrajectory replication, which often fails to generalize across varying objects,\nspatial layouts, and manipulator configurations. To address this limitation, we\nintroduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that\nenables dual-arm robotic systems to perform task-level reasoning and execution\ndirectly from RGB-D human demonstrations. GF-VLA employs an\ninformation-theoretic approach to extract task-relevant cues, selectively\nhighlighting critical hand-object and object-object interactions. These cues\nare structured into temporally ordered scene graphs, which are subsequently\nintegrated with a language-conditioned transformer to produce hierarchical\nbehavior trees and interpretable Cartesian motion primitives. To enhance\nefficiency in bimanual execution, we propose a cross-arm allocation strategy\nthat autonomously determines gripper assignment without requiring explicit\ngeometric modeling. We validate GF-VLA on four dual-arm block assembly\nbenchmarks involving symbolic structure construction and spatial\ngeneralization. Empirical results demonstrate that the proposed representation\nachieves over 95% graph accuracy and 93% subtask segmentation, enabling the\nlanguage-action planner to generate robust, interpretable task policies. When\ndeployed on a dual-arm robot, these policies attain 94% grasp reliability, 89%\nplacement accuracy, and 90% overall task success across stacking,\nletter-formation, and geometric reconfiguration tasks, evidencing strong\ngeneralization and robustness under diverse spatial and semantic variations.",
      "published": "2025-09-09T17:44:36Z",
      "authors": [
        "Shunlei Li",
        "Longsen Gao",
        "Jiuwen Cao",
        "Yingbai Hu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.07957v1",
      "relevance_score": 7
    },
    {
      "title": "Hydrosoft: Non-Holonomic Hydroelastic Models for Compliant Tactile\n  Manipulation",
      "link": "https://arxiv.org/abs/2509.13126v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13126v1.pdf",
      "summary": "Tactile sensors have long been valued for their perceptual capabilities,\noffering rich insights into the otherwise hidden interface between the robot\nand grasped objects. Yet their inherent compliance -- a key driver of\nforce-rich interactions -- remains underexplored. The central challenge is to\ncapture the complex, nonlinear dynamics introduced by these passive-compliant\nelements. Here, we present a computationally efficient non-holonomic\nhydroelastic model that accurately models path-dependent contact force\ndistributions and dynamic surface area variations. Our insight is to extend the\nobject's state space, explicitly incorporating the distributed forces generated\nby the compliant sensor. Our differentiable formulation not only accounts for\npath-dependent behavior but also enables gradient-based trajectory\noptimization, seamlessly integrating with high-resolution tactile feedback. We\ndemonstrate the effectiveness of our approach across a range of simulated and\nreal-world experiments and highlight the importance of modeling the path\ndependence of sensor dynamics.",
      "published": "2025-09-16T14:34:21Z",
      "authors": [
        "Miquel Oller",
        "An Dang",
        "Nima Fazeli"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13126v1",
      "relevance_score": 7
    },
    {
      "title": "GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage\n  Learning",
      "link": "https://arxiv.org/abs/2509.11594v2",
      "pdf_link": "https://arxiv.org/pdf/2509.11594v2.pdf",
      "summary": "GBPP is a fast learning based scorer that selects a robot base pose for\ngrasping from a single RGB-D snapshot. The method uses a two stage curriculum:\n(1) a simple distance-visibility rule auto-labels a large dataset at low cost;\nand (2) a smaller set of high fidelity simulation trials refines the model to\nmatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLP\nscores dense grids of candidate poses, enabling rapid online selection without\nfull task-and-motion optimization. In simulation and on a real mobile\nmanipulator, GBPP outperforms proximity and geometry only baselines, choosing\nsafer and more reachable stances and degrading gracefully when wrong. The\nresults offer a practical recipe for data efficient, geometry aware base\nplacement: use inexpensive heuristics for coverage, then calibrate with\ntargeted simulation.",
      "published": "2025-09-15T05:25:40Z",
      "authors": [
        "Jizhuo Chen",
        "Diwen Liu",
        "Jiaming Wang",
        "Harold Soh"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.11594v2",
      "relevance_score": 7
    },
    {
      "title": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward\n  Functions",
      "link": "https://arxiv.org/abs/2509.07445v1",
      "pdf_link": "https://arxiv.org/pdf/2509.07445v1.pdf",
      "summary": "Large language models (LLMs) are beginning to automate reward design for\ndexterous manipulation. However, no prior work has considered tactile sensing,\nwhich is known to be critical for human-like dexterity. We present Text2Touch,\nbringing LLM-crafted rewards to the challenging task of multi-axis in-hand\nobject rotation with real-world vision based tactile sensing in palm-up and\npalm-down configurations. Our prompt engineering strategy scales to over 70\nenvironment variables, and sim-to-real distillation enables successful policy\ntransfer to a tactile-enabled fully actuated four-fingered dexterous robot\nhand. Text2Touch significantly outperforms a carefully tuned human-engineered\nbaseline, demonstrating superior rotation speed and stability while relying on\nreward functions that are an order of magnitude shorter and simpler. These\nresults illustrate how LLM-designed rewards can significantly reduce the time\nfrom concept to deployable dexterous tactile skills, supporting more rapid and\nscalable multimodal robot learning. Project website:\nhttps://hpfield.github.io/text2touch-website",
      "published": "2025-09-09T07:10:39Z",
      "authors": [
        "Harrison Field",
        "Max Yang",
        "Yijiong Lin",
        "Efi Psomopoulou",
        "David Barton",
        "Nathan F. Lepora"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.07445v1",
      "relevance_score": 7
    },
    {
      "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense\n  Correspondence and Visuotactile Affordance",
      "link": "https://arxiv.org/abs/2509.03889v1",
      "pdf_link": "https://arxiv.org/pdf/2509.03889v1.pdf",
      "summary": "Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.",
      "published": "2025-09-04T05:16:56Z",
      "authors": [
        "Neha Sunil",
        "Megha Tippur",
        "Arnau Saumell",
        "Edward Adelson",
        "Alberto Rodriguez"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.03889v1",
      "relevance_score": 7
    },
    {
      "title": "Object Pose Estimation through Dexterous Touch",
      "link": "https://arxiv.org/abs/2509.13591v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13591v1.pdf",
      "summary": "Robust object pose estimation is essential for manipulation and interaction\ntasks in robotics, particularly in scenarios where visual data is limited or\nsensitive to lighting, occlusions, and appearances. Tactile sensors often offer\nlimited and local contact information, making it challenging to reconstruct the\npose from partial data. Our approach uses sensorimotor exploration to actively\ncontrol a robot hand to interact with the object. We train with Reinforcement\nLearning (RL) to explore and collect tactile data. The collected 3D point\nclouds are used to iteratively refine the object's shape and pose. In our\nsetup, one hand holds the object steady while the other performs active\nexploration. We show that our method can actively explore an object's surface\nto identify critical pose features without prior knowledge of the object's\ngeometry. Supplementary material and more demonstrations will be provided at\nhttps://amirshahid.github.io/BimanualTactilePose .",
      "published": "2025-09-16T23:25:05Z",
      "authors": [
        "Amir-Hossein Shahidzadeh",
        "Jiyue Zhu",
        "Kezhou Chen",
        "Sha Yi",
        "Cornelia Fermüller",
        "Yiannis Aloimonos",
        "Xiaolong Wang"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13591v1",
      "relevance_score": 6
    },
    {
      "title": "Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation\n  with Reinforcement Learning",
      "link": "https://arxiv.org/abs/2509.13534v1",
      "pdf_link": "https://arxiv.org/pdf/2509.13534v1.pdf",
      "summary": "Whole-body manipulation (WBM) for humanoid robots presents a promising\napproach for executing embracing tasks involving bulky objects, where\ntraditional grasping relying on end-effectors only remains limited in such\nscenarios due to inherent stability and payload constraints. This paper\nintroduces a reinforcement learning framework that integrates a pre-trained\nhuman motion prior with a neural signed distance field (NSDF) representation to\nachieve robust whole-body embracing. Our method leverages a teacher-student\narchitecture to distill large-scale human motion data, generating kinematically\nnatural and physically feasible whole-body motion patterns. This facilitates\ncoordinated control across the arms and torso, enabling stable multi-contact\ninteractions that enhance the robustness in manipulation and also the load\ncapacity. The embedded NSDF further provides accurate and continuous geometric\nperception, improving contact awareness throughout long-horizon tasks. We\nthoroughly evaluate the approach through comprehensive simulations and\nreal-world experiments. The results demonstrate improved adaptability to\ndiverse shapes and sizes of objects and also successful sim-to-real transfer.\nThese indicate that the proposed framework offers an effective and practical\nsolution for multi-contact and long-horizon WBM tasks of humanoid robots.",
      "published": "2025-09-16T21:01:24Z",
      "authors": [
        "Chunxin Zheng",
        "Kai Chen",
        "Zhihai Bi",
        "Yulin Li",
        "Liang Pan",
        "Jinni Zhou",
        "Haoang Li",
        "Jun Ma"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.13534v1",
      "relevance_score": 6
    },
    {
      "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous\n  Manipulation",
      "link": "https://arxiv.org/abs/2509.05513v1",
      "pdf_link": "https://arxiv.org/pdf/2509.05513v1.pdf",
      "summary": "Egocentric human videos provide scalable demonstrations for imitation\nlearning, but existing corpora often lack either fine-grained, temporally\nlocalized action descriptions or dexterous hand annotations. We introduce\nOpenEgo, a multimodal egocentric manipulation dataset with standardized\nhand-pose annotations and intention-aligned action primitives. OpenEgo totals\n1107 hours across six public datasets, covering 290 manipulation tasks in 600+\nenvironments. We unify hand-pose layouts and provide descriptive, timestamped\naction primitives. To validate its utility, we train language-conditioned\nimitation-learning policies to predict dexterous hand trajectories. OpenEgo is\ndesigned to lower the barrier to learning dexterous manipulation from\negocentric video and to support reproducible research in vision-language-action\nlearning. All resources and instructions will be released at\nwww.openegocentric.com.",
      "published": "2025-09-05T21:47:55Z",
      "authors": [
        "Ahad Jawaid",
        "Yu Xiang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.05513v1",
      "relevance_score": 6
    },
    {
      "title": "Geometric Red-Teaming for Robotic Manipulation",
      "link": "https://arxiv.org/abs/2509.12379v1",
      "pdf_link": "https://arxiv.org/pdf/2509.12379v1.pdf",
      "summary": "Standard evaluation protocols in robotic manipulation typically assess policy\nperformance over curated, in-distribution test sets, offering limited insight\ninto how systems fail under plausible variation. We introduce Geometric\nRed-Teaming (GRT), a red-teaming framework that probes robustness through\nobject-centric geometric perturbations, automatically generating CrashShapes --\nstructurally valid, user-constrained mesh deformations that trigger\ncatastrophic failures in pre-trained manipulation policies. The method\nintegrates a Jacobian field-based deformation model with a gradient-free,\nsimulator-in-the-loop optimization strategy. Across insertion, articulation,\nand grasping tasks, GRT consistently discovers deformations that collapse\npolicy performance, revealing brittle failure modes missed by static\nbenchmarks. By combining task-level policy rollouts with constraint-aware shape\nexploration, we aim to build a general purpose framework for structured,\nobject-centric robustness evaluation in robotic manipulation. We additionally\nshow that fine-tuning on individual CrashShapes, a process we refer to as\nblue-teaming, improves task success by up to 60 percentage points on those\nshapes, while preserving performance on the original object, demonstrating the\nutility of red-teamed geometries for targeted policy refinement. Finally, we\nvalidate both red-teaming and blue-teaming results with a real robotic arm,\nobserving that simulated CrashShapes reduce task success from 90% to as low as\n22.5%, and that blue-teaming recovers performance to up to 90% on the\ncorresponding real-world geometry -- closely matching simulation outcomes.\nVideos and code can be found on our project website:\nhttps://georedteam.github.io/ .",
      "published": "2025-09-15T19:12:26Z",
      "authors": [
        "Divyam Goel",
        "Yufei Wang",
        "Tiancheng Wu",
        "Guixiu Qiao",
        "Pavel Piliptchak",
        "David Held",
        "Zackory Erickson"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.12379v1",
      "relevance_score": 6
    },
    {
      "title": "exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic\n  Tactile Representation",
      "link": "https://arxiv.org/abs/2509.14688v1",
      "pdf_link": "https://arxiv.org/pdf/2509.14688v1.pdf",
      "summary": "Tactile-aware robot learning faces critical challenges in data collection and\nrepresentation due to data scarcity and sparsity, and the absence of force\nfeedback in existing systems. To address these limitations, we introduce a\ntactile robot learning system with both hardware and algorithm innovations. We\npresent exUMI, an extensible data collection device that enhances the vanilla\nUMI with robust proprioception (via AR MoCap and rotary encoder), modular\nvisuo-tactile sensing, and automated calibration, achieving 100% data\nusability. Building on an efficient collection of over 1 M tactile frames, we\npropose Tactile Prediction Pretraining (TPP), a representation learning\nframework through action-aware temporal tactile prediction, capturing contact\ndynamics and mitigating tactile sparsity. Real-world experiments show that TPP\noutperforms traditional tactile imitation learning. Our work bridges the gap\nbetween human tactile intuition and robot learning through co-designed hardware\nand algorithms, offering open-source resources to advance contact-rich\nmanipulation research. Project page: https://silicx.github.io/exUMI.",
      "published": "2025-09-18T07:30:02Z",
      "authors": [
        "Yue Xu",
        "Litao Wei",
        "Pengyu An",
        "Qingyu Zhang",
        "Yong-Lu Li"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.14688v1",
      "relevance_score": 5
    },
    {
      "title": "Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking",
      "link": "https://arxiv.org/abs/2509.14530v1",
      "pdf_link": "https://arxiv.org/pdf/2509.14530v1.pdf",
      "summary": "Strawberries naturally grow in clusters, interwoven with leaves, stems, and\nother fruits, which frequently leads to occlusion. This inherent growth habit\npresents a significant challenge for robotic picking, as traditional\npercept-plan-control systems struggle to reach fruits amid the clutter.\nEffectively picking an occluded strawberry demands dexterous manipulation to\ncarefully bypass or gently move the surrounding soft objects and precisely\naccess the ideal picking point located at the stem just above the calyx. To\naddress this challenge, we introduce a strawberry-picking robotic system that\nlearns from human demonstrations. Our system features a 4-DoF SCARA arm paired\nwith a human teleoperation interface for efficient data collection and\nleverages an End Pose Assisted Action Chunking Transformer (ACT) to develop a\nfine-grained visuomotor picking policy. Experiments under various occlusion\nscenarios demonstrate that our modified approach significantly outperforms the\ndirect implementation of ACT, underscoring its potential for practical\napplication in occluded strawberry picking.",
      "published": "2025-09-18T01:55:13Z",
      "authors": [
        "Zhenghao Fei",
        "Wenwu Lu",
        "Linsheng Hou",
        "Chen Peng"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.14530v1",
      "relevance_score": 5
    },
    {
      "title": "LeVR: A Modular VR Teleoperation Framework for Imitation Learning in\n  Dexterous Manipulation",
      "link": "https://arxiv.org/abs/2509.14349v1",
      "pdf_link": "https://arxiv.org/pdf/2509.14349v1.pdf",
      "summary": "We introduce LeVR, a modular software framework designed to bridge two\ncritical gaps in robotic imitation learning. First, it provides robust and\nintuitive virtual reality (VR) teleoperation for data collection using robot\narms paired with dexterous hands, addressing a common limitation in existing\nsystems. Second, it natively integrates with the powerful LeRobot imitation\nlearning (IL) framework, enabling the use of VR-based teleoperation data and\nstreamlining the demonstration collection process. To demonstrate LeVR, we\nrelease LeFranX, an open-source implementation for the Franka FER arm and\nRobotEra XHand, two widely used research platforms. LeFranX delivers a\nseamless, end-to-end workflow from data collection to real-world policy\ndeployment. We validate our system by collecting a public dataset of 100 expert\ndemonstrations and use it to successfully fine-tune state-of-the-art visuomotor\npolicies. We provide our open-source framework, implementation, and dataset to\naccelerate IL research for the robotics community.",
      "published": "2025-09-17T18:30:18Z",
      "authors": [
        "Zhengyang Kris Weng",
        "Matthew L. Elwin",
        "Han Liu"
      ],
      "categories": [
        "cs.RO"
      ],
      "source": "ArXiv",
      "arxiv_id": "2509.14349v1",
      "relevance_score": 5
    }
  ],
  "search_keywords": [
    "manipulation",
    "grasp",
    "grasping",
    "pick",
    "place",
    "dexterous",
    "robotic arm",
    "robot hand",
    "end effector",
    "object manipulation",
    "tactile",
    "force control",
    "impedance control",
    "contact",
    "assembly",
    "bin picking",
    "sorting",
    "stacking"
  ]
}