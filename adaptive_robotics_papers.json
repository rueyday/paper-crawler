{
  "last_updated": "2026-01-05T08:36:48.165874",
  "total_papers": 5,
  "papers": [
    {
      "title": "Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework",
      "summary": "Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework.",
      "published": "2026-01-02T08:41:47Z",
      "authors": [
        "Mehdi Heydari Shahna",
        "Pauli Mustalahti",
        "Jouni Mattila"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2601.00610v1",
      "pdf_link": "https://arxiv.org/pdf/2601.00610v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2601.00610v1",
      "relevance_score": 1
    },
    {
      "title": "NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots",
      "summary": "A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies.",
      "published": "2026-01-02T08:40:35Z",
      "authors": [
        "Mehdi Heydari Shahna",
        "Pauli Mustalahti",
        "Jouni Mattila"
      ],
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2601.00609v1",
      "pdf_link": "https://arxiv.org/pdf/2601.00609v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2601.00609v1",
      "relevance_score": 1
    },
    {
      "title": "MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control",
      "summary": "Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.",
      "published": "2025-12-31T16:36:44Z",
      "authors": [
        "Yongwei Zhang",
        "Yuanzhe Xing",
        "Quan Quan",
        "Zhikun She"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2512.24955v1",
      "pdf_link": "https://arxiv.org/pdf/2512.24955v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.24955v1",
      "relevance_score": 1
    },
    {
      "title": "UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots",
      "summary": "A long-standing objective in humanoid robotics is the realization of versatile agents capable of following diverse multimodal instructions with human-level flexibility. Despite advances in humanoid control, bridging high-level multimodal perception with whole-body execution remains a significant bottleneck. Existing methods often struggle to translate heterogeneous instructions -- such as language, music, and trajectories -- into stable, real-time actions. Here we show that UniAct, a two-stage framework integrating a fine-tuned MLLM with a causal streaming pipeline, enables humanoid robots to execute multimodal instructions with sub-500 ms latency. By unifying inputs through a shared discrete codebook via FSQ, UniAct ensures cross-modal alignment while constraining motions to a physically grounded manifold. This approach yields a 19% improvement in the success rate of zero-shot tracking of imperfect reference motions. We validate UniAct on UniMoCap, our 20-hour humanoid motion benchmark, demonstrating robust generalization across diverse real-world scenarios. Our results mark a critical step toward responsive, general-purpose humanoid assistants capable of seamless interaction through unified perception and control.",
      "published": "2025-12-30T16:20:13Z",
      "authors": [
        "Nan Jiang",
        "Zimo He",
        "Wanhe Yu",
        "Lexi Pang",
        "Yunhao Li",
        "Hongjie Li",
        "Jieming Cui",
        "Yuhan Li",
        "Yizhou Wang",
        "Yixin Zhu",
        "Siyuan Huang"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2512.24321v1",
      "pdf_link": "https://arxiv.org/pdf/2512.24321v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.24321v1",
      "relevance_score": 1
    },
    {
      "title": "Safety for Weakly-Hard Control Systems via Graph-Based Barrier Functions",
      "summary": "Despite significant advancement in technology, communication and computational failures are still prevalent in safety-critical engineering applications. Often, networked control systems experience packet dropouts, leading to open-loop behavior that significantly affects the behavior of the system. Similarly, in real-time control applications, control tasks frequently experience computational overruns and thus occasionally no new actuator command is issued. This article addresses the safety verification and controller synthesis problem for a class of control systems subject to weakly-hard constraints, i.e., a set of window-based constraints where the number of failures are bounded within a given time horizon. The results are based on a new notion of graph-based barrier functions that are specifically tailored to the considered system class, offering a set of constraints whose satisfaction leads to safety guarantees despite communication failures. Subsequent reformulations of the safety constraints are proposed to alleviate conservatism and improve computational tractability, and the resulting trade-offs are discussed. Finally, several numerical case studies demonstrate the effectiveness of the proposed approach.",
      "published": "2026-01-01T22:27:50Z",
      "authors": [
        "Marc Seidel",
        "Mahathi Anand",
        "Frank Allgöwer"
      ],
      "categories": [
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2601.00494v1",
      "pdf_link": "https://arxiv.org/pdf/2601.00494v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2601.00494v1",
      "relevance_score": 1
    }
  ],
  "search_keywords": [
    "perception-driven control",
    "adaptive control",
    "learning-based control",
    "sensorimotor learning",
    "physical intelligence",
    "robot learning",
    "adaptive robotics",
    "dynamic modeling",
    "sensor fusion",
    "contact-rich manipulation",
    "tactile perception",
    "force control",
    "impedance control",
    "contact dynamics",
    "interaction control",
    "embedded robotics",
    "real-time control",
    "resource-efficient robotics",
    "state estimation",
    "kalman filter",
    "closed-loop control",
    "perception and control",
    "model-based reinforcement learning",
    "uncertain dynamics",
    "multi-modal sensing",
    "actuation feedback"
  ]
}