{
  "last_updated": "2025-11-10T08:31:08.276615",
  "total_papers": 10,
  "papers": [
    {
      "title": "Adversarially Robust Multitask Adaptive Control",
      "summary": "We study adversarially robust multitask adaptive linear quadratic control; a\nsetting where multiple systems collaboratively learn control policies under\nmodel uncertainty and adversarial corruption. We propose a clustered multitask\napproach that integrates clustering and system identification with resilient\naggregation to mitigate corrupted model updates. Our analysis characterizes how\nclustering accuracy, intra-cluster heterogeneity, and adversarial behavior\naffect the expected regret of certainty-equivalent (CE) control across LQR\ntasks. We establish non-asymptotic bounds demonstrating that the regret\ndecreases inversely with the number of honest systems per cluster and that this\nreduction is preserved under a bounded fraction of adversarial systems within\neach cluster.",
      "published": "2025-11-07T17:25:21Z",
      "authors": [
        "Kasra Fallah",
        "Leonardo F. Toso",
        "James Anderson"
      ],
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "math.OC"
      ],
      "link": "https://arxiv.org/abs/2511.05444v1",
      "pdf_link": "https://arxiv.org/pdf/2511.05444v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.05444v1",
      "relevance_score": 2
    },
    {
      "title": "Epically Powerful: An open-source software and mechatronics\n  infrastructure for wearable robotic systems",
      "summary": "Epically Powerful is an open-source robotics infrastructure that streamlines\nthe underlying framework of wearable robotic systems - managing communication\nprotocols, clocking, actuator commands, visualization, sensor data acquisition,\ndata logging, and more - while also providing comprehensive guides for hardware\nselection, system assembly, and controller implementation. Epically Powerful\ncontains a code base enabling simplified user implementation via Python that\nseamlessly interfaces with various commercial state-of-the-art quasi-direct\ndrive (QDD) actuators, single-board computers, and common sensors, provides\nexample controllers, and enables real-time visualization. To further support\ndevice development, the package also includes a recommended parts list and\ncompatibility guide and detailed documentation on hardware and software\nimplementation. The goal of Epically Powerful is to lower the barrier to\ndeveloping and deploying custom wearable robotic systems without a\npre-specified form factor, enabling researchers to go from raw hardware to\nmodular, robust devices quickly and effectively. Though originally designed\nwith wearable robotics in mind, Epically Powerful is broadly applicable to\nother robotic domains that utilize QDD actuators, single-board computers, and\nsensors for closed-loop control.",
      "published": "2025-11-07T07:11:55Z",
      "authors": [
        "Jennifer K. Leestma",
        "Siddharth R. Nathella",
        "Christoph P. O. Nuesslein",
        "Snehil Mathur",
        "Gregory S. Sawicki",
        "Aaron J. Young"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2511.05033v1",
      "pdf_link": "https://arxiv.org/pdf/2511.05033v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.05033v1",
      "relevance_score": 1
    },
    {
      "title": "Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot\n  Learning",
      "summary": "We present Isaac Lab, the natural successor to Isaac Gym, which extends the\nparadigm of GPU-native robotics simulation into the era of large-scale\nmulti-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,\nphotorealistic rendering, and a modular, composable architecture for designing\nenvironments and training robot policies. Beyond physics and rendering, the\nframework integrates actuator models, multi-frequency sensor simulation, data\ncollection pipelines, and domain randomization tools, unifying best practices\nfor reinforcement and imitation learning at scale within a single extensible\nplatform. We highlight its application to a diverse set of challenges,\nincluding whole-body control, cross-embodiment mobility, contact-rich and\ndexterous manipulation, and the integration of human demonstrations for skill\nacquisition. Finally, we discuss upcoming integration with the differentiable,\nGPU-accelerated Newton physics engine, which promises new opportunities for\nscalable, data-efficient, and gradient-based approaches to robot learning. We\nbelieve Isaac Lab's combination of advanced simulation capabilities, rich\nsensing, and data-center scale execution will help unlock the next generation\nof breakthroughs in robotics research.",
      "published": "2025-11-06T21:43:02Z",
      "authors": [
        " NVIDIA",
        " :",
        "Mayank Mittal",
        "Pascal Roth",
        "James Tigue",
        "Antoine Richard",
        "Octi Zhang",
        "Peter Du",
        "Antonio Serrano-Muñoz",
        "Xinjie Yao",
        "René Zurbrügg",
        "Nikita Rudin",
        "Lukasz Wawrzyniak",
        "Milad Rakhsha",
        "Alain Denzler",
        "Eric Heiden",
        "Ales Borovicka",
        "Ossama Ahmed",
        "Iretiayo Akinola",
        "Abrar Anwar",
        "Mark T. Carlson",
        "Ji Yuan Feng",
        "Animesh Garg",
        "Renato Gasoto",
        "Lionel Gulich",
        "Yijie Guo",
        "M. Gussert",
        "Alex Hansen",
        "Mihir Kulkarni",
        "Chenran Li",
        "Wei Liu",
        "Viktor Makoviychuk",
        "Grzegorz Malczyk",
        "Hammad Mazhar",
        "Masoud Moghani",
        "Adithyavairavan Murali",
        "Michael Noseworthy",
        "Alexander Poddubny",
        "Nathan Ratliff",
        "Welf Rehberg",
        "Clemens Schwarke",
        "Ritvik Singh",
        "James Latham Smith",
        "Bingjie Tang",
        "Ruchik Thaker",
        "Matthew Trepte",
        "Karl Van Wyk",
        "Fangzhou Yu",
        "Alex Millane",
        "Vikram Ramasamy",
        "Remo Steiner",
        "Sangeeta Subramanian",
        "Clemens Volk",
        "CY Chen",
        "Neel Jawale",
        "Ashwin Varghese Kuruttukulam",
        "Michael A. Lin",
        "Ajay Mandlekar",
        "Karsten Patzwaldt",
        "John Welsh",
        "Huihua Zhao",
        "Fatima Anes",
        "Jean-Francois Lafleche",
        "Nicolas Moënne-Loccoz",
        "Soowan Park",
        "Rob Stepinski",
        "Dirk Van Gelder",
        "Chris Amevor",
        "Jan Carius",
        "Jumyung Chang",
        "Anka He Chen",
        "Pablo de Heras Ciechomski",
        "Gilles Daviet",
        "Mohammad Mohajerani",
        "Julia von Muralt",
        "Viktor Reutskyy",
        "Michael Sauter",
        "Simon Schirm",
        "Eric L. Shi",
        "Pierre Terdiman",
        "Kenny Vilella",
        "Tobias Widmer",
        "Gordon Yeoman",
        "Tiffany Chen",
        "Sergey Grizan",
        "Cathy Li",
        "Lotus Li",
        "Connor Smith",
        "Rafael Wiltz",
        "Kostas Alexis",
        "Yan Chang",
        "David Chu",
        "Linxi \"Jim\" Fan",
        "Farbod Farshidian",
        "Ankur Handa",
        "Spencer Huang",
        "Marco Hutter",
        "Yashraj Narang",
        "Soha Pouya",
        "Shiwei Sheng",
        "Yuke Zhu",
        "Miles Macklin",
        "Adam Moravanszky",
        "Philipp Reist",
        "Yunrong Guo",
        "David Hoeller",
        "Gavriel State"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "link": "https://arxiv.org/abs/2511.04831v1",
      "pdf_link": "https://arxiv.org/pdf/2511.04831v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.04831v1",
      "relevance_score": 1
    },
    {
      "title": "ReGen: Generative Robot Simulation via Inverse Design",
      "summary": "Simulation plays a key role in scaling robot learning and validating\npolicies, but constructing simulations remains a labor-intensive process. This\npaper introduces ReGen, a generative simulation framework that automates\nsimulation design via inverse design. Given a robot's behavior -- such as a\nmotion trajectory or an objective function -- and its textual description,\nReGen infers plausible scenarios and environments that could have caused the\nbehavior. ReGen leverages large language models to synthesize scenarios by\nexpanding a directed graph that encodes cause-and-effect relationships,\nrelevant entities, and their properties. This structured graph is then\ntranslated into a symbolic program, which configures and executes a robot\nsimulation environment. Our framework supports (i) augmenting simulations based\non ego-agent behaviors, (ii) controllable, counterfactual scenario generation,\n(iii) reasoning about agent cognition and mental states, and (iv) reasoning\nwith distinct sensing modalities, such as braking due to faulty GPS signals. We\ndemonstrate ReGen in autonomous driving and robot manipulation tasks,\ngenerating more diverse, complex simulated environments compared to existing\nsimulations with high success rates, and enabling controllable generation for\ncorner cases. This approach enhances the validation of robot policies and\nsupports data or simulation augmentation, advancing scalable robot learning for\nimproved generalization and robustness. We provide code and example videos at:\nhttps://regen-sim.github.io/",
      "published": "2025-11-06T19:41:05Z",
      "authors": [
        "Phat Nguyen",
        "Tsun-Hsuan Wang",
        "Zhang-Wei Hong",
        "Erfan Aasi",
        "Andrew Silva",
        "Guy Rosman",
        "Sertac Karaman",
        "Daniela Rus"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2511.04769v1",
      "pdf_link": "https://arxiv.org/pdf/2511.04769v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.04769v1",
      "relevance_score": 1
    },
    {
      "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human\n  and Object Interaction",
      "summary": "Humanoid robots are expected to operate in human-centered environments where\nsafe and natural physical interaction is essential. However, most recent\nreinforcement learning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are typically\nrestricted to base or end-effector control and focus on resisting extreme\nforces rather than enabling compliance. We introduce GentleHumanoid, a\nframework that integrates impedance control into a whole-body motion tracking\npolicy to achieve upper-body compliance. At its core is a unified spring-based\nformulation that models both resistive contacts (restoring forces when pressing\nagainst surfaces) and guiding contacts (pushes or pulls sampled from human\nmotion data). This formulation ensures kinematically consistent forces across\nthe shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through task-adjustable\nforce thresholds. We evaluate our approach in both simulation and on the\nUnitree G1 humanoid across tasks requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and safe object\nmanipulation. Compared to baselines, our policy consistently reduces peak\ncontact forces while maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward humanoid robots\nthat can safely and effectively collaborate with humans and handle objects in\nreal-world environments.",
      "published": "2025-11-06T18:59:33Z",
      "authors": [
        "Qingzhou Lu",
        "Yao Feng",
        "Baiyu Shi",
        "Michael Piseno",
        "Zhenan Bao",
        "C. Karen Liu"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC"
      ],
      "link": "https://arxiv.org/abs/2511.04679v1",
      "pdf_link": "https://arxiv.org/pdf/2511.04679v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.04679v1",
      "relevance_score": 1
    },
    {
      "title": "Enhancing Fault-Tolerant Space Computing: Guidance Navigation and\n  Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen\n  Multi-Core Processors",
      "summary": "Future planetary exploration missions demand high-performance, fault-tolerant\ncomputing to enable autonomous Guidance, Navigation, and Control (GNC) and\nLander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).\nThis paper evaluates the deployment of GNC and LVS algorithms on\nnext-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx\nVersal--demonstrating up to 15x speedup for LVS image processing and over 250x\nspeedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory\noptimization compared to legacy spaceflight hardware. To ensure computational\nreliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for\nTrusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that\nperforms real-time fault detection and correction across redundant cores.\nARBITER is validated in both static optimization tasks (GFOLD) and dynamic\nclosed-loop control (Attitude Control System). A fault injection study further\nidentifies the gradient computation stage in GFOLD as the most sensitive to\nbit-level errors, motivating selective protection strategies and vector-based\noutput arbitration. This work establishes a scalable and energy-efficient\narchitecture for future missions, including Mars Sample Return, Enceladus\nOrbilander, and Ceres Sample Return, where onboard autonomy, low latency, and\nfault resilience are critical.",
      "published": "2025-11-06T04:45:44Z",
      "authors": [
        "Kyongsik Yun",
        "David Bayard",
        "Gerik Kubiak",
        "Austin Owens",
        "Andrew Johnson",
        "Ryan Johnson",
        "Dan Scharf",
        "Thomas Lu"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2511.04052v1",
      "pdf_link": "https://arxiv.org/pdf/2511.04052v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.04052v1",
      "relevance_score": 1
    },
    {
      "title": "Integrating Ergonomics and Manipulability for Upper Limb Postural\n  Optimization in Bimanual Human-Robot Collaboration",
      "summary": "This paper introduces an upper limb postural optimization method for\nenhancing physical ergonomics and force manipulability during bimanual\nhuman-robot co-carrying tasks. Existing research typically emphasizes human\nsafety or manipulative efficiency, whereas our proposed method uniquely\nintegrates both aspects to strengthen collaboration across diverse conditions\n(e.g., different grasping postures of humans, and different shapes of objects).\nSpecifically, the joint angles of a simplified human skeleton model are\noptimized by minimizing the cost function to prioritize safety and manipulative\ncapability. To guide humans towards the optimized posture, the reference\nend-effector poses of the robot are generated through a transformation module.\nA bimanual model predictive impedance controller (MPIC) is proposed for our\nhuman-like robot, CURI, to recalibrate the end effector poses through planned\ntrajectories. The proposed method has been validated through various subjects\nand objects during human-human collaboration (HHC) and human-robot\ncollaboration (HRC). The experimental results demonstrate significant\nimprovement in muscle conditions by comparing the activation of target muscles\nbefore and after optimization.",
      "published": "2025-11-06T03:16:39Z",
      "authors": [
        "Chenzui Li",
        "Yiming Chen",
        "Xi Wu",
        "Giacinto Barresi",
        "Fei Chen"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2511.04009v1",
      "pdf_link": "https://arxiv.org/pdf/2511.04009v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.04009v1",
      "relevance_score": 1
    },
    {
      "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
      "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.",
      "published": "2025-11-06T02:40:48Z",
      "authors": [
        "Yushi Wang",
        "Changsheng Luo",
        "Penghui Chen",
        "Jianran Liu",
        "Weijian Sun",
        "Tong Guo",
        "Kechang Yang",
        "Biao Hu",
        "Yangang Zhang",
        "Mingguo Zhao"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2511.03996v1",
      "pdf_link": "https://arxiv.org/pdf/2511.03996v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.03996v1",
      "relevance_score": 1
    },
    {
      "title": "Neural Operators for Power Systems: A Physics-Informed Framework for\n  Modeling Power System Components",
      "summary": "Modern power systems require fast and accurate dynamic simulations for\nstability assessment, digital twins, and real-time control, but classical ODE\nsolvers are often too slow for large-scale or online applications. We propose a\nneural-operator framework for surrogate modeling of power system components,\nusing Deep Operator Networks (DeepONets) to learn mappings from system states\nand time-varying inputs to full trajectories without step-by-step integration.\nTo enhance generalization and data efficiency, we introduce Physics-Informed\nDeepONets (PI-DeepONets), which embed the residuals of governing equations into\nthe training loss. Our results show that DeepONets, and especially\nPI-DeepONets, achieve accurate predictions under diverse scenarios, providing\nover 30 times speedup compared to high-order ODE solvers. Benchmarking against\nPhysics-Informed Neural Networks (PINNs) highlights superior stability and\nscalability. Our results demonstrate neural operators as a promising path\ntoward real-time, physics-aware simulation of power system dynamics.",
      "published": "2025-11-07T13:15:26Z",
      "authors": [
        "Ioannis Karampinis",
        "Petros Ellinas",
        "Johanna Vorwerk",
        "Spyros Chatzivasileiadis"
      ],
      "categories": [
        "eess.SY",
        "cs.SY"
      ],
      "link": "https://arxiv.org/abs/2511.05216v1",
      "pdf_link": "https://arxiv.org/pdf/2511.05216v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.05216v1",
      "relevance_score": 1
    },
    {
      "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human\n  Demonstrations",
      "summary": "Human videos can be recorded quickly and at scale, making them an appealing\nsource of training data for robot learning. However, humans and robots differ\nfundamentally in embodiment, resulting in mismatched action execution. Direct\nkinematic retargeting of human hand motion can therefore produce actions that\nare physically infeasible for robots. Despite these low-level differences,\nhuman demonstrations provide valuable motion cues about how to manipulate and\ninteract with objects. Our key idea is to exploit the forward diffusion\nprocess: as noise is added to actions, low-level execution differences fade\nwhile high-level task guidance is preserved. We present X-Diffusion, a\nprincipled framework for training diffusion policies that maximally leverages\nhuman data without learning dynamically infeasible motions. X-Diffusion first\ntrains a classifier to predict whether a noisy action is executed by a human or\nrobot. Then, a human action is incorporated into policy training only after\nadding sufficient noise such that the classifier cannot discern its embodiment.\nActions consistent with robot execution supervise fine-grained denoising at low\nnoise levels, while mismatched human actions provide only coarse guidance at\nhigher noise levels. Our experiments show that naive co-training under\nexecution mismatches degrades policy performance, while X-Diffusion\nconsistently improves it. Across five manipulation tasks, X-Diffusion achieves\na 16% higher average success rate than the best baseline. The project website\nis available at https://portal-cornell.github.io/X-Diffusion/.",
      "published": "2025-11-06T18:56:30Z",
      "authors": [
        "Maximus A. Pace",
        "Prithwish Dan",
        "Chuanruo Ning",
        "Atiksh Bhardwaj",
        "Audrey Du",
        "Edward W. Duan",
        "Wei-Chiu Ma",
        "Kushal Kedia"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "link": "https://arxiv.org/abs/2511.04671v1",
      "pdf_link": "https://arxiv.org/pdf/2511.04671v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2511.04671v1",
      "relevance_score": 1
    }
  ],
  "search_keywords": [
    "perception-driven control",
    "adaptive control",
    "learning-based control",
    "sensorimotor learning",
    "physical intelligence",
    "robot learning",
    "adaptive robotics",
    "dynamic modeling",
    "sensor fusion",
    "contact-rich manipulation",
    "tactile perception",
    "force control",
    "impedance control",
    "contact dynamics",
    "interaction control",
    "embedded robotics",
    "real-time control",
    "resource-efficient robotics",
    "state estimation",
    "kalman filter",
    "closed-loop control",
    "perception and control",
    "model-based reinforcement learning",
    "uncertain dynamics",
    "multi-modal sensing",
    "actuation feedback"
  ]
}