{
  "last_updated": "2025-12-22T08:33:28.807024",
  "total_papers": 20,
  "papers": [
    {
      "title": "UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation",
      "summary": "Perception of deformable linear objects (DLOs), such as cables, ropes, and wires, is the cornerstone for successful downstream manipulation. Although vision-based methods have been extensively explored, they remain highly vulnerable to occlusions that commonly arise in constrained manipulation environments due to surrounding obstacles, large and varying deformations, and limited viewpoints. Moreover, the high dimensionality of the state space, the lack of distinctive visual features, and the presence of sensor noises further compound the challenges of reliable DLO perception. To address these open issues, this paper presents UniStateDLO, the first complete DLO perception pipeline with deep-learning methods that achieves robust performance under severe occlusion, covering both single-frame state estimation and cross-frame state tracking from partial point clouds. Both tasks are formulated as conditional generative problems, leveraging the strong capability of diffusion models to capture the complex mapping between highly partial observations and high-dimensional DLO states. UniStateDLO effectively handles a wide range of occlusion patterns, including initial occlusion, self-occlusion, and occlusion caused by multiple objects. In addition, it exhibits strong data efficiency as the entire network is trained solely on a large-scale synthetic dataset, enabling zero-shot sim-to-real generalization without any real-world training data. Comprehensive simulation and real-world experiments demonstrate that UniStateDLO outperforms all state-of-the-art baselines in both estimation and tracking, producing globally smooth yet locally precise DLO state predictions in real time, even under substantial occlusions. Its integration as the front-end module in a closed-loop DLO manipulation system further demonstrates its ability to support stable feedback control in complex, constrained 3-D environments.",
      "published": "2025-12-19T16:35:02Z",
      "authors": [
        "Kangchen Lv",
        "Mingrui Yu",
        "Shihefeng Wang",
        "Xiangyang Ji",
        "Xiang Li"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2512.17764v1",
      "pdf_link": "https://arxiv.org/pdf/2512.17764v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.17764v1",
      "relevance_score": 3
    },
    {
      "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control",
      "summary": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.",
      "published": "2025-12-19T15:04:24Z",
      "authors": [
        "Yao Feng",
        "Chendong Xiang",
        "Xinyi Mao",
        "Hengkai Tan",
        "Zuyue Zhang",
        "Shuhe Huang",
        "Kaiwen Zheng",
        "Haitian Liu",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "link": "https://arxiv.org/abs/2512.17661v1",
      "pdf_link": "https://arxiv.org/pdf/2512.17661v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.17661v1",
      "relevance_score": 3
    },
    {
      "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
      "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
      "published": "2025-12-18T17:27:03Z",
      "authors": [
        "Xiaopeng Lin",
        "Shijie Lian",
        "Bin Yu",
        "Ruoqi Yang",
        "Changti Wu",
        "Yuzhuo Miao",
        "Yurun Jin",
        "Yukun Shi",
        "Cong Huang",
        "Bojun Cheng",
        "Kai Chen"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2512.16793v1",
      "pdf_link": "https://arxiv.org/pdf/2512.16793v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.16793v1",
      "relevance_score": 3
    },
    {
      "title": "Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry",
      "summary": "This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.",
      "published": "2025-12-19T12:14:37Z",
      "authors": [
        "Ufuk Asil",
        "Efendi Nasibov"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "link": "https://arxiv.org/abs/2512.17505v1",
      "pdf_link": "https://arxiv.org/pdf/2512.17505v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.17505v1",
      "relevance_score": 2
    },
    {
      "title": "Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots",
      "summary": "We present an innovative end-to-end framework for synthesizing semantically meaningful co-speech gestures and deploying them in real-time on a humanoid robot. This system addresses the challenge of creating natural, expressive non-verbal communication for robots by integrating advanced gesture generation techniques with robust physical control. Our core innovation lies in the meticulous integration of a semantics-aware gesture synthesis module, which derives expressive reference motions from speech input by leveraging a generative retrieval mechanism based on large language models (LLMs) and an autoregressive Motion-GPT model. This is coupled with a high-fidelity imitation learning control policy, the MotionTracker, which enables the Unitree G1 humanoid robot to execute these complex motions dynamically and maintain balance. To ensure feasibility, we employ a robust General Motion Retargeting (GMR) method to bridge the embodiment gap between human motion data and the robot platform. Through comprehensive evaluation, we demonstrate that our combined system produces semantically appropriate and rhythmically coherent gestures that are accurately tracked and executed by the physical robot. To our knowledge, this work represents a significant step toward general real-world use by providing a complete pipeline for automatic, semantic-aware, co-speech gesture generation and synchronized real-time physical deployment on a humanoid robot.",
      "published": "2025-12-19T02:55:10Z",
      "authors": [
        "Gang Zhang"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2512.17183v1",
      "pdf_link": "https://arxiv.org/pdf/2512.17183v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.17183v1",
      "relevance_score": 2
    },
    {
      "title": "Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests",
      "summary": "Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation. This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests. Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time. A depth refinement stage is included to improve the representation of thin branches and reduce stereo noise, while GPU optimization increases onboard inference throughput from 4 Hz to 10 Hz.\n  The proposed approach is evaluated against several existing learning-based navigation methods under identical environmental conditions and hardware constraints. It demonstrates higher success rates, more stable trajectories, and improved collision avoidance, particularly in highly cluttered forest settings. The system is deployed on a custom quadrotor in three boreal forest environments, achieving fully autonomous completion in all flights in moderate and dense clutter, and 12 out of 15 flights in highly dense underbrush. These results demonstrate improved reliability and safety over existing navigation methods in complex natural environments.",
      "published": "2025-12-19T13:19:33Z",
      "authors": [
        "Guglielmo Del Col",
        "Väinö Karjalainen",
        "Teemu Hakala",
        "Yibo Zhang",
        "Eija Honkavaara"
      ],
      "categories": [
        "cs.RO"
      ],
      "link": "https://arxiv.org/abs/2512.17553v1",
      "pdf_link": "https://arxiv.org/pdf/2512.17553v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.17553v1",
      "relevance_score": 1
    },
    {
      "title": "Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines",
      "summary": "In the field of gas pipeline location, existing pipeline location methods mostly rely on pipeline location instruments. However, when faced with complex and curved pipeline scenarios, these methods often fail due to problems such as cable entanglement and insufficient equipment flexibility. To address this pain point, we designed a self-propelled pipeline robot. This robot can autonomously complete the location work of complex and curved pipelines in complex pipe networks without external dragging. In terms of pipeline mapping technology, traditional visual mapping and laser mapping methods are easily affected by lighting conditions and insufficient features in the confined space of pipelines, resulting in mapping drift and divergence problems. In contrast, the pipeline location method that integrates inertial navigation and wheel odometers is less affected by pipeline environmental factors. Based on this, this paper proposes a pipeline robot location method based on extended Kalman filtering (EKF). Firstly, the body attitude angle is initially obtained through an inertial measurement unit (IMU). Then, the extended Kalman filtering algorithm is used to improve the accuracy of attitude angle estimation. Finally, high-precision pipeline location is achieved by combining wheel odometers. During the testing phase, the roll wheels of the pipeline robot needed to fit tightly against the pipe wall to reduce slippage. However, excessive tightness would reduce the flexibility of motion control due to excessive friction. Therefore, a balance needed to be struck between the robot's motion capability and positioning accuracy. Experiments were conducted using the self-propelled pipeline robot in a rectangular loop pipeline, and the results verified the effectiveness of the proposed dead reckoning algorithm.",
      "published": "2025-12-19T03:58:02Z",
      "authors": [
        "Yan Gao",
        "Jiliang Wang",
        "Minghan Wang",
        "Xiaohua Chen",
        "Demin Chen",
        "Zhiyong Ren",
        "Tian-Yun Huang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "link": "https://arxiv.org/abs/2512.17215v1",
      "pdf_link": "https://arxiv.org/pdf/2512.17215v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.17215v1",
      "relevance_score": 1
    },
    {
      "title": "Large Video Planner Enables Generalizable Robot Control",
      "summary": "General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.",
      "published": "2025-12-17T18:35:54Z",
      "authors": [
        "Boyuan Chen",
        "Tianyuan Zhang",
        "Haoran Geng",
        "Kiwhan Song",
        "Caiyi Zhang",
        "Peihao Li",
        "William T. Freeman",
        "Jitendra Malik",
        "Pieter Abbeel",
        "Russ Tedrake",
        "Vincent Sitzmann",
        "Yilun Du"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "link": "https://arxiv.org/abs/2512.15840v1",
      "pdf_link": "https://arxiv.org/pdf/2512.15840v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.15840v1",
      "relevance_score": 1
    },
    {
      "title": "Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy",
      "summary": "Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \\ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \\textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.",
      "published": "2025-12-19T18:58:11Z",
      "authors": [
        "Aditya Gahlawat",
        "Ahmed Aboudonia",
        "Sandeep Banik",
        "Naira Hovakimyan",
        "Nikolai Matni",
        "Aaron D. Ames",
        "Gioele Zardini",
        "Alberto Speranzon"
      ],
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "link": "https://arxiv.org/abs/2512.17899v1",
      "pdf_link": "https://arxiv.org/pdf/2512.17899v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.17899v1",
      "relevance_score": 1
    },
    {
      "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
      "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",
      "published": "2025-12-18T18:49:41Z",
      "authors": [
        "Arhan Jain",
        "Mingtong Zhang",
        "Kanav Arora",
        "William Chen",
        "Marcel Torne",
        "Muhammad Zubair Irshad",
        "Sergey Zakharov",
        "Yue Wang",
        "Sergey Levine",
        "Chelsea Finn",
        "Wei-Chiu Ma",
        "Dhruv Shah",
        "Abhishek Gupta",
        "Karl Pertsch"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "link": "https://arxiv.org/abs/2512.16881v1",
      "pdf_link": "https://arxiv.org/pdf/2512.16881v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.16881v1",
      "relevance_score": 1
    },
    {
      "title": "Double Horizon Model-Based Policy Optimization",
      "summary": "Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long \"distribution rollout\" (DR) and a short \"training rollout\" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.",
      "published": "2025-12-17T13:37:23Z",
      "authors": [
        "Akihiro Kubo",
        "Paavo Parmas",
        "Shin Ishii"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "link": "https://arxiv.org/abs/2512.15439v1",
      "pdf_link": "https://arxiv.org/pdf/2512.15439v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.15439v1",
      "relevance_score": 1
    },
    {
      "title": "FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments",
      "summary": "Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.",
      "published": "2025-12-17T13:26:17Z",
      "authors": [
        "Quanxi Zhou",
        "Wencan Mao",
        "Manabu Tsukada",
        "John C. S. Lui",
        "Yusheng Ji"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "link": "https://arxiv.org/abs/2512.15430v1",
      "pdf_link": "https://arxiv.org/pdf/2512.15430v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.15430v1",
      "relevance_score": 1
    },
    {
      "title": "Observer-based Differentially Private Consensus for Linear Multi-agent Systems",
      "summary": "This paper investigates the differentially private consensus problem for general linear multi-agent systems (MASs) based on output feedback protocols. To protect the output information, which is considered private data and may be at high risk of exposure, Laplace noise is added to the information exchange. The conditions for achieving mean square and almost sure consensus in observer-based MASs are established using the backstepping method and the convergence theory for nonnegative almost supermartingales. It is shown that the separation principle remains valid for the consensus problem of linear MASs with decaying Laplace noise. Furthermore, the convergence rate is provided. Then, a joint design framework is developed for state estimation gain, feedback control gain, and noise to ensure the preservation of ε-differential privacy. The output information of each agent is shown to be protected at every time step. Finally, sufficient conditions are established for simultaneously achieving consensus and preserving differential privacy for linear MASs utilizing both full-order and reduced-order observers. Meanwhile, an ε*-differentially private consensus is achieved to meet the desired privacy level. Two simulation examples are provided to validate the theoretical results.",
      "published": "2025-12-18T16:34:10Z",
      "authors": [
        "Xiaofeng Zong",
        "Ming-Yu Wang",
        "Jimin Wang",
        "Ji-Feng Zhang"
      ],
      "categories": [
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2512.16736v2",
      "pdf_link": "https://arxiv.org/pdf/2512.16736v2.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.16736v2",
      "relevance_score": 1
    },
    {
      "title": "Learning-based Approximate Model Predictive Control for an Impact Wrench Tool",
      "summary": "Learning-based model predictive control has emerged as a powerful approach for handling complex dynamics in mechatronic systems, enabling data-driven performance improvements while respecting safety constraints. However, when computational resources are severely limited, as in battery-powered tools with embedded processors, existing approaches struggle to meet real-time requirements. In this paper, we address the problem of real-time torque control for impact wrenches, where high-frequency control updates are necessary to accurately track the fast transients occurring during periodic impact events, while maintaining high-performance safety-critical control that mitigates harmful vibrations and component wear. The key novelty of the approach is that we combine data-driven model augmentation through Gaussian process regression with neural network approximation of the resulting control policy. This insight allows us to deploy predictive control on resource-constrained embedded platforms while maintaining both constraint satisfaction and microsecond-level inference times. The proposed framework is evaluated through numerical simulations and hardware experiments on a custom impact wrench testbed. The results show that our approach successfully achieves real-time control suitable for high-frequency operation while maintaining constraint satisfaction and improving tracking accuracy compared to baseline PID control.",
      "published": "2025-12-18T15:01:30Z",
      "authors": [
        "Mark Benazet",
        "Francesco Ricca",
        "Dario Bralla",
        "Melanie N. Zeilinger",
        "Andrea Carron"
      ],
      "categories": [
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2512.16624v1",
      "pdf_link": "https://arxiv.org/pdf/2512.16624v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.16624v1",
      "relevance_score": 1
    },
    {
      "title": "Machine Learning-based Optimal Control for Colloidal Self-Assembly",
      "summary": "Achieving precise control of colloidal self-assembly into specific patterns remains a longstanding challenge due to the complex process dynamics. Recently, machine learning-based state representation and reinforcement learning-based control strategies have started to accumulate popularity in the field, showing great potential in achieving an automatable and generalizable approach to producing patterned colloidal assembly. In this work, we adopted a machine learning-based optimal control framework, combining unsupervised learning and graph convolutional neural work for state observation with deep reinforcement learning-based optimal control policy calculation, to provide a data-driven control approach that can potentially be generalized to other many-body self-assembly systems. With Brownian dynamics simulations, we demonstrated its superior performance as compared to traditional order parameter-based state description, and its efficacy in obtaining ordered 2-dimensional spherical colloidal self-assembly in an electric field-mediated system with an actual success rate of 97%.",
      "published": "2025-12-18T10:59:45Z",
      "authors": [
        "Andres Lizano-Villalobos",
        "Fangyuan Ma",
        "Wentao Tang",
        "Wei Sun",
        "Xun Tang"
      ],
      "categories": [
        "cond-mat.soft",
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2512.16402v1",
      "pdf_link": "https://arxiv.org/pdf/2512.16402v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.16402v1",
      "relevance_score": 1
    },
    {
      "title": "Closed Loop Reference Optimization for Extrusion Additive Manufacturing",
      "summary": "Various defects occur during material extrusion additive manufacturing processes that degrade the quality of the 3D printed parts and lead to significant material waste. This motivates feedback control of the extrusion process to mitigate defects and prevent print failure. We propose a linear quadratic regulator (LQR) for closed-loop control with force feedback to provide accurate width tracking of the extruded filament. Furthermore, we propose preemptive optimization of the reference force given to the LQR that accounts for the performance of the LQR and generates the optimal reference for the closed loop extrusion dynamics and machine constraints. Simulation results demonstrate the improved tracking performance and response time. Experiments on a Fused Filament Fabrication 3D printer showcase a root mean square error improvement of 39.57% compared to tracking the unmodified reference as well as an 83.7% shorter settling time.",
      "published": "2025-12-18T09:16:38Z",
      "authors": [
        "Rawan Hoteit",
        "Andrea Balestra",
        "Nathan Mingard",
        "Efe C. Balta",
        "John Lygeros"
      ],
      "categories": [
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2512.16333v1",
      "pdf_link": "https://arxiv.org/pdf/2512.16333v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.16333v1",
      "relevance_score": 1
    },
    {
      "title": "Lyapunov-based Adaptive Transformer (LyAT) for Control of Stochastic Nonlinear Systems",
      "summary": "This paper presents a novel Lyapunov-based Adaptive Transformer (LyAT) controller for stochastic nonlinear systems. While transformers have shown promise in various control applications due to sequential modeling through self-attention mechanisms, they have not been used within adaptive control architectures that provide stability guarantees. Existing transformer-based approaches for control rely on offline training with fixed weights, resulting in open-loop implementations that lack real-time adaptation capabilities and stability assurances. To address these limitations, a continuous LyAT controller is developed that adaptively estimates drift and diffusion uncertainties in stochastic dynamical systems without requiring offline pre-training. A key innovation is the analytically derived adaptation law constructed from a Lyapunov-based stability analysis, which enables real-time weight updates while guaranteeing probabilistic uniform ultimate boundedness of tracking and parameter estimation errors. Experimental validation on a quadrotor demonstrates the performance of the developed controller.",
      "published": "2025-12-17T22:02:12Z",
      "authors": [
        "Saiedeh Akbari",
        "Xuehui Shen",
        "Wenqian Xue",
        "Jordan C. Insinger",
        "Warren E. Dixon"
      ],
      "categories": [
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2512.15996v1",
      "pdf_link": "https://arxiv.org/pdf/2512.15996v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.15996v1",
      "relevance_score": 1
    },
    {
      "title": "Ising Machines for Model Predictive Path Integral-Based Optimal Control",
      "summary": "We present a sampling-based Model Predictive Control (MPC) method that implements Model Predictive Path Integral (MPPI) as an \\emph{Ising machine}, suitable for novel forms of probabilistic computing. By expressing the control problem as a Quadratic Unconstrained Binary Optimization (QUBO) problem, we map MPC onto an energy landscape suitable for Gibbs sampling from an Ising model. This formulation enables efficient exploration of (near-)optimal control trajectories. We demonstrate that the approach achieves accurate trajectory tracking compared to a reference MPPI implementation, highlighting the potential of Ising-based MPPI for real-time control in robotics and autonomous systems.",
      "published": "2025-12-17T15:37:41Z",
      "authors": [
        "Lorin Werthen-Brabants",
        "Pieter Simoens"
      ],
      "categories": [
        "eess.SY"
      ],
      "link": "https://arxiv.org/abs/2512.15533v1",
      "pdf_link": "https://arxiv.org/pdf/2512.15533v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.15533v1",
      "relevance_score": 1
    },
    {
      "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
      "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
      "published": "2025-12-10T07:59:45Z",
      "authors": [
        "Hai Ci",
        "Xiaokang Liu",
        "Pei Yang",
        "Yiren Song",
        "Mike Zheng Shou"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "link": "https://arxiv.org/abs/2512.09406v1",
      "pdf_link": "https://arxiv.org/pdf/2512.09406v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.09406v1",
      "relevance_score": 1
    },
    {
      "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
      "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
      "published": "2025-12-07T18:57:15Z",
      "authors": [
        "Yichao Shen",
        "Fangyun Wei",
        "Zhiying Du",
        "Yaobo Liang",
        "Yan Lu",
        "Jiaolong Yang",
        "Nanning Zheng",
        "Baining Guo"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "link": "https://arxiv.org/abs/2512.06963v1",
      "pdf_link": "https://arxiv.org/pdf/2512.06963v1.pdf",
      "source": "ArXiv",
      "arxiv_id": "2512.06963v1",
      "relevance_score": 1
    }
  ],
  "search_keywords": [
    "perception-driven control",
    "adaptive control",
    "learning-based control",
    "sensorimotor learning",
    "physical intelligence",
    "robot learning",
    "adaptive robotics",
    "dynamic modeling",
    "sensor fusion",
    "contact-rich manipulation",
    "tactile perception",
    "force control",
    "impedance control",
    "contact dynamics",
    "interaction control",
    "embedded robotics",
    "real-time control",
    "resource-efficient robotics",
    "state estimation",
    "kalman filter",
    "closed-loop control",
    "perception and control",
    "model-based reinforcement learning",
    "uncertain dynamics",
    "multi-modal sensing",
    "actuation feedback"
  ]
}